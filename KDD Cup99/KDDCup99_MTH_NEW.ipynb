{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MTH of KDDCUP99 Dataset \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import time\n",
    "import datetime\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score,precision_recall_fscore_support\n",
    "from sklearn.metrics import f1_score,roc_auc_score\n",
    "from sklearn.ensemble import RandomForestClassifier,ExtraTreesClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import xgboost as xgb\n",
    "from xgboost import plot_importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the  dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read dataset\n",
    "df = pd.read_csv('./data/kddcup99.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.label.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用onehot处理分类数据\n",
    "s = (df.dtypes=='object')\n",
    "object_cols = list(s[s].index)\n",
    "object_cols #看看哪些列不是数值的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfnew = df[['protocol_type', 'service', 'flag']] #3种取值\n",
    "dfnew.protocol_type.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfnew.service.value_counts() #66种"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfnew.flag.value_counts() #11种。 总共80种，被onehot拆分成80个维度,等下编码完拼接起来总共（42+80-3）列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "#进行onehot编码\n",
    "oh_encoder = OneHotEncoder(handle_unknown='ignore',sparse=False)\n",
    "encoded_data = oh_encoder.fit_transform(pd.DataFrame(dfnew))\n",
    "dfobject = pd.DataFrame(encoded_data) #编码后是array，要先变回dataframe或者series，才能和原来的df拼在一起\n",
    "dfobject.to_csv('./data/dfobject.csv',index=0)\n",
    "dfobject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#还原下名字, 编码后表头是int，必须统一成str，不然后面报错。因为是一个feture分裂成n个列，就取名 原名+序号 吧\n",
    "sub1 = dfobject.iloc[0:0, 0:3].rename(lambda x:'protocol_type'+str(x+1),axis=1)\n",
    "sub2 = dfobject.iloc[0:0,3:69].rename(lambda x:'service'+str(x-2),axis=1)\n",
    "sub3 = dfobject.iloc[0:0,69:80].rename(lambda x:'flag'+str(x-68),axis=1)\n",
    "sub = pd.concat([sub1,sub2],axis=1)\n",
    "sub = list(pd.concat([sub,sub3],axis=1))\n",
    "dfobject = pd.read_csv('./data/dfobject.csv',header=None).drop(0,axis=0) #这里感觉好蠢啊，先存再取哈哈哈哈，我不知道怎么删除旧表头...一开始想用.rename()整体改名的, 结果这个函数里写if一直报错\n",
    "dfobject.columns = sub\n",
    "dfobject.index -= 1 #还原索引\n",
    "dfobject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#连接主表 42+80-3=119列\n",
    "df = df.drop(['protocol_type', 'service', 'flag'], axis=1) \n",
    "df = pd.concat([dfobject,df],axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Z-score normalization\n",
    "features = df.dtypes[df.dtypes != 'object'].index\n",
    "df[features] = df[features].apply(\n",
    "    lambda x: (x - x.mean()) / (x.std()))\n",
    "# Fill empty values by 0\n",
    "df = df.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data sampling\n",
    "Due to the space limit of GitHub files and the large size of network traffic data, we sample a small-sized subset for model learning using **k-means cluster sampling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelencoder = LabelEncoder()\n",
    "df.iloc[:, -1] = labelencoder.fit_transform(df.iloc[:, -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_labels = df['label'].value_counts().index.to_list()\n",
    "label_names = []\n",
    "temp_series = df['label'].value_counts().index.to_list()\n",
    "for i in range(len(temp_labels)):\n",
    "    label_names.append(temp_labels[temp_series.index(i)])\n",
    "del temp_labels\n",
    "del temp_series\n",
    "label_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(['label'],axis=1) \n",
    "y = df.iloc[:, -1].values.reshape(-1,1)\n",
    "y=np.ravel(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# use k-means to cluster the data samples and select a proportion of data from each cluster\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "kmeans = MiniBatchKMeans(n_clusters=1000, random_state=0).fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "klabel=kmeans.labels_\n",
    "df['klabel']=klabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['klabel'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = list(df)\n",
    "cols.insert(120, cols.pop(cols.index('label')))\n",
    "df = df.loc[:, cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df #119列+klabel，120列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def typicalSampling(group):\n",
    "    name = group.name\n",
    "    frac = 0.1 #数据比较少多取了一点\n",
    "    return group.sample(frac=frac)\n",
    "\n",
    "result = df.groupby(\n",
    "    'klabel', group_keys=False\n",
    ").apply(typicalSampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "showValues = result['label'].value_counts()\n",
    "showValues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 筛选并删除只有一个的聚类\n",
    "delete_values = np.where(showValues==1,showValues.index,-1)\n",
    "delete_values = np.unique(delete_values).tolist()\n",
    "del delete_values[0]\n",
    "print(delete_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result = result[~result['label'].isin(delete_values)]\n",
    "result['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.iloc[:,-1] = labelencoder.fit_transform(result.iloc[:, -1])\n",
    "result['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = result.drop(['klabel'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_csv('./data/cup99_sample_km.csv',index=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### split train set and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('./data/cup99_sample_km.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(['label'],axis=1).values\n",
    "y = df.iloc[:, -1].values.reshape(-1,1)\n",
    "y=np.ravel(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y, train_size = 0.8, test_size = 0.2, random_state = 0,stratify = y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection by information gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the result output\n",
    "output_df = pd.DataFrame(columns=['Accuracy', 'Precision', 'Recall', 'F1-Score', 'Time'])\n",
    "output_index = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import mutual_info_classif\n",
    "importances = mutual_info_classif(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#将importance中的数据四舍五入到第四位小数， 压缩打包 reverse=True降序排序\n",
    "f_list = sorted(zip(map(lambda x: round(x, 4), importances), features), reverse=True)\n",
    "Sum = 0\n",
    "fs = []\n",
    "#f_list[i][0] 数值 f_list[i][1] 列名 \n",
    "for i in range(0, len(f_list)): \n",
    "    Sum = Sum + f_list[i][0]\n",
    "    fs.append(f_list[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the important features from top to bottom until the accumulated importance reaches 90%\n",
    "f_list2 = sorted(zip(map(lambda x: round(x, 4), importances/Sum), features), reverse=True)\n",
    "Sum2 = 0\n",
    "fs = []\n",
    "for i in range(0, len(f_list2)):\n",
    "    Sum2 = Sum2 + f_list2[i][0]\n",
    "    fs.append(f_list2[i][1])\n",
    "    if Sum2>=0.5:\n",
    "        break        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_fs = df[fs].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_fs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection by Fast Correlation Based Filter (FCBF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from FCBF_Module import FCBF, FCBFK, FCBFiP, get_i\n",
    "fcbf = FCBFK(k = 20)\n",
    "#fcbf.fit(X_fs, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "X_fss = fcbf.fit_transform(X_fs,y)\n",
    "end_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add to output sheet\n",
    "result_dict = {\n",
    "    'Accuracy': np.NaN,\n",
    "    'Precision': np.NaN,\n",
    "    'Recall': np.NaN,\n",
    "    'F1-Score': np.NaN,\n",
    "    'Time': end_time-start_time\n",
    "}\n",
    "output_df = output_df.append(result_dict, ignore_index=True)\n",
    "# Add index name\n",
    "output_index.append('FCBF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_fss.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-split train & test sets after feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_fss,y, train_size = 0.8, test_size = 0.2, random_state = 0,stratify = y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minority = pd.Series(y_train).value_counts()\n",
    "minority"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SMOTE to solve class-imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把不足1000个的聚类平衡成1000\n",
    "smote_values = np.where(minority<1000,minority.index,-1)\n",
    "smote_values = np.unique(smote_values).tolist()\n",
    "del smote_values[0]\n",
    "print(smote_values) #不足的的索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 转为参数里的形式\n",
    "strategy = {}\n",
    "for i in smote_values:\n",
    "    item = {i:1000}\n",
    "    strategy.update(item)\n",
    "print(strategy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "smote=SMOTE(n_jobs=-1,sampling_strategy=strategy,k_neighbors=1)\n",
    "X_train, y_train = smote.fit_resample(X_train, y_train)\n",
    "pd.Series(y_train).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_combined = np.concatenate((X_train, X_test), axis=0)\n",
    "y_combined = np.concatenate((y_train, y_test), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine learning model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training four base learners: decision tree, random forest, extra trees, XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The length of the test set for prediction time measurement\n",
    "len_test = X_test.shape[0]\n",
    "# Prepare the output dir\n",
    "output_dir = 'output/MTH-IDS/output-{}'.format(datetime.datetime.now().strftime('%y%m%d-%H%M%S'))\n",
    "img_dir = os.path.join(output_dir, 'img')\n",
    "os.makedirs(img_dir)\n",
    "# Prepare the log file\n",
    "log_file = open(os.path.join(output_dir, 'classification_report-{}'.format(datetime.datetime.now().strftime('%y%m%d-%H%M%S'))), 'w+')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xg = xgb.XGBClassifier(n_estimators = 10)\n",
    "t1 = time.time()\n",
    "xg.fit(X_train,y_train)\n",
    "t2 = time.time()\n",
    "xg_score=xg.score(X_test,y_test)\n",
    "t3 = time.time()\n",
    "y_predict=xg.predict(X_test)\n",
    "t4 = time.time()\n",
    "y_true=y_test\n",
    "print('Accuracy of XGBoost: '+ str(xg_score))\n",
    "precision,recall,fscore,none= precision_recall_fscore_support(y_true, y_predict, average='weighted') \n",
    "print('Precision of XGBoost: '+(str(precision)))\n",
    "print('Recall of XGBoost: '+(str(recall)))\n",
    "print('F1-score of XGBoost: '+(str(fscore)))\n",
    "report_str = classification_report(y_true,y_predict); log_file.write('******{}******\\n'.format('XGBoost (Original)')+report_str+'\\n'); print(report_str)\n",
    "cm=confusion_matrix(y_true,y_predict)\n",
    "f,ax=plt.subplots(figsize=(18,14))\n",
    "sns.heatmap(cm,annot=True,linewidth=1,linecolor=\"red\",fmt=\".0f\",ax=ax)\n",
    "ax.set_xticklabels(label_names)\n",
    "ax.set_yticklabels(list(reversed(label_names)))\n",
    "plt.xlabel(\"y_pred\")\n",
    "plt.ylabel(\"y_true\")\n",
    "plt.savefig(os.path.join(img_dir, 'XGBoost_original.pdf'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add to output sheet\n",
    "result_dict = {\n",
    "    'Accuracy': xg_score,\n",
    "    'Precision': precision,\n",
    "    'Recall': recall,\n",
    "    'F1-Score': fscore,\n",
    "    'Train_time': t2-t1,\n",
    "    'Predict_time_per_record': (t4-t3)/len_test,\n",
    "    'HPO_time': np.NaN\n",
    "}\n",
    "output_df = output_df.append(result_dict, ignore_index=True)\n",
    "# Add index name\n",
    "output_index.append('XGBoost (Original)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter optimization (HPO) of XGBoost using Bayesian optimization with tree-based Parzen estimator (BO-TPE)\n",
    "Based on the GitHub repo for HPO: https://github.com/LiYangHart/Hyperparameter-Optimization-of-Machine-Learning-Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from hyperopt import hp, fmin, tpe, STATUS_OK, Trials\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "def objective(params):\n",
    "    params = {\n",
    "        'n_estimators': int(params['n_estimators']), \n",
    "        'max_depth': int(params['max_depth']),\n",
    "        'learning_rate':  abs(float(params['learning_rate'])),\n",
    "\n",
    "    }\n",
    "    clf = xgb.XGBClassifier( **params)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    score = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    return {'loss':-score, 'status': STATUS_OK }\n",
    "\n",
    "space = {\n",
    "    'n_estimators': hp.quniform('n_estimators', 10, 100, 5),\n",
    "    'max_depth': hp.quniform('max_depth', 4, 100, 1),\n",
    "    'learning_rate': hp.normal('learning_rate', 0.01, 0.9),\n",
    "}\n",
    "\n",
    "t1 = time.time()\n",
    "\n",
    "best = fmin(fn=objective,\n",
    "            space=space,\n",
    "            algo=tpe.suggest,\n",
    "            max_evals=20)\n",
    "\n",
    "t2 = time.time()\n",
    "\n",
    "print(\"XGBoost: Hyperopt estimated optimum {}\".format(best))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    'n_estimators': int(best['n_estimators']), \n",
    "    'max_depth': int(best['max_depth']),\n",
    "    'learning_rate':  abs(float(best['learning_rate'])),\n",
    "}\n",
    "xg = xgb.XGBClassifier(**params)\n",
    "t3 = time.time()\n",
    "xg.fit(X_train,y_train)\n",
    "t4 = time.time()\n",
    "xg_score=xg.score(X_test,y_test)\n",
    "t5 = time.time()\n",
    "y_predict=xg.predict(X_test)\n",
    "t6 = time.time()\n",
    "y_true=y_test\n",
    "print('Accuracy of XGBoost: '+ str(xg_score))\n",
    "precision,recall,fscore,none= precision_recall_fscore_support(y_true, y_predict, average='weighted') \n",
    "print('Precision of XGBoost: '+(str(precision)))\n",
    "print('Recall of XGBoost: '+(str(recall)))\n",
    "print('F1-score of XGBoost: '+(str(fscore)))\n",
    "report_str = classification_report(y_true,y_predict); log_file.write('******{}******\\n'.format('XGBoost (BO-TPE)')+report_str+'\\n'); print(report_str)\n",
    "cm=confusion_matrix(y_true,y_predict)\n",
    "f,ax=plt.subplots(figsize=(18,14))\n",
    "sns.heatmap(cm,annot=True,linewidth=0.5,linecolor=\"red\",fmt=\".0f\",ax=ax)\n",
    "ax.set_xticklabels(label_names)\n",
    "ax.set_yticklabels(list(reversed(label_names)))\n",
    "plt.xlabel(\"y_pred\")\n",
    "plt.ylabel(\"y_true\")\n",
    "plt.savefig(os.path.join(img_dir, 'XGBoost_BO-TPE.pdf'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add to output sheet\n",
    "result_dict = {\n",
    "    'Accuracy': xg_score,\n",
    "    'Precision': precision,\n",
    "    'Recall': recall,\n",
    "    'F1-Score': fscore,\n",
    "    'HPO_time': t2-t1,\n",
    "    'Train_time': t4-t3,\n",
    "    'Predict_time_per_record': (t6-t5)/len_test\n",
    "}\n",
    "output_df = output_df.append(result_dict, ignore_index=True)\n",
    "# Add index name\n",
    "output_index.append('XGBoost (BO-TPE)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xg_train=xg.predict(X_train)\n",
    "xg_test=xg.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter optimization (HPO) of XGBoost using Particle Swarm Optimization (PSO)\n",
    "Based on the GitHub repo for HPO: https://github.com/LiYangHart/Hyperparameter-Optimization-of-Machine-Learning-Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#XGBoost\n",
    "import optunity\n",
    "import optunity.metrics\n",
    "\n",
    "data= X_combined\n",
    "labels= y_combined.tolist()\n",
    "Y_train = y_train\n",
    "Y_test = y_test\n",
    "# Define the hyperparameter configuration space\n",
    "search = {\n",
    "    'n_estimators': [10, 100],\n",
    "    'max_depth': [5,50],\n",
    "    'learning_rate': [0.01, 0.9]\n",
    "}\n",
    "# Define the objective function\n",
    "@optunity.cross_validated(x=data, y=labels, num_folds=3)\n",
    "def performance(x_train, y_train, x_test, y_test,n_estimators=None, max_depth=None,learning_rate=None):\n",
    "    # fit the model\n",
    "    params = {\n",
    "        'n_estimators': int(n_estimators), \n",
    "        'max_depth': int(max_depth),\n",
    "        'learning_rate':  abs(float(learning_rate)),\n",
    "    }\n",
    "    model = xgb.XGBClassifier( **params)\n",
    "    model.fit(X_train, Y_train)\n",
    "    predictions = model.predict(X_test)\n",
    "    # scores=np.mean(cross_val_score(model, X, y, cv=3, n_jobs=-1,\n",
    "    #                                 scoring=\"accuracy\"))\n",
    "    #return optunity.metrics.roc_auc(y_test, predictions, positive=True)\n",
    "    return optunity.metrics.accuracy(Y_test, predictions)\n",
    "\n",
    "t1 = time.time()\n",
    "\n",
    "optimal_configuration, info, _ = optunity.maximize(performance,\n",
    "                                                  solver_name='particle swarm',\n",
    "                                                  num_evals=20,\n",
    "                                                   **search\n",
    "                                                  )\n",
    "\n",
    "t2 = time.time()\n",
    "\n",
    "print(optimal_configuration)\n",
    "print(\"Accuracy:\"+ str(info.optimum))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'n_estimators': int(optimal_configuration['n_estimators']), \n",
    "    'max_depth': int(optimal_configuration['max_depth']), \n",
    "    'learning_rate': abs(float(optimal_configuration['learning_rate']))\n",
    "}\n",
    "xg = xgb.XGBClassifier(**params)\n",
    "t3 = time.time()\n",
    "xg.fit(X_train,y_train)\n",
    "t4 = time.time()\n",
    "xg_score=xg.score(X_test,y_test)\n",
    "t5 = time.time()\n",
    "y_predict=xg.predict(X_test)\n",
    "t6 = time.time()\n",
    "y_true=y_test\n",
    "print('Accuracy of XGBoost: '+ str(xg_score))\n",
    "precision,recall,fscore,none= precision_recall_fscore_support(y_true, y_predict, average='weighted') \n",
    "print('Precision of XGBoost: '+(str(precision)))\n",
    "print('Recall of XGBoost: '+(str(recall)))\n",
    "print('F1-score of XGBoost: '+(str(fscore)))\n",
    "report_str = classification_report(y_true,y_predict); log_file.write('******{}******\\n'.format('XGBoost (PSO)')+report_str+'\\n'); print(report_str)\n",
    "cm=confusion_matrix(y_true,y_predict)\n",
    "f,ax=plt.subplots(figsize=(18,14))\n",
    "sns.heatmap(cm,annot=True,linewidth=0.5,linecolor=\"red\",fmt=\".0f\",ax=ax)\n",
    "ax.set_xticklabels(label_names)\n",
    "ax.set_yticklabels(list(reversed(label_names)))\n",
    "plt.xlabel(\"y_pred\")\n",
    "plt.ylabel(\"y_true\")\n",
    "plt.savefig(os.path.join(img_dir, 'XGBoost_PSO.pdf'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add to output sheet\n",
    "result_dict = {\n",
    "    'Accuracy': xg_score,\n",
    "    'Precision': precision,\n",
    "    'Recall': recall,\n",
    "    'F1-Score': fscore,\n",
    "    'HPO_time': t2-t1,\n",
    "    'Train_time': t4-t3,\n",
    "    'Predict_time_per_record': (t6-t5)/len_test\n",
    "}\n",
    "output_df = output_df.append(result_dict, ignore_index=True)\n",
    "# Add index name\n",
    "output_index.append('XGBoost (PSO)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter optimization (HPO) of XGBoost using Genetic Algorithm (GA)\n",
    "Based on the GitHub repo for HPO: https://github.com/LiYangHart/Hyperparameter-Optimization-of-Machine-Learning-Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Xgboost\n",
    "from tpot import TPOTClassifier\n",
    "# Define the hyperparameter configuration space\n",
    "parameters = {\n",
    "    'n_estimators': range(10,100),\n",
    "    'max_depth': range(4,100),\n",
    "    'learning_rate': [i/100 for i in range(1, 90)]\n",
    "}\n",
    "# Set the hyperparameters of GA                 \n",
    "ga = TPOTClassifier(generations= 3, population_size= 10, offspring_size= 5,\n",
    "                                 verbosity= 3, early_stop= 5,\n",
    "                                 config_dict=\n",
    "                                 {'xgboost.XGBClassifier': parameters}, \n",
    "                                 cv = 3, scoring = 'accuracy')\n",
    "t1 = time.time()\n",
    "ga.fit(X_combined, y_combined)\n",
    "t2 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper method: convert the values represented by string to its correct type\n",
    "def type_str(input_str:str):\n",
    "    # is integer\n",
    "    if input_str.isdecimal():\n",
    "        return int(input_str)\n",
    "    # is float\n",
    "    elif input_str.isdigit():\n",
    "        return float(input_str)\n",
    "    # is string\n",
    "    elif input_str.startswith('\"') and input_str.endswith('\"'):\n",
    "        # remove quotation marks\n",
    "        return input_str[1: -1]\n",
    "    else:\n",
    "        return input_str\n",
    "\n",
    "# Extract the optimized parameter from the generated pipeline\n",
    "def get_ga_optimized_parameters(fitted_tpot_obj: TPOTClassifier, classifier_name: str, temp_file_name:str='temp_ga_pipeline.py'):\n",
    "    # Export the pipeline\n",
    "    fitted_tpot_obj.export(output_file_name=temp_file_name)\n",
    "    # Read the optimized pipeline\n",
    "    with open(temp_file_name) as temp_file:\n",
    "        lines = temp_file.readlines()\n",
    "    for line in lines:\n",
    "        if classifier_name+'(' in line.strip():\n",
    "            pipeline = line\n",
    "            break\n",
    "    # Extract the optimized parameters\n",
    "    start_index = pipeline.index(classifier_name+'(')\n",
    "    end_index = pipeline.index(')')\n",
    "    parameters_str = pipeline[start_index+len(classifier_name)+1: end_index]\n",
    "    parameters = dict()\n",
    "    for temp_str in parameters_str.split(sep=','):\n",
    "        temp_list = temp_str.split('=')\n",
    "        parameters[temp_list[0].strip()] = type_str(temp_list[1].strip())\n",
    "    # Delect the temp file\n",
    "    os.remove(temp_file_name)\n",
    "    # Return the optimized parameters\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xg = xgb.XGBClassifier(**get_ga_optimized_parameters(ga, 'XGBClassifier'))\n",
    "t3 = time.time()\n",
    "xg.fit(X_train,y_train)\n",
    "t4 = time.time()\n",
    "xg_score=xg.score(X_test,y_test)\n",
    "t5 = time.time()\n",
    "y_predict=xg.predict(X_test)\n",
    "t6 = time.time()\n",
    "y_true=y_test\n",
    "print('Accuracy of XGBoost: '+ str(xg_score))\n",
    "precision,recall,fscore,none= precision_recall_fscore_support(y_true, y_predict, average='weighted') \n",
    "print('Precision of XGBoost: '+(str(precision)))\n",
    "print('Recall of XGBoost: '+(str(recall)))\n",
    "print('F1-score of XGBoost: '+(str(fscore)))\n",
    "report_str = classification_report(y_true,y_predict); log_file.write('******{}******\\n'.format('XGBoost (GA)')+report_str+'\\n'); print(report_str)\n",
    "cm=confusion_matrix(y_true,y_predict)\n",
    "f,ax=plt.subplots(figsize=(18,14))\n",
    "sns.heatmap(cm,annot=True,linewidth=0.5,linecolor=\"red\",fmt=\".0f\",ax=ax)\n",
    "ax.set_xticklabels(label_names)\n",
    "ax.set_yticklabels(list(reversed(label_names)))\n",
    "plt.xlabel(\"y_pred\")\n",
    "plt.ylabel(\"y_true\")\n",
    "plt.savefig(os.path.join(img_dir, 'XGBoost_GA.pdf'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add to output sheet\n",
    "result_dict = {\n",
    "    'Accuracy': xg_score,\n",
    "    'Precision': precision,\n",
    "    'Recall': recall,\n",
    "    'F1-Score': fscore,\n",
    "    'HPO_time': t2-t1,\n",
    "    'Train_time': t4-t3,\n",
    "    'Predict_time_per_record': (t6-t5)/len_test\n",
    "}\n",
    "output_df = output_df.append(result_dict, ignore_index=True)\n",
    "# Add index name\n",
    "output_index.append('XGBoost (GA)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(random_state = 0)\n",
    "t1 = time.time()\n",
    "rf.fit(X_train,y_train)\n",
    "t2 = time.time() \n",
    "rf_score=rf.score(X_test,y_test)\n",
    "t3 = time.time()\n",
    "y_predict=rf.predict(X_test)\n",
    "t4 = time.time()\n",
    "y_true=y_test\n",
    "print('Accuracy of RF: '+ str(rf_score))\n",
    "precision,recall,fscore,none= precision_recall_fscore_support(y_true, y_predict, average='weighted') \n",
    "print('Precision of RF: '+(str(precision)))\n",
    "print('Recall of RF: '+(str(recall)))\n",
    "print('F1-score of RF: '+(str(fscore)))\n",
    "report_str = classification_report(y_true,y_predict); log_file.write('******{}******\\n'.format('RF (Original)')+report_str+'\\n'); print(report_str)\n",
    "cm=confusion_matrix(y_true,y_predict)\n",
    "f,ax=plt.subplots(figsize=(18,14))\n",
    "sns.heatmap(cm,annot=True,linewidth=0.5,linecolor=\"red\",fmt=\".0f\",ax=ax)\n",
    "ax.set_xticklabels(label_names)\n",
    "ax.set_yticklabels(list(reversed(label_names)))\n",
    "plt.xlabel(\"y_pred\")\n",
    "plt.ylabel(\"y_true\")\n",
    "plt.savefig(os.path.join(img_dir, 'RF_original.pdf'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add to output sheet\n",
    "result_dict = {\n",
    "    'Accuracy': rf_score,\n",
    "    'Precision': precision,\n",
    "    'Recall': recall,\n",
    "    'F1-Score': fscore,\n",
    "    'Train_time': t2-t1,\n",
    "    'Predict_time_per_record': (t4/t3)/len_test,\n",
    "    'HPO_time': np.NaN\n",
    "}\n",
    "output_df = output_df.append(result_dict, ignore_index=True)\n",
    "# Add index name\n",
    "output_index.append('RF (Original)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter optimization (HPO) of random forest using Bayesian optimization with tree-based Parzen estimator (BO-TPE)\n",
    "Based on the GitHub repo for HPO: https://github.com/LiYangHart/Hyperparameter-Optimization-of-Machine-Learning-Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Hyperparameter optimization of random forest\n",
    "from hyperopt import hp, fmin, tpe, STATUS_OK, Trials\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "# Define the objective function\n",
    "def objective(params):\n",
    "    params = {\n",
    "        'n_estimators': int(params['n_estimators']), \n",
    "        'max_depth': int(params['max_depth']),\n",
    "        'max_features': int(params['max_features']),\n",
    "        \"min_samples_split\":int(params['min_samples_split']),\n",
    "        \"min_samples_leaf\":int(params['min_samples_leaf']),\n",
    "        \"criterion\":str(params['criterion'])\n",
    "    }\n",
    "    clf = RandomForestClassifier( **params)\n",
    "    clf.fit(X_train,y_train)\n",
    "    score=clf.score(X_test,y_test)\n",
    "\n",
    "    return {'loss':-score, 'status': STATUS_OK }\n",
    "# Define the hyperparameter configuration space\n",
    "available_criterion = ['gini','entropy']\n",
    "space = {\n",
    "    'n_estimators': hp.quniform('n_estimators', 10, 200, 1),\n",
    "    'max_depth': hp.quniform('max_depth', 5, 50, 1),\n",
    "    \"max_features\":hp.quniform('max_features', 1, 20, 1),\n",
    "    \"min_samples_split\":hp.quniform('min_samples_split',2,11,1),\n",
    "    \"min_samples_leaf\":hp.quniform('min_samples_leaf',1,11,1),\n",
    "    \"criterion\":hp.choice('criterion', available_criterion)\n",
    "}\n",
    "\n",
    "t1 = time.time()\n",
    "\n",
    "best = fmin(fn=objective,\n",
    "            space=space,\n",
    "            algo=tpe.suggest,\n",
    "            max_evals=20)\n",
    "\n",
    "t2 = time.time()\n",
    "\n",
    "print(\"Random Forest: Hyperopt estimated optimum {}\".format(best))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    'n_estimators': int(best['n_estimators']), \n",
    "    'max_depth': int(best['max_depth']),\n",
    "    'max_features': int(best['max_features']),\n",
    "    \"min_samples_split\":int(best['min_samples_split']),\n",
    "    \"min_samples_leaf\":int(best['min_samples_leaf']),\n",
    "    \"criterion\":available_criterion[int(best['criterion'])]\n",
    "}\n",
    "rf_hpo = RandomForestClassifier(**params)\n",
    "t3 = time.time()\n",
    "rf_hpo.fit(X_train,y_train)\n",
    "t4 = time.time()\n",
    "rf_score=rf_hpo.score(X_test,y_test)\n",
    "t5 = time.time()\n",
    "y_predict=rf_hpo.predict(X_test)\n",
    "t6 = time.time()\n",
    "y_true=y_test\n",
    "print('Accuracy of RF: '+ str(rf_score))\n",
    "precision,recall,fscore,none= precision_recall_fscore_support(y_true, y_predict, average='weighted') \n",
    "print('Precision of RF: '+(str(precision)))\n",
    "print('Recall of RF: '+(str(recall)))\n",
    "print('F1-score of RF: '+(str(fscore)))\n",
    "report_str = classification_report(y_true,y_predict); log_file.write('******{}******\\n'.format('RF (BO-TPE)')+report_str+'\\n'); print(report_str)\n",
    "cm=confusion_matrix(y_true,y_predict)\n",
    "f,ax=plt.subplots(figsize=(18,14))\n",
    "sns.heatmap(cm,annot=True,linewidth=0.5,linecolor=\"red\",fmt=\".0f\",ax=ax)\n",
    "ax.set_xticklabels(label_names)\n",
    "ax.set_yticklabels(list(reversed(label_names)))\n",
    "plt.xlabel(\"y_pred\")\n",
    "plt.ylabel(\"y_true\")\n",
    "plt.savefig(os.path.join(img_dir, 'RF_BO-TPE.pdf'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add to output sheet\n",
    "result_dict = {\n",
    "    'Accuracy': rf_score,\n",
    "    'Precision': precision,\n",
    "    'Recall': recall,\n",
    "    'F1-Score': fscore,\n",
    "    'HPO_time': t2-t1,\n",
    "    'Train_time': t4-t3,\n",
    "    'Predict_time_per_record': (t6-t5)/len_test\n",
    "}\n",
    "output_df = output_df.append(result_dict, ignore_index=True)\n",
    "# Add index name\n",
    "output_index.append('RF (BO-TPE)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rf_train=rf_hpo.predict(X_train)\n",
    "rf_test=rf_hpo.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter optimization (HPO) of random forest using Particle Swarm Optimization (PSO)\n",
    "Based on the GitHub repo for HPO: https://github.com/LiYangHart/Hyperparameter-Optimization-of-Machine-Learning-Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Forest\n",
    "import optunity\n",
    "import optunity.metrics\n",
    "\n",
    "data=X\n",
    "labels=y.tolist()\n",
    "Y_train = y_train\n",
    "Y_test = y_test\n",
    "# Define the hyperparameter configuration space\n",
    "search = {\n",
    "    'n_estimators': [10, 100],\n",
    "    'max_features': [1, 20],\n",
    "    'max_depth': [5,50],\n",
    "    \"min_samples_split\":[2,11],\n",
    "    \"min_samples_leaf\":[1,11],\n",
    "    \"criterion\":[0,1]\n",
    "         }\n",
    "available_criterion = ['gini', 'entropy']\n",
    "# Define the objective function\n",
    "@optunity.cross_validated(x=data, y=labels, num_folds=3)\n",
    "def performance(x_train, y_train, x_test, y_test,n_estimators=None, max_features=None,max_depth=None,min_samples_split=None,min_samples_leaf=None,criterion=None):\n",
    "    # fit the model\n",
    "    if criterion<0.5:\n",
    "        cri=available_criterion[0]\n",
    "    else:\n",
    "        cri=available_criterion[1]\n",
    "    model = RandomForestClassifier(n_estimators=int(n_estimators),\n",
    "                                   max_features=int(max_features),\n",
    "                                   max_depth=int(max_depth),\n",
    "                                   min_samples_split=int(min_samples_split),\n",
    "                                   min_samples_leaf=int(min_samples_leaf),\n",
    "                                   criterion=cri,\n",
    "                                  )\n",
    "    model.fit(X_train, Y_train)\n",
    "    predictions = model.predict(X_test)\n",
    "    # scores=np.mean(cross_val_score(model, X, y, cv=3, n_jobs=-1,\n",
    "    #                                 scoring=\"accuracy\"))\n",
    "    #return optunity.metrics.roc_auc(y_test, predictions, positive=True)\n",
    "    return optunity.metrics.accuracy(Y_test, predictions)\n",
    "\n",
    "t1 = time.time()\n",
    "\n",
    "optimal_configuration, info, _ = optunity.maximize(performance,\n",
    "                                                  solver_name='particle swarm',\n",
    "                                                  num_evals=20,\n",
    "                                                   **search\n",
    "                                                  )\n",
    "\n",
    "t2 = time.time()\n",
    "\n",
    "print(optimal_configuration)\n",
    "print(\"Accuracy:\"+ str(info.optimum))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'n_estimators': int(optimal_configuration['n_estimators']), \n",
    "    'min_samples_leaf': int(optimal_configuration['min_samples_leaf']), \n",
    "    'max_depth': int(optimal_configuration['max_depth']), \n",
    "    'min_samples_split': int(optimal_configuration['min_samples_split']), \n",
    "    'max_features': int(optimal_configuration['max_features']), \n",
    "    'criterion': available_criterion[int(optimal_configuration['criterion']+0.5)]\n",
    "}\n",
    "rf_hpo = RandomForestClassifier(**params)\n",
    "t3 = time.time()\n",
    "rf_hpo.fit(X_train,y_train)\n",
    "t4 = time.time()\n",
    "rf_score=rf_hpo.score(X_test,y_test)\n",
    "t5 = time.time()\n",
    "y_predict=rf_hpo.predict(X_test)\n",
    "t6 = time.time()\n",
    "y_true=y_test\n",
    "print('Accuracy of RF: '+ str(rf_score))\n",
    "precision,recall,fscore,none= precision_recall_fscore_support(y_true, y_predict, average='weighted') \n",
    "print('Precision of RF: '+(str(precision)))\n",
    "print('Recall of RF: '+(str(recall)))\n",
    "print('F1-score of RF: '+(str(fscore)))\n",
    "report_str = classification_report(y_true,y_predict); log_file.write('******{}******\\n'.format('RF (PSO)')+report_str+'\\n'); print(report_str)\n",
    "cm=confusion_matrix(y_true,y_predict)\n",
    "f,ax=plt.subplots(figsize=(18,14))\n",
    "sns.heatmap(cm,annot=True,linewidth=0.5,linecolor=\"red\",fmt=\".0f\",ax=ax)\n",
    "ax.set_xticklabels(label_names)\n",
    "ax.set_yticklabels(list(reversed(label_names)))\n",
    "plt.xlabel(\"y_pred\")\n",
    "plt.ylabel(\"y_true\")\n",
    "plt.savefig(os.path.join(img_dir, 'RF_PSO.pdf'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add to output sheet\n",
    "result_dict = {\n",
    "    'Accuracy': rf_score,\n",
    "    'Precision': precision,\n",
    "    'Recall': recall,\n",
    "    'F1-Score': fscore,\n",
    "    'HPO_time': t2-t1,\n",
    "    'Train_time': t4-t3,\n",
    "    'Predict_time_per_record': (t6-t5)/len_test\n",
    "}\n",
    "output_df = output_df.append(result_dict, ignore_index=True)\n",
    "# Add index name\n",
    "output_index.append('RF (PSO)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter optimization (HPO) of random forest using Genetic Algorithm (GA)\n",
    "Based on the GitHub repo for HPO: https://github.com/LiYangHart/Hyperparameter-Optimization-of-Machine-Learning-Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Forest\n",
    "from tpot import TPOTClassifier\n",
    "# Define the hyperparameter configuration space\n",
    "parameters = {\n",
    "    'n_estimators': range(20,200),\n",
    "    \"max_features\":range(1,20),\n",
    "    'max_depth': range(10,100),\n",
    "    \"min_samples_split\":range(2,11),\n",
    "    \"min_samples_leaf\":range(1,11),\n",
    "    \"criterion\":['gini','entropy']\n",
    "             }\n",
    "# Set the hyperparameters of GA                 \n",
    "ga = TPOTClassifier(generations= 3, population_size= 10, offspring_size= 5,\n",
    "                                 verbosity= 3, early_stop= 5,\n",
    "                                 config_dict=\n",
    "                                 {'sklearn.ensemble.RandomForestClassifier': parameters}, \n",
    "                                 cv = 3, scoring = 'accuracy')\n",
    "t1 = time.time()\n",
    "ga.fit(X_combined, y_combined)\n",
    "t2 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_hpo = RandomForestClassifier(**get_ga_optimized_parameters(ga, 'RandomForestClassifier'))\n",
    "t3 = time.time()\n",
    "rf_hpo.fit(X_train,y_train)\n",
    "t4 = time.time()\n",
    "rf_score=rf_hpo.score(X_test,y_test)\n",
    "t5 = time.time()\n",
    "y_predict=rf_hpo.predict(X_test)\n",
    "t6 = time.time()\n",
    "y_true=y_test\n",
    "print('Accuracy of RF: '+ str(rf_score))\n",
    "precision,recall,fscore,none= precision_recall_fscore_support(y_true, y_predict, average='weighted') \n",
    "print('Precision of RF: '+(str(precision)))\n",
    "print('Recall of RF: '+(str(recall)))\n",
    "print('F1-score of RF: '+(str(fscore)))\n",
    "report_str = classification_report(y_true,y_predict); log_file.write('******{}******\\n'.format('RF (GA)')+report_str+'\\n'); print(report_str)\n",
    "cm=confusion_matrix(y_true,y_predict)\n",
    "f,ax=plt.subplots(figsize=(18,14))\n",
    "sns.heatmap(cm,annot=True,linewidth=0.5,linecolor=\"red\",fmt=\".0f\",ax=ax)\n",
    "ax.set_xticklabels(label_names)\n",
    "ax.set_yticklabels(list(reversed(label_names)))\n",
    "plt.xlabel(\"y_pred\")\n",
    "plt.ylabel(\"y_true\")\n",
    "plt.savefig(os.path.join(img_dir, 'RF_GA.pdf'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add to output sheet\n",
    "result_dict = {\n",
    "    'Accuracy': rf_score,\n",
    "    'Precision': precision,\n",
    "    'Recall': recall,\n",
    "    'F1-Score': fscore,\n",
    "    'HPO_time': t2-t1,\n",
    "    'Train_time': t4-t3,\n",
    "    'Predict_time_per_record': (t6-t5)/len_test\n",
    "}\n",
    "output_df = output_df.append(result_dict, ignore_index=True)\n",
    "# Add index name\n",
    "output_index.append('RF (GA)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply DT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier(random_state = 0)\n",
    "t1 = time.time()\n",
    "dt.fit(X_train,y_train)\n",
    "t2 = time.time() \n",
    "dt_score=dt.score(X_test,y_test)\n",
    "t3 = time.time()\n",
    "y_predict=dt.predict(X_test)\n",
    "t4 = time.time()\n",
    "y_true=y_test\n",
    "print('Accuracy of DT: '+ str(dt_score))\n",
    "precision,recall,fscore,none= precision_recall_fscore_support(y_true, y_predict, average='weighted') \n",
    "print('Precision of DT: '+(str(precision)))\n",
    "print('Recall of DT: '+(str(recall)))\n",
    "print('F1-score of DT: '+(str(fscore)))\n",
    "report_str = classification_report(y_true,y_predict); log_file.write('******{}******\\n'.format('DT (Original)')+report_str+'\\n'); print(report_str)\n",
    "cm=confusion_matrix(y_true,y_predict)\n",
    "f,ax=plt.subplots(figsize=(18,14))\n",
    "sns.heatmap(cm,annot=True,linewidth=0.5,linecolor=\"red\",fmt=\".0f\",ax=ax)\n",
    "ax.set_xticklabels(label_names)\n",
    "ax.set_yticklabels(list(reversed(label_names)))\n",
    "plt.xlabel(\"y_pred\")\n",
    "plt.ylabel(\"y_true\")\n",
    "plt.savefig(os.path.join(img_dir, 'DT_original.pdf'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add to output sheet\n",
    "result_dict = {\n",
    "    'Accuracy': dt_score,\n",
    "    'Precision': precision,\n",
    "    'Recall': recall,\n",
    "    'F1-Score': fscore,\n",
    "    'Train_time': t2-t1,\n",
    "    'Predict_time_per_record': (t4-t3)/len_test,\n",
    "    'HPO_time': np.NaN\n",
    "}\n",
    "output_df = output_df.append(result_dict, ignore_index=True)\n",
    "# Add index name\n",
    "output_index.append('DT (Original)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter optimization (HPO) of decision tree using Bayesian optimization with tree-based Parzen estimator (BO-TPE)\n",
    "Based on the GitHub repo for HPO: https://github.com/LiYangHart/Hyperparameter-Optimization-of-Machine-Learning-Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Hyperparameter optimization of decision tree\n",
    "from hyperopt import hp, fmin, tpe, STATUS_OK, Trials\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "# Define the objective function\n",
    "def objective(params):\n",
    "    params = {\n",
    "        'max_depth': int(params['max_depth']),\n",
    "        'max_features': int(params['max_features']),\n",
    "        \"min_samples_split\":int(params['min_samples_split']),\n",
    "        \"min_samples_leaf\":int(params['min_samples_leaf']),\n",
    "        \"criterion\":str(params['criterion'])\n",
    "    }\n",
    "    clf = DecisionTreeClassifier( **params)\n",
    "    clf.fit(X_train,y_train)\n",
    "    score=clf.score(X_test,y_test)\n",
    "\n",
    "    return {'loss':-score, 'status': STATUS_OK }\n",
    "# Define the hyperparameter configuration space\n",
    "available_criterion = ['gini','entropy']\n",
    "space = {\n",
    "    'max_depth': hp.quniform('max_depth', 5, 50, 1),\n",
    "    \"max_features\":hp.quniform('max_features', 1, 20, 1),\n",
    "    \"min_samples_split\":hp.quniform('min_samples_split',2,11,1),\n",
    "    \"min_samples_leaf\":hp.quniform('min_samples_leaf',1,11,1),\n",
    "    \"criterion\":hp.choice('criterion',available_criterion)\n",
    "}\n",
    "\n",
    "t1 = time.time()\n",
    "\n",
    "best = fmin(fn=objective,\n",
    "            space=space,\n",
    "            algo=tpe.suggest,\n",
    "            max_evals=50)\n",
    "\n",
    "t2 = time.time()\n",
    "\n",
    "print(\"Decision tree: Hyperopt estimated optimum {}\".format(best))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    'max_depth': int(best['max_depth']),\n",
    "    'max_features': int(best['max_features']),\n",
    "    \"min_samples_split\":int(best['min_samples_split']),\n",
    "    \"min_samples_leaf\":int(best['min_samples_leaf']),\n",
    "    \"criterion\":available_criterion[int(best['criterion'])]\n",
    "}\n",
    "dt_hpo = DecisionTreeClassifier(**params)\n",
    "t3 = time.time()\n",
    "dt_hpo.fit(X_train,y_train)\n",
    "t4 = time.time()\n",
    "dt_score=dt_hpo.score(X_test,y_test)\n",
    "t5 = time.time()\n",
    "y_predict=dt_hpo.predict(X_test)\n",
    "t6 = time.time()\n",
    "y_true=y_test\n",
    "print('Accuracy of DT: '+ str(dt_score))\n",
    "precision,recall,fscore,none= precision_recall_fscore_support(y_true, y_predict, average='weighted') \n",
    "print('Precision of DT: '+(str(precision)))\n",
    "print('Recall of DT: '+(str(recall)))\n",
    "print('F1-score of DT: '+(str(fscore)))\n",
    "report_str = classification_report(y_true,y_predict); log_file.write('******{}******\\n'.format('DT (BO-TPE)')+report_str+'\\n'); print(report_str)\n",
    "cm=confusion_matrix(y_true,y_predict)\n",
    "f,ax=plt.subplots(figsize=(18,14))\n",
    "sns.heatmap(cm,annot=True,linewidth=0.5,linecolor=\"red\",fmt=\".0f\",ax=ax)\n",
    "ax.set_xticklabels(label_names)\n",
    "ax.set_yticklabels(list(reversed(label_names)))\n",
    "plt.xlabel(\"y_pred\")\n",
    "plt.ylabel(\"y_true\")\n",
    "plt.savefig(os.path.join(img_dir, 'DT_BO-TPE.pdf'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add to output sheet\n",
    "result_dict = {\n",
    "    'Accuracy': dt_score,\n",
    "    'Precision': precision,\n",
    "    'Recall': recall,\n",
    "    'F1-Score': fscore,\n",
    "    'HPO_time': t2-t1,\n",
    "    'Train_time': t4-t3,\n",
    "    'Predict_time_per_record': (t6-t5)/len_test\n",
    "}\n",
    "output_df = output_df.append(result_dict, ignore_index=True)\n",
    "# Add index name\n",
    "output_index.append('DT (BO-TPE)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dt_train=dt_hpo.predict(X_train)\n",
    "dt_test=dt_hpo.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter optimization (HPO) of decision tree using Particle Swarm Optimization (PSO)\n",
    "Based on the GitHub repo for HPO: https://github.com/LiYangHart/Hyperparameter-Optimization-of-Machine-Learning-Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Forest\n",
    "import optunity\n",
    "import optunity.metrics\n",
    "\n",
    "data=X_train\n",
    "labels=y_train.tolist()\n",
    "Y_train = y_train\n",
    "Y_test = y_test\n",
    "# Define the hyperparameter configuration space\n",
    "search = {\n",
    "    'max_features': [1, 20],\n",
    "    'max_depth': [5,50],\n",
    "    \"min_samples_split\":[2,11],\n",
    "    \"min_samples_leaf\":[1,11],\n",
    "    \"criterion\":[0,1]\n",
    "}\n",
    "available_criterion = ['gini', 'entropy']\n",
    "# Define the objective function\n",
    "@optunity.cross_validated(x=data, y=labels, num_folds=3)\n",
    "def performance(x_train, y_train, x_test, y_test,max_features=None,max_depth=None,min_samples_split=None,min_samples_leaf=None,criterion=None):\n",
    "    # fit the model\n",
    "    if criterion<0.5:\n",
    "        cri=available_criterion[0]\n",
    "    else:\n",
    "        cri=available_criterion[1]\n",
    "    model = DecisionTreeClassifier(max_features=int(max_features),\n",
    "                                   max_depth=int(max_depth),\n",
    "                                   min_samples_split=int(min_samples_split),\n",
    "                                   min_samples_leaf=int(min_samples_leaf),\n",
    "                                   criterion=cri,\n",
    "                                  )\n",
    "    model.fit(X_train, Y_train)\n",
    "    predictions = model.predict(X_test)\n",
    "    # scores=np.mean(cross_val_score(model, X, y, cv=3, n_jobs=-1,\n",
    "                                    # scoring=\"accuracy\"))\n",
    "    # return optunity.metrics.roc_auc(y_test, predictions, positive=True)\n",
    "    return optunity.metrics.accuracy(Y_test, predictions)\n",
    "\n",
    "t1 = time.time()\n",
    "\n",
    "optimal_configuration, info, _ = optunity.maximize(performance,\n",
    "                                                  solver_name='particle swarm',\n",
    "                                                  num_evals=20,\n",
    "                                                   **search\n",
    "                                                  )\n",
    "\n",
    "t2 = time.time()\n",
    "\n",
    "print(optimal_configuration)\n",
    "print(\"Accuracy:\"+ str(info.optimum))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'min_samples_leaf': int(optimal_configuration['min_samples_leaf']), \n",
    "    'max_depth': int(optimal_configuration['max_depth']), \n",
    "    'min_samples_split': int(optimal_configuration['min_samples_split']), \n",
    "    'max_features': int(optimal_configuration['max_features']), \n",
    "    'criterion': available_criterion[int(optimal_configuration['criterion']+0.5)]\n",
    "}\n",
    "dt_hpo = DecisionTreeClassifier(**params)\n",
    "t3 = time.time()\n",
    "dt_hpo.fit(X_train,y_train)\n",
    "t4 = time.time()\n",
    "dt_score=dt_hpo.score(X_test,y_test)\n",
    "t5 = time.time()\n",
    "y_predict=dt_hpo.predict(X_test)\n",
    "t6 = time.time()\n",
    "y_true=y_test\n",
    "print('Accuracy of DT: '+ str(dt_score))\n",
    "precision,recall,fscore,none= precision_recall_fscore_support(y_true, y_predict, average='weighted') \n",
    "print('Precision of DT: '+(str(precision)))\n",
    "print('Recall of DT: '+(str(recall)))\n",
    "print('F1-score of DT: '+(str(fscore)))\n",
    "report_str = classification_report(y_true,y_predict); log_file.write('******{}******\\n'.format('DT (PSO)')+report_str+'\\n'); print(report_str)\n",
    "cm=confusion_matrix(y_true,y_predict)\n",
    "f,ax=plt.subplots(figsize=(18,14))\n",
    "sns.heatmap(cm,annot=True,linewidth=0.5,linecolor=\"red\",fmt=\".0f\",ax=ax)\n",
    "ax.set_xticklabels(label_names)\n",
    "ax.set_yticklabels(list(reversed(label_names)))\n",
    "plt.xlabel(\"y_pred\")\n",
    "plt.ylabel(\"y_true\")\n",
    "plt.savefig(os.path.join(img_dir, 'DT_PSO.pdf'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add to output sheet\n",
    "result_dict = {\n",
    "    'Accuracy': dt_score,\n",
    "    'Precision': precision,\n",
    "    'Recall': recall,\n",
    "    'F1-Score': fscore,\n",
    "    'HPO_time': t2-t1,\n",
    "    'Train_time': t4-t3,\n",
    "    'Predict_time_per_record': (t6-t5)/len_test\n",
    "}\n",
    "output_df = output_df.append(result_dict, ignore_index=True)\n",
    "# Add index name\n",
    "output_index.append('DT (PSO)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter optimization (HPO) of decision tree using Genetic Algorithm (GA)\n",
    "Based on the GitHub repo for HPO: https://github.com/LiYangHart/Hyperparameter-Optimization-of-Machine-Learning-Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Forest\n",
    "from tpot import TPOTClassifier\n",
    "# Define the hyperparameter configuration space\n",
    "parameters = {\n",
    "    \"max_features\":range(1,20),\n",
    "    'max_depth': range(10,100),\n",
    "    \"min_samples_split\":range(2,11),\n",
    "    \"min_samples_leaf\":range(1,11),\n",
    "    \"criterion\":['gini','entropy']\n",
    "}\n",
    "# Set the hyperparameters of GA                 \n",
    "ga = TPOTClassifier(generations= 3, population_size= 10, offspring_size= 5,\n",
    "                                 verbosity= 3, early_stop= 5,\n",
    "                                 config_dict=\n",
    "                                 {'sklearn.tree.DecisionTreeClassifier': parameters}, \n",
    "                                 cv = 3, scoring = 'accuracy')\n",
    "t1 = time.time()\n",
    "ga.fit(X_combined, y_combined)\n",
    "t2 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_hpo = DecisionTreeClassifier(**get_ga_optimized_parameters(ga, 'DecisionTreeClassifier'))\n",
    "t3 = time.time()\n",
    "dt_hpo.fit(X_train,y_train)\n",
    "t4 = time.time()\n",
    "dt_score=dt_hpo.score(X_test,y_test)\n",
    "t5 = time.time()\n",
    "y_predict=dt_hpo.predict(X_test)\n",
    "t6 = time.time()\n",
    "y_true=y_test\n",
    "print('Accuracy of DT: '+ str(dt_score))\n",
    "precision,recall,fscore,none= precision_recall_fscore_support(y_true, y_predict, average='weighted') \n",
    "print('Precision of DT: '+(str(precision)))\n",
    "print('Recall of DT: '+(str(recall)))\n",
    "print('F1-score of DT: '+(str(fscore)))\n",
    "report_str = classification_report(y_true,y_predict); log_file.write('******{}******\\n'.format('DT (GA)')+report_str+'\\n'); print(report_str)\n",
    "cm=confusion_matrix(y_true,y_predict)\n",
    "f,ax=plt.subplots(figsize=(18,14))\n",
    "sns.heatmap(cm,annot=True,linewidth=0.5,linecolor=\"red\",fmt=\".0f\",ax=ax)\n",
    "ax.set_xticklabels(label_names)\n",
    "ax.set_yticklabels(list(reversed(label_names)))\n",
    "plt.xlabel(\"y_pred\")\n",
    "plt.ylabel(\"y_true\")\n",
    "plt.savefig(os.path.join(img_dir, 'DT_GA.pdf'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add to output sheet\n",
    "result_dict = {\n",
    "    'Accuracy': dt_score,\n",
    "    'Precision': precision,\n",
    "    'Recall': recall,\n",
    "    'F1-Score': fscore,\n",
    "    'HPO_time': t2-t1,\n",
    "    'Train_time': t4-t3,\n",
    "    'Predict_time_per_record': (t6-t5)/len_test\n",
    "}\n",
    "output_df = output_df.append(result_dict, ignore_index=True)\n",
    "# Add index name\n",
    "output_index.append('DT (GA)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "et = ExtraTreesClassifier(random_state = 0)\n",
    "t1 = time.time()\n",
    "et.fit(X_train,y_train)\n",
    "t2 = time.time() \n",
    "et_score=et.score(X_test,y_test)\n",
    "t3 = time.time()\n",
    "y_predict=et.predict(X_test)\n",
    "t4 = time.time()\n",
    "y_true=y_test\n",
    "print('Accuracy of ET: '+ str(et_score))\n",
    "precision,recall,fscore,none= precision_recall_fscore_support(y_true, y_predict, average='weighted') \n",
    "print('Precision of ET: '+(str(precision)))\n",
    "print('Recall of ET: '+(str(recall)))\n",
    "print('F1-score of ET: '+(str(fscore)))\n",
    "report_str = classification_report(y_true,y_predict); log_file.write('******{}******\\n'.format('ET (Original)')+report_str+'\\n'); print(report_str)\n",
    "cm=confusion_matrix(y_true,y_predict)\n",
    "f,ax=plt.subplots(figsize=(18,14))\n",
    "sns.heatmap(cm,annot=True,linewidth=0.5,linecolor=\"red\",fmt=\".0f\",ax=ax)\n",
    "ax.set_xticklabels(label_names)\n",
    "ax.set_yticklabels(list(reversed(label_names)))\n",
    "plt.xlabel(\"y_pred\")\n",
    "plt.ylabel(\"y_true\")\n",
    "plt.savefig(os.path.join(img_dir, 'ET_original.pdf'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add to output sheet\n",
    "result_dict = {\n",
    "    'Accuracy': et_score,\n",
    "    'Precision': precision,\n",
    "    'Recall': recall,\n",
    "    'F1-Score': fscore,\n",
    "    'Train_time': t2-t1,\n",
    "    'Predict_time_per_record': (t4-t3)/len_test,\n",
    "    'HPO_time': np.NaN\n",
    "}\n",
    "output_df = output_df.append(result_dict, ignore_index=True)\n",
    "# Add index name\n",
    "output_index.append('ET (Original)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter optimization (HPO) of extra trees using Bayesian optimization with tree-based Parzen estimator (BO-TPE)\n",
    "Based on the GitHub repo for HPO: https://github.com/LiYangHart/Hyperparameter-Optimization-of-Machine-Learning-Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Hyperparameter optimization of extra trees\n",
    "from hyperopt import hp, fmin, tpe, STATUS_OK, Trials\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "# Define the objective function\n",
    "def objective(params):\n",
    "    params = {\n",
    "        'n_estimators': int(params['n_estimators']), \n",
    "        'max_depth': int(params['max_depth']),\n",
    "        'max_features': int(params['max_features']),\n",
    "        \"min_samples_split\":int(params['min_samples_split']),\n",
    "        \"min_samples_leaf\":int(params['min_samples_leaf']),\n",
    "        \"criterion\":str(params['criterion'])\n",
    "    }\n",
    "    clf = ExtraTreesClassifier( **params)\n",
    "    clf.fit(X_train,y_train)\n",
    "    score=clf.score(X_test,y_test)\n",
    "\n",
    "    return {'loss':-score, 'status': STATUS_OK }\n",
    "# Define the hyperparameter configuration space\n",
    "available_criterion = ['gini','entropy']\n",
    "space = {\n",
    "    'n_estimators': hp.quniform('n_estimators', 10, 200, 1),\n",
    "    'max_depth': hp.quniform('max_depth', 5, 50, 1),\n",
    "    \"max_features\":hp.quniform('max_features', 1, 20, 1),\n",
    "    \"min_samples_split\":hp.quniform('min_samples_split',2,11,1),\n",
    "    \"min_samples_leaf\":hp.quniform('min_samples_leaf',1,11,1),\n",
    "    \"criterion\":hp.choice('criterion',available_criterion)\n",
    "}\n",
    "\n",
    "t1 = time.time()\n",
    "\n",
    "best = fmin(fn=objective,\n",
    "            space=space,\n",
    "            algo=tpe.suggest,\n",
    "            max_evals=20)\n",
    "\n",
    "t2 = time.time()\n",
    "\n",
    "print(\"Random Forest: Hyperopt estimated optimum {}\".format(best))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    'n_estimators': int(best['n_estimators']), \n",
    "    'max_depth': int(best['max_depth']),\n",
    "    'max_features': int(best['max_features']),\n",
    "    \"min_samples_split\":int(best['min_samples_split']),\n",
    "    \"min_samples_leaf\":int(best['min_samples_leaf']),\n",
    "    \"criterion\":available_criterion[int(best['criterion'])]\n",
    "}\n",
    "et_hpo = ExtraTreesClassifier(**params)\n",
    "t3 = time.time()\n",
    "et_hpo.fit(X_train,y_train) \n",
    "t4 = time.time()\n",
    "et_score=et_hpo.score(X_test,y_test)\n",
    "t5 = time.time()\n",
    "y_predict=et_hpo.predict(X_test)\n",
    "t6 = time.time()\n",
    "y_true=y_test\n",
    "print('Accuracy of ET: '+ str(et_score))\n",
    "precision,recall,fscore,none= precision_recall_fscore_support(y_true, y_predict, average='weighted') \n",
    "print('Precision of ET: '+(str(precision)))\n",
    "print('Recall of ET: '+(str(recall)))\n",
    "print('F1-score of ET: '+(str(fscore)))\n",
    "report_str = classification_report(y_true,y_predict); log_file.write('******{}******\\n'.format('ET (BO-TPE)')+report_str+'\\n'); print(report_str)\n",
    "cm=confusion_matrix(y_true,y_predict)\n",
    "f,ax=plt.subplots(figsize=(18,14))\n",
    "sns.heatmap(cm,annot=True,linewidth=0.5,linecolor=\"red\",fmt=\".0f\",ax=ax)\n",
    "ax.set_xticklabels(label_names)\n",
    "ax.set_yticklabels(list(reversed(label_names)))\n",
    "plt.xlabel(\"y_pred\")\n",
    "plt.ylabel(\"y_true\")\n",
    "plt.savefig(os.path.join(img_dir, 'ET_BO-TPE.pdf'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add to output sheet\n",
    "result_dict = {\n",
    "    'Accuracy': et_score,\n",
    "    'Precision': precision,\n",
    "    'Recall': recall,\n",
    "    'F1-Score': fscore,\n",
    "    'HPO_time': t2-t1,\n",
    "    'Train_time': t4-t3,\n",
    "    'Predict_time_per_record': (t6-t5)/len_test\n",
    "}\n",
    "output_df = output_df.append(result_dict, ignore_index=True)\n",
    "# Add index name\n",
    "output_index.append('ET (BO-TPE)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "et_train=et_hpo.predict(X_train)\n",
    "et_test=et_hpo.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter optimization (HPO) of extra trees using Particle Swarm Optimization (PSO)\n",
    "Based on the GitHub repo for HPO: https://github.com/LiYangHart/Hyperparameter-Optimization-of-Machine-Learning-Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Forest\n",
    "import optunity\n",
    "import optunity.metrics\n",
    "\n",
    "data=X_train\n",
    "labels=y_train.tolist()\n",
    "Y_train = y_train\n",
    "Y_test = y_test\n",
    "# Define the hyperparameter configuration space\n",
    "search = {\n",
    "    'n_estimators': [10, 200],\n",
    "    'max_features': [1, 20],\n",
    "    'max_depth': [5,50],\n",
    "    \"min_samples_split\":[2,11],\n",
    "    \"min_samples_leaf\":[1,11],\n",
    "    \"criterion\":[0,1]\n",
    "}\n",
    "available_criterion = ['gini', 'entropy']\n",
    "# Define the objective function\n",
    "@optunity.cross_validated(x=data, y=labels, num_folds=3)\n",
    "def performance(x_train, y_train, x_test, y_test,n_estimators=None,max_features=None,max_depth=None,min_samples_split=None,min_samples_leaf=None,criterion=None):\n",
    "    # fit the model\n",
    "    if criterion<0.5:\n",
    "        cri=available_criterion[0]\n",
    "    else:\n",
    "        cri=available_criterion[1]\n",
    "    model = ExtraTreesClassifier(n_estimators=int(n_estimators),\n",
    "                                   max_features=int(max_features),\n",
    "                                   max_depth=int(max_depth),\n",
    "                                   min_samples_split=int(min_samples_split),\n",
    "                                   min_samples_leaf=int(min_samples_leaf),\n",
    "                                   criterion=cri,\n",
    "                                  )\n",
    "    model.fit(X_train, Y_train)\n",
    "    predictions = model.predict(X_test)\n",
    "    # scores=np.mean(cross_val_score(model, X, y, cv=3, n_jobs=-1,\n",
    "                                    # scoring=\"accuracy\"))\n",
    "    # return optunity.metrics.roc_auc(y_test, predictions, positive=True)\n",
    "    return optunity.metrics.accuracy(Y_test, predictions)\n",
    "\n",
    "t1 = time.time()\n",
    "\n",
    "optimal_configuration, info, _ = optunity.maximize(performance,\n",
    "                                                  solver_name='particle swarm',\n",
    "                                                  num_evals=20,\n",
    "                                                   **search\n",
    "                                                  )\n",
    "\n",
    "t2 = time.time()\n",
    "\n",
    "print(optimal_configuration)\n",
    "print(\"Accuracy:\"+ str(info.optimum))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'n_estimators': int(optimal_configuration['n_estimators']), \n",
    "    'min_samples_leaf': int(optimal_configuration['min_samples_leaf']), \n",
    "    'max_depth': int(optimal_configuration['max_depth']), \n",
    "    'min_samples_split': int(optimal_configuration['min_samples_split']), \n",
    "    'max_features': int(optimal_configuration['max_features']), \n",
    "    'criterion': available_criterion[int(optimal_configuration['criterion']+0.5)]\n",
    "}\n",
    "et_hpo = ExtraTreesClassifier(**params)\n",
    "t3 = time.time()\n",
    "et_hpo.fit(X_train,y_train)\n",
    "t4 = time.time() \n",
    "et_score=et_hpo.score(X_test,y_test)\n",
    "t5 = time.time()\n",
    "y_predict=et_hpo.predict(X_test)\n",
    "t6 = time.time()\n",
    "y_true=y_test\n",
    "print('Accuracy of ET: '+ str(et_score))\n",
    "precision,recall,fscore,none= precision_recall_fscore_support(y_true, y_predict, average='weighted') \n",
    "print('Precision of ET: '+(str(precision)))\n",
    "print('Recall of ET: '+(str(recall)))\n",
    "print('F1-score of ET: '+(str(fscore)))\n",
    "report_str = classification_report(y_true,y_predict); log_file.write('******{}******\\n'.format('ET (PSO)')+report_str+'\\n'); print(report_str)\n",
    "cm=confusion_matrix(y_true,y_predict)\n",
    "f,ax=plt.subplots(figsize=(18,14))\n",
    "sns.heatmap(cm,annot=True,linewidth=0.5,linecolor=\"red\",fmt=\".0f\",ax=ax)\n",
    "ax.set_xticklabels(label_names)\n",
    "ax.set_yticklabels(list(reversed(label_names)))\n",
    "plt.xlabel(\"y_pred\")\n",
    "plt.ylabel(\"y_true\")\n",
    "plt.savefig(os.path.join(img_dir, 'ET_PSO.pdf'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add to output sheet\n",
    "result_dict = {\n",
    "    'Accuracy': et_score,\n",
    "    'Precision': precision,\n",
    "    'Recall': recall,\n",
    "    'F1-Score': fscore,\n",
    "    'HPO_time': t2-t1,\n",
    "    'Train_time': t4-t3,\n",
    "    'Predict_time_per_record': (t6-t5)/len_test\n",
    "}\n",
    "output_df = output_df.append(result_dict, ignore_index=True)\n",
    "# Add index name\n",
    "output_index.append('ET (PSO)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter optimization (HPO) of extra trees using Genetic Algorithm (GA)\n",
    "Based on the GitHub repo for HPO: https://github.com/LiYangHart/Hyperparameter-Optimization-of-Machine-Learning-Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Forest\n",
    "from tpot import TPOTClassifier\n",
    "# Define the hyperparameter configuration space\n",
    "parameters = {\n",
    "    'n_estimators': range(20,200),\n",
    "    \"max_features\":range(1,20),\n",
    "    'max_depth': range(10,100),\n",
    "    \"min_samples_split\":range(2,11),\n",
    "    \"min_samples_leaf\":range(1,11),\n",
    "    \"criterion\":['gini','entropy']\n",
    "             }\n",
    "# Set the hyperparameters of GA                 \n",
    "ga = TPOTClassifier(generations= 3, population_size= 10, offspring_size= 5,\n",
    "                                 verbosity= 3, early_stop= 5,\n",
    "                                 config_dict=\n",
    "                                 {'sklearn.ensemble.ExtraTreesClassifier': parameters}, \n",
    "                                 cv = 3, scoring = 'accuracy')\n",
    "t1 = time.time()\n",
    "ga.fit(X_combined, y_combined)\n",
    "t2 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "et_hpo = ExtraTreesClassifier(**get_ga_optimized_parameters(ga, 'ExtraTreesClassifier'))\n",
    "t3 = time.time()\n",
    "et_hpo.fit(X_train,y_train)\n",
    "t4 = time.time() \n",
    "et_score=et_hpo.score(X_test,y_test)\n",
    "t5 = time.time()\n",
    "y_predict=et_hpo.predict(X_test)\n",
    "t6 = time.time()\n",
    "y_true=y_test\n",
    "print('Accuracy of ET: '+ str(et_score))\n",
    "precision,recall,fscore,none= precision_recall_fscore_support(y_true, y_predict, average='weighted') \n",
    "print('Precision of ET: '+(str(precision)))\n",
    "print('Recall of ET: '+(str(recall)))\n",
    "print('F1-score of ET: '+(str(fscore)))\n",
    "report_str = classification_report(y_true,y_predict); log_file.write('******{}******\\n'.format('ET (GA)')+report_str+'\\n'); print(report_str)\n",
    "cm=confusion_matrix(y_true,y_predict)\n",
    "f,ax=plt.subplots(figsize=(18,14))\n",
    "sns.heatmap(cm,annot=True,linewidth=0.5,linecolor=\"red\",fmt=\".0f\",ax=ax)\n",
    "ax.set_xticklabels(label_names)\n",
    "ax.set_yticklabels(list(reversed(label_names)))\n",
    "plt.xlabel(\"y_pred\")\n",
    "plt.ylabel(\"y_true\")\n",
    "plt.savefig(os.path.join(img_dir, 'ET_GA.pdf'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add to output sheet\n",
    "result_dict = {\n",
    "    'Accuracy': et_score,\n",
    "    'Precision': precision,\n",
    "    'Recall': recall,\n",
    "    'F1-Score': fscore,\n",
    "    'HPO_time': t2-t1,\n",
    "    'Train_time': t4-t3,\n",
    "    'Predict_time_per_record': (t6-t5)/len_test\n",
    "}\n",
    "output_df = output_df.append(result_dict, ignore_index=True)\n",
    "# Add index name\n",
    "output_index.append('ET (GA)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "base_predictions_train = pd.DataFrame( {\n",
    "    'DecisionTree': dt_train.ravel(),\n",
    "    'RandomForest': rf_train.ravel(),\n",
    "    'ExtraTrees': et_train.ravel(),\n",
    "    'XgBoost': xg_train.ravel(),\n",
    "    })\n",
    "base_predictions_train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dt_train=dt_train.reshape(-1, 1)\n",
    "et_train=et_train.reshape(-1, 1)\n",
    "rf_train=rf_train.reshape(-1, 1)\n",
    "xg_train=xg_train.reshape(-1, 1)\n",
    "dt_test=dt_test.reshape(-1, 1)\n",
    "et_test=et_test.reshape(-1, 1)\n",
    "rf_test=rf_test.reshape(-1, 1)\n",
    "xg_test=xg_test.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dt_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_train = np.concatenate(( dt_train, et_train, rf_train, xg_train), axis=1)\n",
    "x_test = np.concatenate(( dt_test, et_test, rf_test, xg_test), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t1 = time.time()\n",
    "stk = xgb.XGBClassifier().fit(x_train, y_train)\n",
    "t2 = time.time()\n",
    "y_predict=stk.predict(x_test)\n",
    "t3 = time.time()\n",
    "y_true=y_test\n",
    "stk_score=accuracy_score(y_true,y_predict)\n",
    "print('Accuracy of Stacking: '+ str(stk_score))\n",
    "precision,recall,fscore,none= precision_recall_fscore_support(y_true, y_predict, average='weighted') \n",
    "print('Precision of Stacking: '+(str(precision)))\n",
    "print('Recall of Stacking: '+(str(recall)))\n",
    "print('F1-score of Stacking: '+(str(fscore)))\n",
    "report_str = classification_report(y_true,y_predict); log_file.write('******{}******\\n'.format('Stacking (Original)')+report_str+'\\n'); print(report_str)\n",
    "cm=confusion_matrix(y_true,y_predict)\n",
    "f,ax=plt.subplots(figsize=(18,14))\n",
    "sns.heatmap(cm,annot=True,linewidth=0.5,linecolor=\"red\",fmt=\".0f\",ax=ax)\n",
    "ax.set_xticklabels(label_names)\n",
    "ax.set_yticklabels(list(reversed(label_names)))\n",
    "plt.xlabel(\"y_pred\")\n",
    "plt.ylabel(\"y_true\")\n",
    "plt.savefig(os.path.join(img_dir, 'Stacking_original.pdf'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add to output sheet\n",
    "result_dict = {\n",
    "    'Accuracy': stk_score,\n",
    "    'Precision': precision,\n",
    "    'Recall': recall,\n",
    "    'F1-Score': fscore,\n",
    "    'Train_time': t2-t1,\n",
    "    'Predict_time_per_record': (t3-t2)/len_test\n",
    "}\n",
    "output_df = output_df.append(result_dict, ignore_index=True)\n",
    "# Add index name\n",
    "output_index.append('Stacking (Original)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter optimization (HPO) of the stacking ensemble model (XGBoost) using Bayesian optimization with tree-based Parzen estimator (BO-TPE)\n",
    "Based on the GitHub repo for HPO: https://github.com/LiYangHart/Hyperparameter-Optimization-of-Machine-Learning-Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from hyperopt import hp, fmin, tpe, STATUS_OK, Trials\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "def objective(params):\n",
    "    params = {\n",
    "        'n_estimators': int(params['n_estimators']), \n",
    "        'max_depth': int(params['max_depth']),\n",
    "        'learning_rate':  abs(float(params['learning_rate'])),\n",
    "\n",
    "    }\n",
    "    clf = xgb.XGBClassifier( **params)\n",
    "    clf.fit(x_train, y_train)\n",
    "    y_pred = clf.predict(x_test)\n",
    "    score = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    return {'loss':-score, 'status': STATUS_OK }\n",
    "\n",
    "space = {\n",
    "    'n_estimators': hp.quniform('n_estimators', 10, 100, 5),\n",
    "    'max_depth': hp.quniform('max_depth', 4, 100, 1),\n",
    "    'learning_rate': hp.normal('learning_rate', 0.01, 0.9),\n",
    "}\n",
    "\n",
    "t1 = time.time()\n",
    "\n",
    "best = fmin(fn=objective,\n",
    "            space=space,\n",
    "            algo=tpe.suggest,\n",
    "            max_evals=20)\n",
    "\n",
    "t2 = time.time()\n",
    "\n",
    "print(\"XGBoost: Hyperopt estimated optimum {}\".format(best))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    'n_estimators': int(best['n_estimators']), \n",
    "    'max_depth': int(best['max_depth']),\n",
    "    'learning_rate':  abs(float(best['learning_rate'])),\n",
    "}\n",
    "xg = xgb.XGBClassifier(**params)\n",
    "t3 = time.time()\n",
    "xg.fit(x_train,y_train)\n",
    "t4 = time.time()\n",
    "xg_score=xg.score(x_test,y_test)\n",
    "t5 = time.time()\n",
    "y_predict=xg.predict(x_test)\n",
    "t6 = time.time()\n",
    "y_true=y_test\n",
    "print('Accuracy of XGBoost: '+ str(xg_score))\n",
    "precision,recall,fscore,none= precision_recall_fscore_support(y_true, y_predict, average='weighted') \n",
    "print('Precision of XGBoost: '+(str(precision)))\n",
    "print('Recall of XGBoost: '+(str(recall)))\n",
    "print('F1-score of XGBoost: '+(str(fscore)))\n",
    "report_str = classification_report(y_true,y_predict); log_file.write('******{}******\\n'.format('Stacking (BO-TPE)')+report_str+'\\n'); print(report_str)\n",
    "cm=confusion_matrix(y_true,y_predict)\n",
    "f,ax=plt.subplots(figsize=(18,14))\n",
    "sns.heatmap(cm,annot=True,linewidth=0.5,linecolor=\"red\",fmt=\".0f\",ax=ax)\n",
    "ax.set_xticklabels(label_names)\n",
    "ax.set_yticklabels(list(reversed(label_names)))\n",
    "plt.xlabel(\"y_pred\")\n",
    "plt.ylabel(\"y_true\")\n",
    "plt.savefig(os.path.join(img_dir, 'Stacking_BO-TPE.pdf'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add to output sheet\n",
    "result_dict = {\n",
    "    'Accuracy': xg_score,\n",
    "    'Precision': precision,\n",
    "    'Recall': recall,\n",
    "    'F1-Score': fscore,\n",
    "    'HPO_time': t2-t1,\n",
    "    'Train_time': t4-t3,\n",
    "    'Predict_time_per_record': (t6-t5)/len_test\n",
    "}\n",
    "output_df = output_df.append(result_dict, ignore_index=True)\n",
    "# Add index name\n",
    "output_index.append('Stacking (BO-TPE)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter optimization (HPO) of stacking ensemble model (XGBoost) using Particle Swarm Optimization (PSO)\n",
    "Based on the GitHub repo for HPO: https://github.com/LiYangHart/Hyperparameter-Optimization-of-Machine-Learning-Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#XGBoost\n",
    "import optunity\n",
    "import optunity.metrics\n",
    "\n",
    "data= X_combined\n",
    "labels= y_combined.tolist()\n",
    "Y_train = y_train\n",
    "Y_test = y_test\n",
    "# Define the hyperparameter configuration space\n",
    "search = {\n",
    "    'n_estimators': [10, 100],\n",
    "    'max_depth': [5,50],\n",
    "    'learning_rate': [0.01, 0.9]\n",
    "}\n",
    "# Define the objective function\n",
    "@optunity.cross_validated(x=data, y=labels, num_folds=3)\n",
    "def performance(x_train, y_train, x_test, y_test,n_estimators=None, max_depth=None,learning_rate=None):\n",
    "    # fit the model\n",
    "    params = {\n",
    "        'n_estimators': int(n_estimators), \n",
    "        'max_depth': int(max_depth),\n",
    "        'learning_rate':  abs(float(learning_rate)),\n",
    "    }\n",
    "    model = xgb.XGBClassifier( **params)\n",
    "    model.fit(X_train, Y_train)\n",
    "    predictions = model.predict(X_test)\n",
    "    # scores=np.mean(cross_val_score(model, X, y, cv=3, n_jobs=-1,\n",
    "    #                                 scoring=\"accuracy\"))\n",
    "    #return optunity.metrics.roc_auc(y_test, predictions, positive=True)\n",
    "    return optunity.metrics.accuracy(Y_test, predictions)\n",
    "\n",
    "t1 = time.time()\n",
    "\n",
    "optimal_configuration, info, _ = optunity.maximize(performance,\n",
    "                                                  solver_name='particle swarm',\n",
    "                                                  num_evals=20,\n",
    "                                                   **search\n",
    "                                                  )\n",
    "\n",
    "t2 = time.time()\n",
    "\n",
    "print(optimal_configuration)\n",
    "print(\"Accuracy:\"+ str(info.optimum))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'n_estimators': int(optimal_configuration['n_estimators']), \n",
    "    'max_depth': int(optimal_configuration['max_depth']), \n",
    "    'learning_rate': abs(float(optimal_configuration['learning_rate']))\n",
    "}\n",
    "xg = xgb.XGBClassifier(**params)\n",
    "t3 = time.time()\n",
    "xg.fit(x_train,y_train)\n",
    "t4 = time.time()\n",
    "xg_score=xg.score(x_test,y_test)\n",
    "t5 = time.time()\n",
    "y_predict=xg.predict(x_test)\n",
    "t6 = time.time()\n",
    "y_true=y_test\n",
    "print('Accuracy of XGBoost: '+ str(xg_score))\n",
    "precision,recall,fscore,none= precision_recall_fscore_support(y_true, y_predict, average='weighted') \n",
    "print('Precision of XGBoost: '+(str(precision)))\n",
    "print('Recall of XGBoost: '+(str(recall)))\n",
    "print('F1-score of XGBoost: '+(str(fscore)))\n",
    "report_str = classification_report(y_true,y_predict); log_file.write('******{}******\\n'.format('Stacking (PSO)')+report_str+'\\n'); print(report_str)\n",
    "cm=confusion_matrix(y_true,y_predict)\n",
    "f,ax=plt.subplots(figsize=(18,14))\n",
    "sns.heatmap(cm,annot=True,linewidth=0.5,linecolor=\"red\",fmt=\".0f\",ax=ax)\n",
    "ax.set_xticklabels(label_names)\n",
    "ax.set_yticklabels(list(reversed(label_names)))\n",
    "plt.xlabel(\"y_pred\")\n",
    "plt.ylabel(\"y_true\")\n",
    "plt.savefig(os.path.join(img_dir, 'Stacking_PSO.pdf'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add to output sheet\n",
    "result_dict = {\n",
    "    'Accuracy': xg_score,\n",
    "    'Precision': precision,\n",
    "    'Recall': recall,\n",
    "    'F1-Score': fscore,\n",
    "    'HPO_time': t2-t1,\n",
    "    'Train_time': t4-t3,\n",
    "    'Predict_time_per_record': (t6-t5)/len_test\n",
    "}\n",
    "output_df = output_df.append(result_dict, ignore_index=True)\n",
    "# Add index name\n",
    "output_index.append('Stacking (PSO)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter optimization (HPO) of stacking ensemble model (XGBoost) using Genetic Algorithm (GA)\n",
    "Based on the GitHub repo for HPO: https://github.com/LiYangHart/Hyperparameter-Optimization-of-Machine-Learning-Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#XGBoost\n",
    "from tpot import TPOTClassifier\n",
    "# Define the hyperparameter configuration space\n",
    "parameters = {\n",
    "    'n_estimators': range(10,100),\n",
    "    'max_depth': range(4,100),\n",
    "    'learning_rate': [i/100 for i in range(1, 90)]\n",
    "}\n",
    "# Set the hyperparameters of GA                 \n",
    "ga = TPOTClassifier(generations= 3, population_size= 10, offspring_size= 5,\n",
    "                                 verbosity= 3, early_stop= 5,\n",
    "                                 config_dict=\n",
    "                                 {'xgboost.XGBClassifier': parameters}, \n",
    "                                 cv = 3, scoring = 'accuracy')\n",
    "t1 = time.time()\n",
    "ga.fit(X_combined, y_combined)\n",
    "t2 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xg = xgb.XGBClassifier(**get_ga_optimized_parameters(ga, 'XGBClassifier'))\n",
    "t3 = time.time()\n",
    "xg.fit(x_train,y_train)\n",
    "t4 = time.time()\n",
    "xg_score=xg.score(x_test,y_test)\n",
    "t5 = time.time()\n",
    "y_predict=xg.predict(x_test)\n",
    "t6 = time.time()\n",
    "y_true=y_test\n",
    "print('Accuracy of XGBoost: '+ str(xg_score))\n",
    "precision,recall,fscore,none= precision_recall_fscore_support(y_true, y_predict, average='weighted') \n",
    "print('Precision of XGBoost: '+(str(precision)))\n",
    "print('Recall of XGBoost: '+(str(recall)))\n",
    "print('F1-score of XGBoost: '+(str(fscore)))\n",
    "report_str = classification_report(y_true,y_predict); log_file.write('******{}******\\n'.format('Stacking (GA)')+report_str+'\\n'); print(report_str)\n",
    "cm=confusion_matrix(y_true,y_predict)\n",
    "f,ax=plt.subplots(figsize=(18,14))\n",
    "sns.heatmap(cm,annot=True,linewidth=0.5,linecolor=\"red\",fmt=\".0f\",ax=ax)\n",
    "ax.set_xticklabels(label_names)\n",
    "ax.set_yticklabels(list(reversed(label_names)))\n",
    "plt.xlabel(\"y_pred\")\n",
    "plt.ylabel(\"y_true\")\n",
    "plt.savefig(os.path.join(img_dir, 'Stacking_GA.pdf'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add to output sheet\n",
    "result_dict = {\n",
    "    'Accuracy': xg_score,\n",
    "    'Precision': precision,\n",
    "    'Recall': recall,\n",
    "    'F1-Score': fscore,\n",
    "    'HPO_time': t2-t1,\n",
    "    'Train_time': t4-t3,\n",
    "    'Predict_time_per_record': (t6-t5)/len_test\n",
    "}\n",
    "output_df = output_df.append(result_dict, ignore_index=True)\n",
    "# Add index name\n",
    "output_index.append('Stacking (GA)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the index\n",
    "output_df.index = output_index\n",
    "# Save the result to file\n",
    "output_df.to_excel(os.path.join(output_dir, 'result-{}.xlsx'.format(datetime.datetime.now().strftime('%y%m%d-%H%M%S'))))\n",
    "# Close the logging file\n",
    "log_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "interpreter": {
   "hash": "9af53353804afc09f90d8f022eaec6e99fffff96cfdda280d845f608b681a8ce"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
