{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Transfer Learning and Optimized CNN Based Intrusion Detection System for Internet of Vehicles \n",
    "This is the code for the paper entitled \"**A Transfer Learning and Optimized CNN Based Intrusion Detection System for Internet of Vehicles**\" accepted in IEEE International Conference on Communications (IEEE ICC).  \n",
    "Authors: Li Yang (lyang339@uwo.ca) and Abdallah Shami (Abdallah.Shami@uwo.ca)  \n",
    "Organization: The Optimized Computing and Communications (OC2) Lab, ECE Department, Western University\n",
    "\n",
    "**Notebook 2: CNN Model Development**  \n",
    "Aims:  \n",
    "&nbsp; 1): Generate training and test images  \n",
    "&nbsp; 2): Construct CNN models (a CNN model by own, Xception, VGG16, VGG19, Resnet, Inception, InceptionResnet)  \n",
    "&nbsp; 3): Tune the hyperparameters of CNN models (hyperparameter optimization)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.inception_resnet_v2 import InceptionResNetV2\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
    "from tensorflow.keras.applications.mobilenet import MobileNet\n",
    "from tensorflow.keras.applications.resnet50 import  ResNet50\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.applications.vgg19 import VGG19\n",
    "from tensorflow.keras.applications.xception import  Xception\n",
    "from tensorflow.keras.layers import Dense,Flatten,GlobalAveragePooling2D,Input,Conv2D,MaxPooling2D,Dropout\n",
    "from tensorflow.keras.models import Model,load_model,Sequential\n",
    "from tensorflow.keras.preprocessing.image import  ImageDataGenerator\n",
    "from tensorflow.keras.preprocessing.image import load_img,img_to_array\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from PIL import Image\n",
    "from sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score\n",
    "import datetime\n",
    "import time\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow.keras.callbacks as kcallbacks\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import sklearn.metrics as metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Training and Test Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#generate training and test images\n",
    "TARGET_SIZE=(224,224)\n",
    "INPUT_SIZE=(224,224,3)\n",
    "BATCHSIZE=32\t#could try 128 or 32\n",
    "\n",
    "#Normalization\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        './train_224/',\n",
    "        target_size=TARGET_SIZE,\n",
    "        batch_size=BATCHSIZE,\n",
    "        class_mode='categorical')\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "        './test_224/',\n",
    "        target_size=TARGET_SIZE,\n",
    "        batch_size=BATCHSIZE,\n",
    "        class_mode='categorical')\n",
    "num_class = train_generator.num_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the image plotting functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the figures\n",
    "class LossHistory(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = {'batch':[], 'epoch':[]}\n",
    "        self.accuracy = {'batch':[], 'epoch':[]}\n",
    "        self.val_loss = {'batch':[], 'epoch':[]}\n",
    "        self.val_acc = {'batch':[], 'epoch':[]}\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        self.losses['batch'].append(logs.get('loss'))\n",
    "        self.accuracy['batch'].append(logs.get('acc'))\n",
    "        self.val_loss['batch'].append(logs.get('val_loss'))\n",
    "        self.val_acc['batch'].append(logs.get('val_acc'))\n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        self.losses['epoch'].append(logs.get('loss'))\n",
    "        self.accuracy['epoch'].append(logs.get('acc'))\n",
    "        self.val_loss['epoch'].append(logs.get('val_loss'))\n",
    "        self.val_acc['epoch'].append(logs.get('val_acc'))\n",
    "    def loss_plot(self, loss_type):\n",
    "        iters = range(len(self.losses[loss_type]))\n",
    "        plt.figure()\n",
    "        plt.plot(iters, self.losses[loss_type], 'g', label='train loss')\n",
    "        if loss_type == 'epoch':\n",
    "            # acc\n",
    "            plt.plot(iters, self.accuracy[loss_type], 'r', label='train acc')\n",
    "            # loss\n",
    "            plt.plot(iters, self.losses[loss_type], 'g', label='train loss')\n",
    "            # val_acc\n",
    "            plt.plot(iters, self.val_acc[loss_type], 'b', label='val acc')\n",
    "            # val_loss\n",
    "            plt.plot(iters, self.val_loss[loss_type], 'k', label='val loss')\n",
    "        plt.grid(True)\n",
    "        plt.xlabel(loss_type)\n",
    "        plt.ylabel('acc-loss')\n",
    "        plt.legend(loc=\"upper right\")\n",
    "        plt.show()\n",
    "    def get_best(self, target_type:str='epoch'):\n",
    "        # Get the index of the best record\n",
    "        max_index = self.accuracy[target_type].index(max(self.accuracy[target_type]))\n",
    "        # Return the accuracy, loss, val_acc, val_loss of the best record\n",
    "        return {\n",
    "            'accuracy': self.accuracy[target_type][max_index], \n",
    "            'loss': self.losses[target_type][max_index], \n",
    "            'val_acc': self.val_acc[target_type][max_index], \n",
    "            'val_loss': self.val_loss[target_type][max_index]\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "history_this= LossHistory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the processing time measurement functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure the running time of model training\n",
    "class TimeMeasurement(keras.callbacks.Callback):\n",
    "    def __init__(self):\n",
    "        self.start_time=None\n",
    "        self.stop_time=None\n",
    "    # Start timing when trainning starts\n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.start_time=time.time()\n",
    "        self.stop_time=None\n",
    "    # Stop timing when trainning ends\n",
    "    def on_train_end(self, logs=None):\n",
    "        self.stop_time=time.time()\n",
    "        if self.start_time is None: print(\"Time Measuring Failed\")\n",
    "    # Get processing time\n",
    "    def get_processing_time(self):\n",
    "        if (self.start_time is None or self.stop_time is None): raise Exception(\"Wrong Time Measurement\")\n",
    "        else: return self.stop_time-self.start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timer = TimeMeasurement()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the output sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class output_sheet:\n",
    "    def __init__(self, columns:list=['accuracy', 'loss', 'val_acc', 'val_loss', 'hpo_time', 'train_time']):\n",
    "        self.output_df = pd.DataFrame(columns=columns)\n",
    "        # self.output_index = list()\n",
    "    def add(self, item:str, **values:dict):\n",
    "        # self.output_df = self.output_df.append(values, ignore_index=True)\n",
    "        temp = pd.DataFrame(values, columns=self.output_df.columns.to_list(), index=[item])\n",
    "        self.output_df = pd.concat([self.output_df, temp], axis=0)\n",
    "        # self.output_index.append(item)\n",
    "    # def apply_index(self):\n",
    "    #     self.output_df.index = self.output_index\n",
    "    def to_excel(self, path=None):\n",
    "        if path is None: path='result-{}.xlsx'.format(datetime.datetime.now().strftime('%y%m%d-%H%M%S'))\n",
    "        # self.apply_index()\n",
    "        self.output_df.to_excel(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = output_sheet(columns=['accuracy', 'loss', 'val_acc', 'val_loss', 'hpo_time', 'train_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Prepare the result output\n",
    "# output_df = pd.DataFrame(columns=['acc', 'loss', 'val_acc', 'val_loss', 'hpo_time', 'train_time'])\n",
    "# output_index = list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct CNN models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1: a CNN model by own (baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_by_own(input_shape,num_class,epochs,savepath='./model_own.h5',history=history_this,timer=timer):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(64,(3,3),strides=(1,1),input_shape=input_shape,padding='same',activation='relu',kernel_initializer='glorot_uniform'))\n",
    "    model.add(Conv2D(64,(3,3),strides=(1,1),padding='same',activation='relu',kernel_initializer='glorot_uniform'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Conv2D(128,(3,3),strides=(1,1),padding='same',activation='relu',kernel_initializer='glorot_uniform'))\n",
    "    model.add(Conv2D(128,(3,3),strides=(1,1),padding='same',activation='relu',kernel_initializer='glorot_uniform'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Conv2D(256,(3,3),strides=(1,1),padding='same',activation='relu',kernel_initializer='glorot_uniform'))\n",
    "    model.add(Conv2D(256,(3,3),strides=(1,1),padding='same',activation='relu',kernel_initializer='glorot_uniform'))\n",
    "    model.add(Conv2D(256,(3,3),strides=(1,1),padding='same',activation='relu',kernel_initializer='glorot_uniform'))\n",
    "    model.add(GlobalAveragePooling2D())\n",
    "    model.add(Dense(256,activation='relu'))\n",
    "    model.add(Dropout(rate=0.5))\n",
    "    model.add(Dense(num_class,activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "    #train model\n",
    "    earlyStopping=kcallbacks.EarlyStopping(monitor='val_acc', patience=2, verbose=1, mode='auto')\n",
    "    saveBestModel = kcallbacks.ModelCheckpoint(filepath=savepath, monitor='val_acc', verbose=1, save_best_only=True, mode='auto')\n",
    "    hist=model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=len(train_generator),\n",
    "        epochs=epochs,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=len(validation_generator),\n",
    "        callbacks=[earlyStopping,saveBestModel,history,timer],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cnn_by_own(input_shape=INPUT_SIZE,num_class=num_class,epochs=20)\n",
    "history_this.loss_plot('epoch')\n",
    "history_this.loss_plot('batch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation accuracy of a CNN by own: 99.884%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.add('model_own', train_time=timer.get_processing_time(), **history_this.get_best())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2: Xception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xception( num_class, epochs,savepath='./xception.h5',history=history_this,input_shape=INPUT_SIZE,timer=timer):\n",
    "    model_fine_tune = Xception(include_top=False, weights='imagenet', input_shape=input_shape)\n",
    "    for layer in model_fine_tune.layers[:131]:\t\t#could be tuned to be 50, 100, or 131\n",
    "        layer.trainable = False\n",
    "    for layer in model_fine_tune.layers[131:]:\n",
    "        layer.trainable = True\n",
    "    model = GlobalAveragePooling2D()(model_fine_tune.output)\n",
    "    model=Dense(units=256,activation='relu')(model)\n",
    "    model=Dropout(0.5)(model)\n",
    "    model = Dense(num_class, activation='softmax')(model)\n",
    "    model = Model(model_fine_tune.input, model, name='xception')\n",
    "    opt = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "    #train model\n",
    "    earlyStopping = kcallbacks.EarlyStopping(\n",
    "        monitor='val_acc', patience=3, verbose=1, mode='auto')\t#patience could be tuned by 2 and 3\n",
    "    saveBestModel = kcallbacks.ModelCheckpoint(\n",
    "        filepath=savepath,\n",
    "        monitor='val_acc',\n",
    "        verbose=1,\n",
    "        save_best_only=True,\n",
    "        mode='auto')\n",
    "    hist = model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=len(train_generator),\n",
    "        epochs=epochs,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=len(validation_generator),\n",
    "        #use_multiprocessing=True, \n",
    "        callbacks=[earlyStopping, saveBestModel, history, timer],\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#default only 50, tf36cnn 99\n",
    "xception(num_class=num_class,epochs=20)\n",
    "history_this.loss_plot('epoch')\n",
    "history_this.loss_plot('batch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation accuracy of Xception: 100.0%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.add('Xception', train_time=timer.get_processing_time(), **history_this.get_best())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.output_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3: VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def vgg16( num_class, epochs,savepath='./VGG16.h5',history=history_this,input_shape=INPUT_SIZE,timer=timer):\n",
    "    model_fine_tune = VGG16(include_top=False, weights='imagenet', input_shape=input_shape)\n",
    "    for layer in model_fine_tune.layers[:15]:\t#the number of frozen layers for transfer learning, have tuned from 5-18\n",
    "        layer.trainable = False\n",
    "    for layer in model_fine_tune.layers[15:]:\n",
    "        layer.trainable = True\n",
    "    model = GlobalAveragePooling2D()(model_fine_tune.output) #GlobalAveragePooling2D layer to convert the features to a single 1280-element vector per image\n",
    "    model=Dense(units=256,activation='relu')(model)\n",
    "    model=Dropout(0.5)(model)\n",
    "    model = Dense(num_class, activation='softmax')(model)\n",
    "    model = Model(model_fine_tune.input, model, name='vgg')\n",
    "    opt = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\t#tuned learning rate to be 0.001\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\t#set the loss function to be binary crossentropy\n",
    "    #train model\n",
    "    earlyStopping = kcallbacks.EarlyStopping(\n",
    "        monitor='val_acc', patience=2, verbose=1, mode='auto')\t#set early stop patience to save training time\n",
    "    saveBestModel = kcallbacks.ModelCheckpoint(\n",
    "        filepath=savepath,\n",
    "        monitor='val_acc',\n",
    "        verbose=1,\n",
    "        save_best_only=True,\n",
    "        mode='auto')\n",
    "    hist = model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=len(train_generator),\n",
    "        epochs=epochs,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=len(validation_generator),\n",
    "        #use_multiprocessing=True, \n",
    "        #workers=2,\n",
    "        callbacks=[earlyStopping, saveBestModel, history, timer],\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vgg16(num_class=num_class,epochs=20)\t#tf36cnn\n",
    "history_this.loss_plot('epoch')\n",
    "history_this.loss_plot('batch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation accuracy of VGG16: 100.0%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.add('VGG16', train_time=timer.get_processing_time(), **history_this.get_best())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 4: VGG19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def vgg19( num_class, epochs,savepath='./VGG19.h5',history=history_this,input_shape=INPUT_SIZE,timer=timer):\n",
    "    model_fine_tune = VGG19(include_top=False, weights='imagenet', input_shape=input_shape)\n",
    "    for layer in model_fine_tune.layers[:19]:\t#the number of frozen layers for transfer learning, have tuned from 5-18\n",
    "        layer.trainable = False\n",
    "    for layer in model_fine_tune.layers[19:]:\n",
    "        layer.trainable = True\n",
    "    model = GlobalAveragePooling2D()(model_fine_tune.output)\n",
    "    model=Dense(units=256,activation='relu')(model)\n",
    "    model=Dropout(0.5)(model)\n",
    "    model = Dense(num_class, activation='softmax')(model)\n",
    "    model = Model(model_fine_tune.input, model, name='vgg')\n",
    "    opt = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\t#tuned learning rate to be 0.001\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\t#set the loss function to be binary crossentropy\n",
    "    #train model\n",
    "    earlyStopping = kcallbacks.EarlyStopping(\n",
    "        monitor='val_acc', patience=2, verbose=1, mode='auto')\t#set early stop patience to save training time\n",
    "    saveBestModel = kcallbacks.ModelCheckpoint(\n",
    "        filepath=savepath,\n",
    "        monitor='val_acc',\n",
    "        verbose=1,\n",
    "        save_best_only=True,\n",
    "        mode='auto')\n",
    "    hist = model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=len(train_generator),\n",
    "        epochs=epochs,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=len(validation_generator),\n",
    "        #use_multiprocessing=True, \n",
    "        #workers=2,\n",
    "        callbacks=[earlyStopping, saveBestModel, history, timer],\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vgg19(num_class=num_class,epochs=20)\t#binary classificaiton\n",
    "history_this.loss_plot('epoch')\n",
    "history_this.loss_plot('batch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation accuracy of VGG19: 100.0%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.add('VGG19', train_time=timer.get_processing_time(), **history_this.get_best())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 5: ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def resnet( num_class, epochs,savepath='./resnet.h5',history=history_this,input_shape=INPUT_SIZE,timer=timer):\n",
    "    model_fine_tune = ResNet50(include_top=False, weights='imagenet', input_shape=input_shape)\n",
    "    for layer in model_fine_tune.layers[:120]:\t#the number of frozen layers for transfer learning, have tuned from 50-150\n",
    "        layer.trainable = False\n",
    "    for layer in model_fine_tune.layers[120:]:\t#the number of trainable layers for transfer learning\n",
    "        layer.trainable = True\n",
    "    model = GlobalAveragePooling2D()(model_fine_tune.output)\n",
    "    model=Dense(units=256,activation='relu')(model)\n",
    "    model=Dropout(0.5)(model)\n",
    "    model = Dense(num_class, activation='softmax')(model)\n",
    "    model = Model(model_fine_tune.input, model, name='resnet')\n",
    "    opt = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\t#tuned learning rate to be 0.001\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy']) #set the loss function to be binary crossentropy\n",
    "    #train model\n",
    "    earlyStopping = kcallbacks.EarlyStopping(\n",
    "        monitor='val_acc', patience=2, verbose=1, mode='auto')\t#set early stop patience to save training time\n",
    "    saveBestModel = kcallbacks.ModelCheckpoint(\n",
    "        filepath=savepath,\n",
    "        monitor='val_acc',\n",
    "        verbose=1,\n",
    "        save_best_only=True,\n",
    "        mode='auto')\n",
    "    hist = model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=len(train_generator),\n",
    "        epochs=epochs,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=len(validation_generator),\n",
    "        #use_multiprocessing=True, \n",
    "        callbacks=[earlyStopping, saveBestModel, history, timer],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "resnet(num_class=num_class,epochs=20)\t#binary classificaiton\n",
    "history_this.loss_plot('epoch')\n",
    "history_this.loss_plot('batch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation accuracy of Resnet: 98.652%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.add('Resnet', train_time=timer.get_processing_time(), **history_this.get_best())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 6: Inception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def inception( num_class, epochs,savepath='./inception.h5',history=history_this,input_shape=INPUT_SIZE, timer=timer):\n",
    "    model_fine_tune = InceptionV3(include_top=False, weights='imagenet', input_shape=input_shape)\n",
    "    for layer in model_fine_tune.layers[:35]:\t#the number of frozen layers for transfer learning, have tuned from 50-150\n",
    "        layer.trainable = False\n",
    "    for layer in model_fine_tune.layers[35:]:\t#the number of trainable layers for transfer learning\n",
    "        layer.trainable = True\n",
    "    model = GlobalAveragePooling2D()(model_fine_tune.output)\n",
    "    model=Dense(units=256,activation='relu')(model)\n",
    "    model=Dropout(0.5)(model)\n",
    "    model = Dense(num_class, activation='softmax')(model)\n",
    "    model = Model(model_fine_tune.input, model, name='resnet')\n",
    "    opt = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\t#tuned learning rate to be 0.001\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy']) #set the loss function to be binary crossentropy\n",
    "    #train model\n",
    "    earlyStopping = kcallbacks.EarlyStopping(\n",
    "        monitor='val_acc', patience=2, verbose=1, mode='auto')\t#set early stop patience to save training time\n",
    "    saveBestModel = kcallbacks.ModelCheckpoint(\n",
    "        filepath=savepath,\n",
    "        monitor='val_acc',\n",
    "        verbose=1,\n",
    "        save_best_only=True,\n",
    "        mode='auto')\n",
    "    hist = model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=len(train_generator),\n",
    "        epochs=epochs,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=len(validation_generator),\n",
    "        #use_multiprocessing=True, \n",
    "        callbacks=[earlyStopping, saveBestModel, history, timer],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "inception(num_class=num_class,epochs=20)\t#binary classificaiton\n",
    "history_this.loss_plot('epoch')\n",
    "history_this.loss_plot('batch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation accuracy of Inception: 100.0%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.add('Inception', train_time=timer.get_processing_time(), **history_this.get_best())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 7: InceptionResnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def inceptionresnet( num_class, epochs,savepath='./inceptionresnet.h5',history=history_this,input_shape=INPUT_SIZE, timer=timer):\n",
    "    model_fine_tune = InceptionResNetV2(include_top=False, weights='imagenet', input_shape=input_shape)\n",
    "    for layer in model_fine_tune.layers[:500]:\t#the number of frozen layers for transfer learning, have tuned from 400-550\n",
    "        layer.trainable = False\n",
    "    for layer in model_fine_tune.layers[500:]:\t#the number of trainable layers for transfer learning\n",
    "        layer.trainable = True\n",
    "    model = GlobalAveragePooling2D()(model_fine_tune.output)\n",
    "    model=Dense(units=256,activation='relu')(model)\n",
    "    model=Dropout(0.5)(model)\n",
    "    model = Dense(num_class, activation='softmax')(model)\n",
    "    model = Model(model_fine_tune.input, model, name='resnet')\n",
    "    opt = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\t#tuned learning rate to be 0.001\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy']) #set the loss function to be binary crossentropy\n",
    "    #train model\n",
    "    earlyStopping = kcallbacks.EarlyStopping(\n",
    "        monitor='val_acc', patience=2, verbose=1, mode='auto')\t#set early stop patience to save training time\n",
    "    saveBestModel = kcallbacks.ModelCheckpoint(\n",
    "        filepath=savepath,\n",
    "        monitor='val_acc',\n",
    "        verbose=1,\n",
    "        save_best_only=True,\n",
    "        mode='auto')\n",
    "    hist = model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=len(train_generator),\n",
    "        epochs=epochs,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=len(validation_generator),\n",
    "        #use_multiprocessing=True, \n",
    "        callbacks=[earlyStopping, saveBestModel, history, timer],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "inceptionresnet(num_class=num_class,epochs=20)\t# 5-class classificaiton\n",
    "history_this.loss_plot('epoch')\n",
    "history_this.loss_plot('batch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation accuracy of InceptionResnet: 99.993%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.add('InceptionResnet', train_time=timer.get_processing_time(), **history_this.get_best())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Optimization \n",
    "Use VGG16 as an example.  \n",
    "\n",
    "Tuned hyperparameters of CNN: \n",
    "1. The number of frozen layers\n",
    "2. The number of epochs\n",
    "3. Early stop patience\n",
    "4. Learning rate\n",
    "5. Dropout rate\n",
    "\n",
    "Hyperparameter optimization methods:\n",
    "1. Random search\n",
    "2. Bayesian optimization - Tree Parzen Estimator(BO-TPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prediction(model, rootdir='./test_224/'):\n",
    "#read images from validation folder\n",
    "    test_laels = []\n",
    "    test_images=[]\n",
    "    for subdir, dirs, files in os.walk(rootdir):\n",
    "        for file in files:\n",
    "            if not (file.endswith(\".jpeg\"))|(file.endswith(\".jpg\"))|(file.endswith(\".png\")):\n",
    "                continue\n",
    "            test_laels.append(subdir.split('/')[-1])\n",
    "            test_images.append(os.path.join(subdir, file))\n",
    "\n",
    "    predict=[]\n",
    "    length=len(test_images)\n",
    "    label=validation_generator.class_indices\n",
    "    label={v: k for k, v in label.items()}\n",
    "    for i in range(length):\n",
    "        inputimg=test_images[i]\n",
    "        test_batch=[]\n",
    "        thisimg=np.array(Image.open(inputimg))/255 #read all the images in validation set\n",
    "        #print(thisimg)\n",
    "        test_shape=(1,)+thisimg.shape\n",
    "        thisimg=thisimg.reshape(test_shape)\n",
    "        model_batch=model.predict(thisimg) #use master model to process the input image\n",
    "        #generate result by model 1\n",
    "        prob=model_batch[0,np.argmax(model_batch,axis=1)[0]]\n",
    "        res=label[np.argmax(model_batch,axis=1)[0]]\n",
    "        predict.append(res)\n",
    "    acc=accuracy_score(test_laels,predict)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model by Own"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_by_own(input_shape,num_class,epochs=20,patience=2, dropout_rate=0.5,verbose=0,savepath='./model_own.h5',history=history_this,timer=timer):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(64,(3,3),strides=(1,1),input_shape=input_shape,padding='same',activation='relu',kernel_initializer='glorot_uniform'))\n",
    "    model.add(Conv2D(64,(3,3),strides=(1,1),padding='same',activation='relu',kernel_initializer='glorot_uniform'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Conv2D(128,(3,3),strides=(1,1),padding='same',activation='relu',kernel_initializer='glorot_uniform'))\n",
    "    model.add(Conv2D(128,(3,3),strides=(1,1),padding='same',activation='relu',kernel_initializer='glorot_uniform'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Conv2D(256,(3,3),strides=(1,1),padding='same',activation='relu',kernel_initializer='glorot_uniform'))\n",
    "    model.add(Conv2D(256,(3,3),strides=(1,1),padding='same',activation='relu',kernel_initializer='glorot_uniform'))\n",
    "    model.add(Conv2D(256,(3,3),strides=(1,1),padding='same',activation='relu',kernel_initializer='glorot_uniform'))\n",
    "    model.add(GlobalAveragePooling2D())\n",
    "    model.add(Dense(256,activation='relu'))\n",
    "    model.add(Dropout(rate=dropout_rate))\n",
    "    model.add(Dense(num_class,activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "    #train model\n",
    "    earlyStopping=kcallbacks.EarlyStopping(monitor='val_acc', patience=patience, verbose=verbose, mode='auto')\n",
    "    saveBestModel = kcallbacks.ModelCheckpoint(filepath=savepath, monitor='val_acc', verbose=verbose, save_best_only=True, mode='auto')\n",
    "    hist=model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=len(train_generator),\n",
    "        epochs=epochs,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=len(validation_generator),\n",
    "        callbacks=[earlyStopping,saveBestModel,history,timer],\n",
    "        verbose=verbose\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the objective function to be optimized\n",
    "import time\n",
    "from hyperopt import hp, fmin, tpe, rand, STATUS_OK, Trials\n",
    "import matplotlib.pyplot as plt\n",
    "import statistics \n",
    "\n",
    "def objective(params, num_class=num_class):\n",
    "    \n",
    "    params = {\n",
    "        'epochs': int(params['epochs']),\n",
    "        'patience': int(params['patience']),\n",
    "        'dropout_rate': abs(float(params['dropout_rate'])),\n",
    "    }\n",
    "    # frozen=params['frozen']\n",
    "    # epochs=params['epochs']\n",
    "    # patience=params['patience']\n",
    "    # lr=params['lr']\n",
    "    # dropout_rate=params['dropout_rate']\n",
    "\n",
    "    # vgg16(num_class=num_class, frozen=frozen,epochs=epochs,patience=patience, lr=lr, dropout_rate=dropout_rate)\n",
    "\n",
    "    cnn_by_own(input_shape=INPUT_SIZE, num_class=num_class, **params)\n",
    "\n",
    "    acc=prediction(model=load_model('./model_own.h5'))\n",
    "\n",
    "    print('accuracy:%s'%acc)\n",
    "    return {'loss': -acc, 'status': STATUS_OK }\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BO-TPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameter optimization by Bayesian optimization - Tree Parzen Estimator\n",
    "space = {\n",
    "    'epochs': hp.quniform('epochs', 5, 21, 5),\n",
    "    'patience': hp.quniform('patience', 2, 4, 1),\n",
    "    'dropout_rate': hp.quniform('dropout_rate', 0.3, 0.6, 0.1),\n",
    "}\n",
    "\n",
    "t1=time.time()\n",
    "best = fmin(fn=objective,\n",
    "            space=space,\n",
    "            algo=tpe.suggest,\n",
    "            max_evals=10)\n",
    "print(\"Hyperopt estimated optimum {}\".format(best))\n",
    "t2=time.time()\n",
    "print(\"Time: \"+str(t2-t1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrain the model by using the best hyperparameter values to obtain the best model\n",
    "params = {\n",
    "        'epochs': int(best['epochs']),\n",
    "        'patience': int(best['patience']),\n",
    "        'dropout_rate': abs(float(best['dropout_rate'])),\n",
    "    }\n",
    "cnn_by_own(input_shape=INPUT_SIZE, num_class=num_class, verbose=1, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.add('model_own (BO-TPE)', hpo_time=t2-t1, train_time=timer.get_processing_time(), **history_this.get_best())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameter optimization by Random search\n",
    "space = {\n",
    "    'epochs': hp.quniform('epochs', 5, 21, 5),\n",
    "    'patience': hp.quniform('patience', 2, 4, 1),\n",
    "    'dropout_rate': hp.quniform('dropout_rate', 0.3, 0.6, 0.1),\n",
    "}\n",
    "\n",
    "t1=time.time()\n",
    "best = fmin(fn=objective,\n",
    "            space=space,\n",
    "            algo=rand.suggest,\n",
    "            max_evals=10)\n",
    "print(\"Hyperopt estimated optimum {}\".format(best))\n",
    "t2=time.time()\n",
    "print(\"Time: \"+str(t2-t1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrain the model by using the best hyperparameter values to obtain the best model\n",
    "params = {\n",
    "        'epochs': int(best['epochs']),\n",
    "        'patience': int(best['patience']),\n",
    "        'dropout_rate': abs(float(best['dropout_rate'])),\n",
    "    }\n",
    "cnn_by_own(input_shape=INPUT_SIZE, num_class=num_class, verbose=1, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.add('model_own (Random Search)', hpo_time=t2-t1, train_time=timer.get_processing_time(), **history_this.get_best())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def vgg16(num_class,epochs=20,frozen=15,lr=0.001,patience=2, dropout_rate=0.5,verbose=0, savepath='./VGG16.h5',history=history_this,timer=timer,input_shape=INPUT_SIZE):\n",
    "    model_fine_tune = VGG16(include_top=False, weights='imagenet', input_shape=input_shape)\n",
    "    for layer in model_fine_tune.layers[:frozen]:\t#the number of frozen layers for transfer learning, have tuned from 5-18\n",
    "        layer.trainable = False\n",
    "    for layer in model_fine_tune.layers[frozen:]:\n",
    "        layer.trainable = True\n",
    "    model = GlobalAveragePooling2D()(model_fine_tune.output)\n",
    "    model=Dense(units=256,activation='relu')(model)\n",
    "    model=Dropout(dropout_rate)(model)\n",
    "    model = Dense(num_class, activation='softmax')(model)\n",
    "    model = Model(model_fine_tune.input, model, name='vgg')\n",
    "    opt = keras.optimizers.Adam(lr=lr, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\t#tuned learning rate to be 0.001\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\t#set the loss function to be binary crossentropy\n",
    "    #train model\n",
    "    earlyStopping = kcallbacks.EarlyStopping(\n",
    "        monitor='val_acc', patience=patience, verbose=verbose, mode='auto')\t#set early stop patience to save training time\n",
    "    saveBestModel = kcallbacks.ModelCheckpoint(\n",
    "        filepath=savepath,\n",
    "        monitor='val_acc',\n",
    "        verbose=verbose,\n",
    "        save_best_only=True,\n",
    "        mode='auto')\n",
    "    hist = model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=len(train_generator),\n",
    "        epochs=epochs,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=len(validation_generator),\n",
    "        #use_multiprocessing=True, \n",
    "        #workers=2,\n",
    "        callbacks=[earlyStopping, saveBestModel, history, timer],\n",
    "        verbose = verbose\n",
    "    )\n",
    "    return hist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#define the objective function to be optimized\n",
    "import time\n",
    "from hyperopt import hp, fmin, tpe, rand, STATUS_OK, Trials\n",
    "import matplotlib.pyplot as plt\n",
    "import statistics \n",
    "\n",
    "def objective(params, num_class=num_class):\n",
    "    \n",
    "    params = {\n",
    "        'frozen': int(params['frozen']),\n",
    "        'epochs': int(params['epochs']),\n",
    "        'patience': int(params['patience']),\n",
    "        'lr': abs(float(params['lr'])),\n",
    "        'dropout_rate': abs(float(params['dropout_rate'])),\n",
    "    }\n",
    "    # frozen=params['frozen']\n",
    "    # epochs=params['epochs']\n",
    "    # patience=params['patience']\n",
    "    # lr=params['lr']\n",
    "    # dropout_rate=params['dropout_rate']\n",
    "\n",
    "    # vgg16(num_class=num_class, frozen=frozen,epochs=epochs,patience=patience, lr=lr, dropout_rate=dropout_rate)\n",
    "\n",
    "    vgg16(num_class=num_class, **params)\n",
    "\n",
    "    acc=prediction(model=load_model('./VGG16.h5'))\n",
    "\n",
    "    print('accuracy:%s'%acc)\n",
    "    return {'loss': -acc, 'status': STATUS_OK }\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BO-TPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#Hyperparameter optimization by Bayesian optimization - Tree Parzen Estimator\n",
    "space = {\n",
    "    'frozen': hp.quniform('frozen', 15, 18, 1),\n",
    "    'epochs': hp.quniform('epochs', 5, 21, 5),\n",
    "    'patience': hp.quniform('patience', 2, 4, 1),\n",
    "    'lr': hp.quniform('lr', 0.001, 0.006, 0.001),\n",
    "    'dropout_rate': hp.quniform('dropout_rate', 0.3, 0.6, 0.1),\n",
    "}\n",
    "\n",
    "t1=time.time()\n",
    "best = fmin(fn=objective,\n",
    "            space=space,\n",
    "            algo=tpe.suggest,\n",
    "            max_evals=10)\n",
    "print(\"Hyperopt estimated optimum {}\".format(best))\n",
    "t2=time.time()\n",
    "print(\"Time: \"+str(t2-t1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrain the model by using the best hyperparameter values to obtain the best model\n",
    "params = {\n",
    "        'frozen': int(best['frozen']),\n",
    "        'epochs': int(best['epochs']),\n",
    "        'patience': int(best['patience']),\n",
    "        'lr': abs(float(best['lr'])),\n",
    "        'dropout_rate': abs(float(best['dropout_rate'])),\n",
    "    }\n",
    "vgg16(num_class=num_class, verbose=1, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.add('VGG16 (BO-TPE)', hpo_time=t2-t1, train_time=timer.get_processing_time(), **history_this.get_best())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#Hyperparameter optimization by Random search\n",
    "space = {\n",
    "    'frozen': hp.quniform('frozen', 15, 18, 1),\n",
    "    'epochs': hp.quniform('epochs', 5, 21, 5),\n",
    "    'patience': hp.quniform('patience', 2, 4, 1),\n",
    "    'lr': hp.quniform('lr', 0.001, 0.006, 0.001),\n",
    "    'dropout_rate': hp.quniform('dropout_rate', 0.3, 0.6, 0.1),\n",
    "}\n",
    "\n",
    "t1=time.time()\n",
    "best = fmin(fn=objective,\n",
    "            space=space,\n",
    "            algo=rand.suggest,\n",
    "            max_evals=10)\n",
    "print(\"Hyperopt estimated optimum {}\".format(best))\n",
    "t2=time.time()\n",
    "print(\"Time: \"+str(t2-t1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Retrain the model by using the best hyperparameter values to obtain the best model\n",
    "params = {\n",
    "        'frozen': int(best['frozen']),\n",
    "        'epochs': int(best['epochs']),\n",
    "        'patience': int(best['patience']),\n",
    "        'lr': abs(float(best['lr'])),\n",
    "        'dropout_rate': abs(float(best['dropout_rate'])),\n",
    "    }\n",
    "vgg16(num_class=num_class, verbose=1, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.add('VGG16 (Random Search)', hpo_time=t2-t1, train_time=timer.get_processing_time(), **history_this.get_best())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGG19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vgg19( num_class,epochs=20,frozen=15,lr=0.001,patience=2, dropout_rate=0.5,verbose=0,savepath='./VGG19.h5',history=history_this,input_shape=INPUT_SIZE,timer=timer):\n",
    "    model_fine_tune = VGG19(include_top=False, weights='imagenet', input_shape=input_shape)\n",
    "    for layer in model_fine_tune.layers[:frozen]:\t#the number of frozen layers for transfer learning, have tuned from 5-18\n",
    "        layer.trainable = False\n",
    "    for layer in model_fine_tune.layers[frozen:]:\n",
    "        layer.trainable = True\n",
    "    model = GlobalAveragePooling2D()(model_fine_tune.output)\n",
    "    model=Dense(units=256,activation='relu')(model)\n",
    "    model=Dropout(dropout_rate)(model)\n",
    "    model = Dense(num_class, activation='softmax')(model)\n",
    "    model = Model(model_fine_tune.input, model, name='vgg')\n",
    "    opt = keras.optimizers.Adam(lr=lr, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\t#tuned learning rate to be 0.001\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\t#set the loss function to be binary crossentropy\n",
    "    #train model\n",
    "    earlyStopping = kcallbacks.EarlyStopping(\n",
    "        monitor='val_acc', patience=patience, verbose=verbose, mode='auto')\t#set early stop patience to save training time\n",
    "    saveBestModel = kcallbacks.ModelCheckpoint(\n",
    "        filepath=savepath,\n",
    "        monitor='val_acc',\n",
    "        verbose=verbose,\n",
    "        save_best_only=True,\n",
    "        mode='auto')\n",
    "    hist = model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=len(train_generator),\n",
    "        epochs=epochs,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=len(validation_generator),\n",
    "        verbose=verbose,\n",
    "        #use_multiprocessing=True, \n",
    "        #workers=2,\n",
    "        callbacks=[earlyStopping, saveBestModel, history, timer],\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the objective function to be optimized\n",
    "import time\n",
    "from hyperopt import hp, fmin, tpe, rand, STATUS_OK, Trials\n",
    "import matplotlib.pyplot as plt\n",
    "import statistics \n",
    "\n",
    "def objective(params, num_class=num_class):\n",
    "    \n",
    "    params = {\n",
    "        'frozen': int(params['frozen']),\n",
    "        'epochs': int(params['epochs']),\n",
    "        'patience': int(params['patience']),\n",
    "        'lr': abs(float(params['lr'])),\n",
    "        'dropout_rate': abs(float(params['dropout_rate'])),\n",
    "    }\n",
    "    # frozen=params['frozen']\n",
    "    # epochs=params['epochs']\n",
    "    # patience=params['patience']\n",
    "    # lr=params['lr']\n",
    "    # dropout_rate=params['dropout_rate']\n",
    "\n",
    "    # vgg16(num_class=num_class, frozen=frozen,epochs=epochs,patience=patience, lr=lr, dropout_rate=dropout_rate)\n",
    "\n",
    "    vgg19(num_class=num_class, **params)\n",
    "\n",
    "    acc=prediction(model=load_model('./VGG19.h5'))\n",
    "\n",
    "    print('accuracy:%s'%acc)\n",
    "    return {'loss': -acc, 'status': STATUS_OK }\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BO-TPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameter optimization by Bayesian optimization - Tree Parzen Estimator\n",
    "space = {\n",
    "    'frozen': hp.quniform('frozen', 15, 18, 1),\n",
    "    'epochs': hp.quniform('epochs', 5, 21, 5),\n",
    "    'patience': hp.quniform('patience', 2, 4, 1),\n",
    "    'lr': hp.quniform('lr', 0.001, 0.006, 0.001),\n",
    "    'dropout_rate': hp.quniform('dropout_rate', 0.3, 0.6, 0.1),\n",
    "}\n",
    "\n",
    "t1=time.time()\n",
    "best = fmin(fn=objective,\n",
    "            space=space,\n",
    "            algo=tpe.suggest,\n",
    "            max_evals=10)\n",
    "print(\"Hyperopt estimated optimum {}\".format(best))\n",
    "t2=time.time()\n",
    "print(\"Time: \"+str(t2-t1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrain the model by using the best hyperparameter values to obtain the best model\n",
    "params = {\n",
    "        'frozen': int(best['frozen']),\n",
    "        'epochs': int(best['epochs']),\n",
    "        'patience': int(best['patience']),\n",
    "        'lr': abs(float(best['lr'])),\n",
    "        'dropout_rate': abs(float(best['dropout_rate'])),\n",
    "    }\n",
    "vgg19(num_class=num_class, verbose=1, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.add('VGG19 (BO-TPE)', hpo_time=t2-t1, train_time=timer.get_processing_time(), **history_this.get_best())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameter optimization by Random search\n",
    "space = {\n",
    "    'frozen': hp.quniform('frozen', 15, 18, 1),\n",
    "    'epochs': hp.quniform('epochs', 5, 21, 5),\n",
    "    'patience': hp.quniform('patience', 2, 4, 1),\n",
    "    'lr': hp.quniform('lr', 0.001, 0.006, 0.001),\n",
    "    'dropout_rate': hp.quniform('dropout_rate', 0.3, 0.6, 0.1),\n",
    "}\n",
    "\n",
    "t1=time.time()\n",
    "best = fmin(fn=objective,\n",
    "            space=space,\n",
    "            algo=rand.suggest,\n",
    "            max_evals=10)\n",
    "print(\"Hyperopt estimated optimum {}\".format(best))\n",
    "t2=time.time()\n",
    "print(\"Time: \"+str(t2-t1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrain the model by using the best hyperparameter values to obtain the best model\n",
    "params = {\n",
    "        'frozen': int(best['frozen']),\n",
    "        'epochs': int(best['epochs']),\n",
    "        'patience': int(best['patience']),\n",
    "        'lr': abs(float(best['lr'])),\n",
    "        'dropout_rate': abs(float(best['dropout_rate'])),\n",
    "    }\n",
    "vgg19(num_class=num_class, verbose=1, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.add('VGG19 (Random Search)', hpo_time=t2-t1, train_time=timer.get_processing_time(), **history_this.get_best())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resnet( num_class, epochs=20,frozen=120,lr=0.001,patience=2, dropout_rate=0.5,verbose=0,savepath='./resnet.h5',history=history_this,input_shape=INPUT_SIZE,timer=timer):\n",
    "    model_fine_tune = ResNet50(include_top=False, weights='imagenet', input_shape=input_shape)\n",
    "    for layer in model_fine_tune.layers[:frozen]:\t#the number of frozen layers for transfer learning, have tuned from 50-150\n",
    "        layer.trainable = False\n",
    "    for layer in model_fine_tune.layers[frozen:]:\t#the number of trainable layers for transfer learning\n",
    "        layer.trainable = True\n",
    "    model = GlobalAveragePooling2D()(model_fine_tune.output)\n",
    "    model=Dense(units=256,activation='relu')(model)\n",
    "    model=Dropout(dropout_rate)(model)\n",
    "    model = Dense(num_class, activation='softmax')(model)\n",
    "    model = Model(model_fine_tune.input, model, name='resnet')\n",
    "    opt = keras.optimizers.Adam(lr=lr, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\t#tuned learning rate to be 0.001\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy']) #set the loss function to be binary crossentropy\n",
    "    #train model\n",
    "    earlyStopping = kcallbacks.EarlyStopping(\n",
    "        monitor='val_acc', patience=patience, verbose=verbose, mode='auto')\t#set early stop patience to save training time\n",
    "    saveBestModel = kcallbacks.ModelCheckpoint(\n",
    "        filepath=savepath,\n",
    "        monitor='val_acc',\n",
    "        verbose=verbose,\n",
    "        save_best_only=True,\n",
    "        mode='auto')\n",
    "    hist = model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=len(train_generator),\n",
    "        epochs=epochs,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=len(validation_generator),\n",
    "        #use_multiprocessing=True, \n",
    "        verbose=verbose,\n",
    "        callbacks=[earlyStopping, saveBestModel, history, timer],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the objective function to be optimized\n",
    "import time\n",
    "from hyperopt import hp, fmin, tpe, rand, STATUS_OK, Trials\n",
    "import matplotlib.pyplot as plt\n",
    "import statistics \n",
    "\n",
    "def objective(params, num_class=num_class):\n",
    "    \n",
    "    params = {\n",
    "        'frozen': int(params['frozen']),\n",
    "        'epochs': int(params['epochs']),\n",
    "        'patience': int(params['patience']),\n",
    "        'lr': abs(float(params['lr'])),\n",
    "        'dropout_rate': abs(float(params['dropout_rate'])),\n",
    "    }\n",
    "    # frozen=params['frozen']\n",
    "    # epochs=params['epochs']\n",
    "    # patience=params['patience']\n",
    "    # lr=params['lr']\n",
    "    # dropout_rate=params['dropout_rate']\n",
    "\n",
    "    # vgg16(num_class=num_class, frozen=frozen,epochs=epochs,patience=patience, lr=lr, dropout_rate=dropout_rate)\n",
    "\n",
    "    resnet(num_class=num_class, **params)\n",
    "\n",
    "    acc=prediction(model=load_model('./resnet.h5'))\n",
    "\n",
    "    print('accuracy:%s'%acc)\n",
    "    return {'loss': -acc, 'status': STATUS_OK }\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BO-TPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameter optimization by Bayesian optimization - Tree Parzen Estimator\n",
    "space = {\n",
    "    'frozen': hp.quniform('frozen', 50, 150, 10),\n",
    "    'epochs': hp.quniform('epochs', 5, 21, 5),\n",
    "    'patience': hp.quniform('patience', 2, 4, 1),\n",
    "    'lr': hp.quniform('lr', 0.001, 0.006, 0.001),\n",
    "    'dropout_rate': hp.quniform('dropout_rate', 0.3, 0.6, 0.1),\n",
    "}\n",
    "\n",
    "t1=time.time()\n",
    "best = fmin(fn=objective,\n",
    "            space=space,\n",
    "            algo=tpe.suggest,\n",
    "            max_evals=10)\n",
    "print(\"Hyperopt estimated optimum {}\".format(best))\n",
    "t2=time.time()\n",
    "print(\"Time: \"+str(t2-t1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrain the model by using the best hyperparameter values to obtain the best model\n",
    "params = {\n",
    "        'frozen': int(best['frozen']),\n",
    "        'epochs': int(best['epochs']),\n",
    "        'patience': int(best['patience']),\n",
    "        'lr': abs(float(best['lr'])),\n",
    "        'dropout_rate': abs(float(best['dropout_rate'])),\n",
    "    }\n",
    "resnet(num_class=num_class, verbose=1, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.add('ResNet (BO-TPE)', hpo_time=t2-t1, train_time=timer.get_processing_time(), **history_this.get_best())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameter optimization by Random search\n",
    "space = {\n",
    "    'frozen': hp.quniform('frozen', 50, 150, 10),\n",
    "    'epochs': hp.quniform('epochs', 5, 21, 5),\n",
    "    'patience': hp.quniform('patience', 2, 4, 1),\n",
    "    'lr': hp.quniform('lr', 0.001, 0.006, 0.001),\n",
    "    'dropout_rate': hp.quniform('dropout_rate', 0.3, 0.6, 0.1),\n",
    "}\n",
    "\n",
    "t1=time.time()\n",
    "best = fmin(fn=objective,\n",
    "            space=space,\n",
    "            algo=rand.suggest,\n",
    "            max_evals=10)\n",
    "print(\"Hyperopt estimated optimum {}\".format(best))\n",
    "t2=time.time()\n",
    "print(\"Time: \"+str(t2-t1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrain the model by using the best hyperparameter values to obtain the best model\n",
    "params = {\n",
    "        'frozen': int(best['frozen']),\n",
    "        'epochs': int(best['epochs']),\n",
    "        'patience': int(best['patience']),\n",
    "        'lr': abs(float(best['lr'])),\n",
    "        'dropout_rate': abs(float(best['dropout_rate'])),\n",
    "    }\n",
    "resnet(num_class=num_class, verbose=1, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.add('ResNet (Random Search)', hpo_time=t2-t1, train_time=timer.get_processing_time(), **history_this.get_best())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inception( num_class, epochs=20,frozen=120,lr=0.001,patience=2, dropout_rate=0.5,verbose=0,savepath='./inception.h5',history=history_this,input_shape=INPUT_SIZE, timer=timer):\n",
    "    model_fine_tune = InceptionV3(include_top=False, weights='imagenet', input_shape=input_shape)\n",
    "    for layer in model_fine_tune.layers[:frozen]:\t#the number of frozen layers for transfer learning, have tuned from 50-150\n",
    "        layer.trainable = False\n",
    "    for layer in model_fine_tune.layers[frozen:]:\t#the number of trainable layers for transfer learning\n",
    "        layer.trainable = True\n",
    "    model = GlobalAveragePooling2D()(model_fine_tune.output)\n",
    "    model=Dense(units=256,activation='relu')(model)\n",
    "    model=Dropout(dropout_rate)(model)\n",
    "    model = Dense(num_class, activation='softmax')(model)\n",
    "    model = Model(model_fine_tune.input, model, name='resnet')\n",
    "    opt = keras.optimizers.Adam(lr=lr, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\t#tuned learning rate to be 0.001\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy']) #set the loss function to be binary crossentropy\n",
    "    #train model\n",
    "    earlyStopping = kcallbacks.EarlyStopping(\n",
    "        monitor='val_acc', patience=patience, verbose=verbose, mode='auto')\t#set early stop patience to save training time\n",
    "    saveBestModel = kcallbacks.ModelCheckpoint(\n",
    "        filepath=savepath,\n",
    "        monitor='val_acc',\n",
    "        verbose=verbose,\n",
    "        save_best_only=True,\n",
    "        mode='auto')\n",
    "    hist = model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=len(train_generator),\n",
    "        epochs=epochs,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=len(validation_generator),\n",
    "        #use_multiprocessing=True, \n",
    "        verbose=verbose,\n",
    "        callbacks=[earlyStopping, saveBestModel, history, timer],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the objective function to be optimized\n",
    "import time\n",
    "from hyperopt import hp, fmin, tpe, rand, STATUS_OK, Trials\n",
    "import matplotlib.pyplot as plt\n",
    "import statistics \n",
    "\n",
    "def objective(params, num_class=num_class):\n",
    "    \n",
    "    params = {\n",
    "        'frozen': int(params['frozen']),\n",
    "        'epochs': int(params['epochs']),\n",
    "        'patience': int(params['patience']),\n",
    "        'lr': abs(float(params['lr'])),\n",
    "        'dropout_rate': abs(float(params['dropout_rate'])),\n",
    "    }\n",
    "    # frozen=params['frozen']\n",
    "    # epochs=params['epochs']\n",
    "    # patience=params['patience']\n",
    "    # lr=params['lr']\n",
    "    # dropout_rate=params['dropout_rate']\n",
    "\n",
    "    # vgg16(num_class=num_class, frozen=frozen,epochs=epochs,patience=patience, lr=lr, dropout_rate=dropout_rate)\n",
    "\n",
    "    inception(num_class=num_class, **params)\n",
    "\n",
    "    acc=prediction(model=load_model('./inception.h5'))\n",
    "\n",
    "    print('accuracy:%s'%acc)\n",
    "    return {'loss': -acc, 'status': STATUS_OK }\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BO-TPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameter optimization by Bayesian optimization - Tree Parzen Estimator\n",
    "space = {\n",
    "    'frozen': hp.quniform('frozen', 50, 150, 10),\n",
    "    'epochs': hp.quniform('epochs', 5, 21, 5),\n",
    "    'patience': hp.quniform('patience', 2, 4, 1),\n",
    "    'lr': hp.quniform('lr', 0.001, 0.006, 0.001),\n",
    "    'dropout_rate': hp.quniform('dropout_rate', 0.3, 0.6, 0.1),\n",
    "}\n",
    "\n",
    "t1=time.time()\n",
    "best = fmin(fn=objective,\n",
    "            space=space,\n",
    "            algo=tpe.suggest,\n",
    "            max_evals=10)\n",
    "print(\"Hyperopt estimated optimum {}\".format(best))\n",
    "t2=time.time()\n",
    "print(\"Time: \"+str(t2-t1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrain the model by using the best hyperparameter values to obtain the best model\n",
    "params = {\n",
    "        'frozen': int(best['frozen']),\n",
    "        'epochs': int(best['epochs']),\n",
    "        'patience': int(best['patience']),\n",
    "        'lr': abs(float(best['lr'])),\n",
    "        'dropout_rate': abs(float(best['dropout_rate'])),\n",
    "    }\n",
    "inception(num_class=num_class, verbose=1, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.add('Inception (BO-TPE)', hpo_time=t2-t1, train_time=timer.get_processing_time(), **history_this.get_best())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameter optimization by Random search\n",
    "space = {\n",
    "    'frozen': hp.quniform('frozen', 50, 150, 10),\n",
    "    'epochs': hp.quniform('epochs', 5, 21, 5),\n",
    "    'patience': hp.quniform('patience', 2, 4, 1),\n",
    "    'lr': hp.quniform('lr', 0.001, 0.006, 0.001),\n",
    "    'dropout_rate': hp.quniform('dropout_rate', 0.3, 0.6, 0.1),\n",
    "}\n",
    "\n",
    "t1=time.time()\n",
    "best = fmin(fn=objective,\n",
    "            space=space,\n",
    "            algo=rand.suggest,\n",
    "            max_evals=10)\n",
    "print(\"Hyperopt estimated optimum {}\".format(best))\n",
    "t2=time.time()\n",
    "print(\"Time: \"+str(t2-t1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrain the model by using the best hyperparameter values to obtain the best model\n",
    "params = {\n",
    "        'frozen': int(best['frozen']),\n",
    "        'epochs': int(best['epochs']),\n",
    "        'patience': int(best['patience']),\n",
    "        'lr': abs(float(best['lr'])),\n",
    "        'dropout_rate': abs(float(best['dropout_rate'])),\n",
    "    }\n",
    "inception(num_class=num_class, verbose=1, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.add('Inception (Random Search)', hpo_time=t2-t1, train_time=timer.get_processing_time(), **history_this.get_best())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## InceptionResnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inceptionresnet( num_class, epochs=20,frozen=120,lr=0.001,patience=2, dropout_rate=0.5,verbose=0,savepath='./inceptionresnet.h5',history=history_this,input_shape=INPUT_SIZE, timer=timer):\n",
    "    model_fine_tune = InceptionResNetV2(include_top=False, weights='imagenet', input_shape=input_shape)\n",
    "    for layer in model_fine_tune.layers[:frozen]:\t#the number of frozen layers for transfer learning, have tuned from 400-550\n",
    "        layer.trainable = False\n",
    "    for layer in model_fine_tune.layers[frozen:]:\t#the number of trainable layers for transfer learning\n",
    "        layer.trainable = True\n",
    "    model = GlobalAveragePooling2D()(model_fine_tune.output)\n",
    "    model=Dense(units=256,activation='relu')(model)\n",
    "    model=Dropout(dropout_rate)(model)\n",
    "    model = Dense(num_class, activation='softmax')(model)\n",
    "    model = Model(model_fine_tune.input, model, name='resnet')\n",
    "    opt = keras.optimizers.Adam(lr=lr, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\t#tuned learning rate to be 0.001\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy']) #set the loss function to be binary crossentropy\n",
    "    #train model\n",
    "    earlyStopping = kcallbacks.EarlyStopping(\n",
    "        monitor='val_acc', patience=patience, verbose=verbose, mode='auto')\t#set early stop patience to save training time\n",
    "    saveBestModel = kcallbacks.ModelCheckpoint(\n",
    "        filepath=savepath,\n",
    "        monitor='val_acc',\n",
    "        verbose=verbose,\n",
    "        save_best_only=True,\n",
    "        mode='auto')\n",
    "    hist = model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=len(train_generator),\n",
    "        epochs=epochs,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=len(validation_generator),\n",
    "        #use_multiprocessing=True, \n",
    "        verbose=verbose,\n",
    "        callbacks=[earlyStopping, saveBestModel, history, timer],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the objective function to be optimized\n",
    "import time\n",
    "from hyperopt import hp, fmin, tpe, rand, STATUS_OK, Trials\n",
    "import matplotlib.pyplot as plt\n",
    "import statistics \n",
    "\n",
    "def objective(params, num_class=num_class):\n",
    "    \n",
    "    params = {\n",
    "        'frozen': int(params['frozen']),\n",
    "        'epochs': int(params['epochs']),\n",
    "        'patience': int(params['patience']),\n",
    "        'lr': abs(float(params['lr'])),\n",
    "        'dropout_rate': abs(float(params['dropout_rate'])),\n",
    "    }\n",
    "    # frozen=params['frozen']\n",
    "    # epochs=params['epochs']\n",
    "    # patience=params['patience']\n",
    "    # lr=params['lr']\n",
    "    # dropout_rate=params['dropout_rate']\n",
    "\n",
    "    # vgg16(num_class=num_class, frozen=frozen,epochs=epochs,patience=patience, lr=lr, dropout_rate=dropout_rate)\n",
    "\n",
    "    inceptionresnet(num_class=num_class, **params)\n",
    "\n",
    "    acc=prediction(model=load_model('./inceptionresnet.h5'))\n",
    "\n",
    "    print('accuracy:%s'%acc)\n",
    "    return {'loss': -acc, 'status': STATUS_OK }\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BO-TPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameter optimization by Bayesian optimization - Tree Parzen Estimator\n",
    "space = {\n",
    "    'frozen': hp.quniform('frozen', 400, 500, 10),\n",
    "    'epochs': hp.quniform('epochs', 5, 21, 5),\n",
    "    'patience': hp.quniform('patience', 2, 4, 1),\n",
    "    'lr': hp.quniform('lr', 0.001, 0.006, 0.001),\n",
    "    'dropout_rate': hp.quniform('dropout_rate', 0.3, 0.6, 0.1),\n",
    "}\n",
    "\n",
    "t1=time.time()\n",
    "best = fmin(fn=objective,\n",
    "            space=space,\n",
    "            algo=tpe.suggest,\n",
    "            max_evals=10)\n",
    "print(\"Hyperopt estimated optimum {}\".format(best))\n",
    "t2=time.time()\n",
    "print(\"Time: \"+str(t2-t1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrain the model by using the best hyperparameter values to obtain the best model\n",
    "params = {\n",
    "        'frozen': int(best['frozen']),\n",
    "        'epochs': int(best['epochs']),\n",
    "        'patience': int(best['patience']),\n",
    "        'lr': abs(float(best['lr'])),\n",
    "        'dropout_rate': abs(float(best['dropout_rate'])),\n",
    "    }\n",
    "inceptionresnet(num_class=num_class, verbose=1, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.add('InceptionResnet (BO-TPE)', hpo_time=t2-t1, train_time=timer.get_processing_time(), **history_this.get_best())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameter optimization by Random search\n",
    "space = {\n",
    "    'frozen': hp.quniform('frozen', 400, 500, 10),\n",
    "    'epochs': hp.quniform('epochs', 5, 21, 5),\n",
    "    'patience': hp.quniform('patience', 2, 4, 1),\n",
    "    'lr': hp.quniform('lr', 0.001, 0.006, 0.001),\n",
    "    'dropout_rate': hp.quniform('dropout_rate', 0.3, 0.6, 0.1),\n",
    "}\n",
    "\n",
    "t1=time.time()\n",
    "best = fmin(fn=objective,\n",
    "            space=space,\n",
    "            algo=rand.suggest,\n",
    "            max_evals=10)\n",
    "print(\"Hyperopt estimated optimum {}\".format(best))\n",
    "t2=time.time()\n",
    "print(\"Time: \"+str(t2-t1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrain the model by using the best hyperparameter values to obtain the best model\n",
    "params = {\n",
    "        'frozen': int(best['frozen']),\n",
    "        'epochs': int(best['epochs']),\n",
    "        'patience': int(best['patience']),\n",
    "        'lr': abs(float(best['lr'])),\n",
    "        'dropout_rate': abs(float(best['dropout_rate'])),\n",
    "    }\n",
    "inceptionresnet(num_class=num_class, verbose=1, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.add('InceptionResnet (Random Search)', hpo_time=t2-t1, train_time=timer.get_processing_time(), **history_this.get_best())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.to_excel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Online GPU renting platform specification\n",
    "# WeChat Message\n",
    "import requests\n",
    "resp = requests.get(\n",
    "    \"https://www.autodl.com/api/v1/wechat/message/push?token={token}&title={title}&name={name}&content={content}\".format(\n",
    "        token=\"add7ba8eea4b\",\n",
    "        title=\"From AutoDL\",\n",
    "        name=\"UNSW-NB15 CNN\",\n",
    "        content=\"Training Complete\")\n",
    ")\n",
    "print(resp.content.decode())\n",
    "# Shutdown\n",
    "!shutdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "9af53353804afc09f90d8f022eaec6e99fffff96cfdda280d845f608b681a8ce"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
