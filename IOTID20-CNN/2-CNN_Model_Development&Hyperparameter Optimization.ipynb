{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Transfer Learning and Optimized CNN Based Intrusion Detection System for Internet of Vehicles \n",
    "This is the code for the paper entitled \"**A Transfer Learning and Optimized CNN Based Intrusion Detection System for Internet of Vehicles**\" accepted in IEEE International Conference on Communications (IEEE ICC).  \n",
    "Authors: Li Yang (lyang339@uwo.ca) and Abdallah Shami (Abdallah.Shami@uwo.ca)  \n",
    "Organization: The Optimized Computing and Communications (OC2) Lab, ECE Department, Western University\n",
    "\n",
    "**Notebook 2: CNN Model Development**  \n",
    "Aims:  \n",
    "&nbsp; 1): Generate training and test images  \n",
    "&nbsp; 2): Construct CNN models (a CNN model by own, Xception, VGG16, VGG19, Resnet, Inception, InceptionResnet)  \n",
    "&nbsp; 3): Tune the hyperparameters of CNN models (hyperparameter optimization)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.inception_resnet_v2 import InceptionResNetV2\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
    "from tensorflow.keras.applications.mobilenet import MobileNet\n",
    "from tensorflow.keras.applications.resnet50 import  ResNet50\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.applications.vgg19 import VGG19\n",
    "from tensorflow.keras.applications.xception import  Xception\n",
    "from tensorflow.keras.layers import Dense,Flatten,GlobalAveragePooling2D,Input,Conv2D,MaxPooling2D,Dropout\n",
    "from tensorflow.keras.models import Model,load_model,Sequential\n",
    "from tensorflow.keras.preprocessing.image import  ImageDataGenerator\n",
    "from tensorflow.keras.preprocessing.image import load_img,img_to_array\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from PIL import Image\n",
    "from sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score,precision_recall_fscore_support\n",
    "import datetime\n",
    "import time\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow.keras.callbacks as kcallbacks\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import sklearn.metrics as metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Training and Test Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2076 images belonging to 5 classes.\n",
      "Found 518 images belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "#generate training and test images\n",
    "TARGET_SIZE=(224,224)\n",
    "INPUT_SIZE=(224,224,3)\n",
    "BATCHSIZE=128\t#could try 128 or 32\n",
    "\n",
    "train_rootdir = './train_224/'\n",
    "test_rootdir = './test_224/'\n",
    "\n",
    "#Normalization\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        train_rootdir,\n",
    "        target_size=TARGET_SIZE,\n",
    "        batch_size=BATCHSIZE,\n",
    "        class_mode='categorical')\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "        test_rootdir,\n",
    "        target_size=TARGET_SIZE,\n",
    "        batch_size=BATCHSIZE,\n",
    "        class_mode='categorical')\n",
    "num_class = train_generator.num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels = []\n",
    "test_images=[]\n",
    "for subdir, dirs, files in os.walk(test_rootdir):\n",
    "    for file in files:\n",
    "        if not (file.endswith(\".jpeg\"))|(file.endswith(\".jpg\"))|(file.endswith(\".png\")):\n",
    "            continue\n",
    "        test_labels.append(subdir.split('/')[-1])\n",
    "        test_images.append(os.path.join(subdir, file))\n",
    "\n",
    "label=validation_generator.class_indices\n",
    "label={v: k for k, v in label.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction(model, test_images=test_images, label=label, batch_size=BATCHSIZE):\n",
    "    predict=[]\n",
    "    length=len(test_images)\n",
    "    for i in range(((length-1)//batch_size)+1):\n",
    "        inputimg=test_images[batch_size*i:batch_size*(i+1)]\n",
    "        test_batch=[]\n",
    "        for path in inputimg:\n",
    "            thisimg=np.array(Image.open(path))/255\n",
    "            test_batch.append(thisimg)\n",
    "        # thisimg=np.array(Image.open(inputimg))/255 #read all the images in validation set\n",
    "        #print(thisimg)\n",
    "        # test_shape=(1,)+thisimg.shape\n",
    "        # thisimg=thisimg.reshape(test_shape)\n",
    "        model_batch=model.predict(np.array(test_batch)) #use master model to process the input image\n",
    "        #generate result by model 1\n",
    "        # prob=model_batch[0,np.argmax(model_batch,axis=1)[0]]\n",
    "        predict_batch=list(np.argmax(model_batch,axis=1))\n",
    "        predict_batch=[label[con] for con in predict_batch]\n",
    "        predict.extend(predict_batch)\n",
    "    return predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the image plotting functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the figures\n",
    "#when extra_data enabled please put this callback before early_stopping in case of any problem\n",
    "class LossHistory(keras.callbacks.Callback):\n",
    "    def __init__(self, need_extra_data:bool=True, test_images=test_images, test_labels=test_labels, label=label):\n",
    "        # Enable the recording of precision, recall, f1-score (only epoch-wise)\n",
    "        self.extra_data = need_extra_data\n",
    "        if need_extra_data:\n",
    "            self.test_images = test_images\n",
    "            self.test_labels = test_labels\n",
    "            self.label = label\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = {'batch':[], 'epoch':[]}\n",
    "        self.accuracy = {'batch':[], 'epoch':[]}\n",
    "        self.val_loss = {'batch':[], 'epoch':[]}\n",
    "        self.val_acc = {'batch':[], 'epoch':[]}\n",
    "        # need extra data --> reset recording list\n",
    "        # These matrics only make sense over the entire epoch\n",
    "        if self.extra_data:\n",
    "            self.precision = []\n",
    "            self.recall = []\n",
    "            self.f1_score = []\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        self.losses['batch'].append(logs.get('loss'))\n",
    "        self.accuracy['batch'].append(logs.get('acc'))\n",
    "        self.val_loss['batch'].append(logs.get('val_loss'))\n",
    "        self.val_acc['batch'].append(logs.get('val_acc'))\n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        self.losses['epoch'].append(logs.get('loss'))\n",
    "        self.accuracy['epoch'].append(logs.get('acc'))\n",
    "        self.val_loss['epoch'].append(logs.get('val_loss'))\n",
    "        self.val_acc['epoch'].append(logs.get('val_acc'))\n",
    "        # need extra data --> calculate and record\n",
    "        if self.extra_data:\n",
    "            # Get prediciton\n",
    "            temp = self.model.stop_training\n",
    "            y_pred = get_prediction(model=self.model, test_images=self.test_images, label=self.label)\n",
    "            self.model.stop_training = temp\n",
    "            # Calculate extra data\n",
    "            precision,recall,fscore,_= precision_recall_fscore_support(self.test_labels, y_pred, average='weighted', zero_division=0)\n",
    "            # Record\n",
    "            self.precision.append(precision)\n",
    "            self.recall.append(recall)\n",
    "            self.f1_score.append(fscore)\n",
    "    def loss_plot(self, loss_type):\n",
    "        iters = range(len(self.losses[loss_type]))\n",
    "        plt.figure()\n",
    "        # plt.plot(iters, self.losses[loss_type], 'g', label='train loss')\n",
    "        if loss_type == 'epoch':\n",
    "            # acc\n",
    "            plt.plot(iters, self.accuracy[loss_type], 'r', label='train acc')\n",
    "            # loss\n",
    "            plt.plot(iters, self.losses[loss_type], 'g', label='train loss')\n",
    "            # val_acc\n",
    "            plt.plot(iters, self.val_acc[loss_type], 'b', label='val acc')\n",
    "            # val_loss\n",
    "            plt.plot(iters, self.val_loss[loss_type], 'k', label='val loss')\n",
    "        else:\n",
    "            plt.plot(iters, self.losses[loss_type], 'g', label='train loss')\n",
    "        plt.grid(True)\n",
    "        plt.xlabel(loss_type)\n",
    "        plt.ylabel('acc-loss')\n",
    "        plt.legend(loc=\"upper right\")\n",
    "        plt.show()\n",
    "    def get_best(self, target_type:str='epoch', need_extra_data:bool=True):\n",
    "        # Get the index of the best record\n",
    "        max_index = self.accuracy[target_type].index(max(self.accuracy[target_type]))\n",
    "        # Return the accuracy, loss, val_acc, val_loss of the best record\n",
    "        temp={\n",
    "            'accuracy': self.accuracy[target_type][max_index], \n",
    "            'loss': self.losses[target_type][max_index], \n",
    "            'val_acc': self.val_acc[target_type][max_index], \n",
    "            'val_loss': self.val_loss[target_type][max_index]\n",
    "            }\n",
    "        # Add extra data if needed and available\n",
    "        if self.extra_data and need_extra_data and target_type=='epoch':\n",
    "            temp['precision']=self.precision[max_index]\n",
    "            temp['recall']=self.recall[max_index]\n",
    "            temp['f1-score']=self.f1_score[max_index]\n",
    "        return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "history_this = LossHistory(need_extra_data=True)\n",
    "history_hpo = LossHistory(need_extra_data=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the processing time measurement functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure the running time of model training\n",
    "class TimeMeasurement(keras.callbacks.Callback):\n",
    "    def __init__(self):\n",
    "        self.start_time=None\n",
    "        self.stop_time=None\n",
    "    # Start timing when trainning starts\n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.start_time=time.time()\n",
    "        self.stop_time=None\n",
    "    # Stop timing when trainning ends\n",
    "    def on_train_end(self, logs=None):\n",
    "        self.stop_time=time.time()\n",
    "        if self.start_time is None: print(\"Time Measuring Failed\")\n",
    "    # Get processing time\n",
    "    def get_processing_time(self):\n",
    "        if (self.start_time is None or self.stop_time is None): raise Exception(\"Wrong Time Measurement\")\n",
    "        else: return self.stop_time-self.start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "timer = TimeMeasurement()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the output sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class output_sheet:\n",
    "    def __init__(self, columns:list=['accuracy', 'loss', 'val_acc', 'val_loss', 'precision', 'recall', 'f1-score', 'hpo_time', 'train_time']):\n",
    "        self.output_df = pd.DataFrame(columns=columns)\n",
    "        # self.output_index = list()\n",
    "    def add(self, item:str, **values:dict):\n",
    "        # self.output_df = self.output_df.append(values, ignore_index=True)\n",
    "        temp = pd.DataFrame(values, columns=self.output_df.columns.to_list(), index=[item])\n",
    "        self.output_df = pd.concat([self.output_df, temp], axis=0)\n",
    "        # self.output_index.append(item)\n",
    "    # def apply_index(self):\n",
    "    #     self.output_df.index = self.output_index\n",
    "    def to_excel(self, path=None):\n",
    "        if path is None: path='2-result-{}.xlsx'.format(datetime.datetime.now().strftime('%y%m%d-%H%M%S'))\n",
    "        # self.apply_index()\n",
    "        self.output_df.to_excel(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = output_sheet(columns=['accuracy', 'loss', 'val_acc', 'val_loss', 'precision', 'recall', 'f1-score', 'hpo_time', 'train_time'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct CNN models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1: a CNN model by own (baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_by_own(input_shape,num_class,epochs,savepath='./model_own.h5',history=history_this,timer=timer):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(64,(3,3),strides=(1,1),input_shape=input_shape,padding='same',activation='relu',kernel_initializer='glorot_uniform'))\n",
    "    model.add(Conv2D(64,(3,3),strides=(1,1),padding='same',activation='relu',kernel_initializer='glorot_uniform'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Conv2D(128,(3,3),strides=(1,1),padding='same',activation='relu',kernel_initializer='glorot_uniform'))\n",
    "    model.add(Conv2D(128,(3,3),strides=(1,1),padding='same',activation='relu',kernel_initializer='glorot_uniform'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Conv2D(256,(3,3),strides=(1,1),padding='same',activation='relu',kernel_initializer='glorot_uniform'))\n",
    "    model.add(Conv2D(256,(3,3),strides=(1,1),padding='same',activation='relu',kernel_initializer='glorot_uniform'))\n",
    "    model.add(Conv2D(256,(3,3),strides=(1,1),padding='same',activation='relu',kernel_initializer='glorot_uniform'))\n",
    "    model.add(GlobalAveragePooling2D())\n",
    "    model.add(Dense(256,activation='relu'))\n",
    "    model.add(Dropout(rate=0.5))\n",
    "    model.add(Dense(num_class,activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "    #train model\n",
    "    earlyStopping=kcallbacks.EarlyStopping(monitor='val_acc', patience=2, verbose=1, mode='auto')\n",
    "    saveBestModel = kcallbacks.ModelCheckpoint(filepath=savepath, monitor='val_acc', verbose=1, save_best_only=True, mode='auto')\n",
    "    hist=model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=len(train_generator),\n",
    "        epochs=epochs,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=len(validation_generator),\n",
    "        callbacks=[history, timer, earlyStopping, saveBestModel],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\FOCUS\\.conda\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1940: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "17/17 [==============================] - 29s 768ms/step - loss: 1.1996 - accuracy: 0.6604 - val_loss: 1.1388 - val_accuracy: 0.6293\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 2/20\n",
      "17/17 [==============================] - 6s 397ms/step - loss: 0.9522 - accuracy: 0.6734 - val_loss: 0.9367 - val_accuracy: 0.6293\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 3/20\n",
      "17/17 [==============================] - 6s 358ms/step - loss: 0.7108 - accuracy: 0.7466 - val_loss: 0.5279 - val_accuracy: 0.8494\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 4/20\n",
      "17/17 [==============================] - 6s 362ms/step - loss: 0.5083 - accuracy: 0.8483 - val_loss: 0.4551 - val_accuracy: 0.8494\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 5/20\n",
      "17/17 [==============================] - 6s 361ms/step - loss: 0.4086 - accuracy: 0.8738 - val_loss: 0.3606 - val_accuracy: 0.8514\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 6/20\n",
      "17/17 [==============================] - 6s 361ms/step - loss: 0.3107 - accuracy: 0.8897 - val_loss: 0.2457 - val_accuracy: 0.8514\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 7/20\n",
      "17/17 [==============================] - 6s 373ms/step - loss: 0.2760 - accuracy: 0.8969 - val_loss: 0.2332 - val_accuracy: 0.8649\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 8/20\n",
      "17/17 [==============================] - 6s 370ms/step - loss: 0.1549 - accuracy: 0.9432 - val_loss: 0.0871 - val_accuracy: 0.9517\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 9/20\n",
      "17/17 [==============================] - 6s 371ms/step - loss: 0.9452 - accuracy: 0.7683 - val_loss: 0.7745 - val_accuracy: 0.6795\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 10/20\n",
      "17/17 [==============================] - 6s 369ms/step - loss: 0.5295 - accuracy: 0.8295 - val_loss: 0.3869 - val_accuracy: 0.8514\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 11/20\n",
      "17/17 [==============================] - 6s 365ms/step - loss: 0.2740 - accuracy: 0.8902 - val_loss: 0.1993 - val_accuracy: 0.9054\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 12/20\n",
      "17/17 [==============================] - 6s 367ms/step - loss: 0.1531 - accuracy: 0.9345 - val_loss: 0.1295 - val_accuracy: 0.9363\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 13/20\n",
      "17/17 [==============================] - 6s 368ms/step - loss: 0.1398 - accuracy: 0.9412 - val_loss: 0.1810 - val_accuracy: 0.9324\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 14/20\n",
      "17/17 [==============================] - 6s 363ms/step - loss: 0.1291 - accuracy: 0.9461 - val_loss: 0.1138 - val_accuracy: 0.9363\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 15/20\n",
      "17/17 [==============================] - 6s 372ms/step - loss: 0.0967 - accuracy: 0.9547 - val_loss: 0.0813 - val_accuracy: 0.9479\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 16/20\n",
      "17/17 [==============================] - 6s 370ms/step - loss: 0.1176 - accuracy: 0.9494 - val_loss: 0.1100 - val_accuracy: 0.9363\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 17/20\n",
      "17/17 [==============================] - 6s 373ms/step - loss: 0.0978 - accuracy: 0.9504 - val_loss: 0.1044 - val_accuracy: 0.9363\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 18/20\n",
      "17/17 [==============================] - 6s 364ms/step - loss: 0.0935 - accuracy: 0.9562 - val_loss: 0.1199 - val_accuracy: 0.9363\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 19/20\n",
      "17/17 [==============================] - 6s 367ms/step - loss: 0.0964 - accuracy: 0.9542 - val_loss: 0.0546 - val_accuracy: 0.9788\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 20/20\n",
      "17/17 [==============================] - 6s 369ms/step - loss: 0.0959 - accuracy: 0.9547 - val_loss: 0.0676 - val_accuracy: 0.9884\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cnn_by_own(input_shape=INPUT_SIZE,num_class=num_class,epochs=20)\n",
    "history_this.loss_plot('epoch')\n",
    "history_this.loss_plot('batch')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation accuracy of a CNN by own: 99.884%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'>' not supported between instances of 'NoneType' and 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [13]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m output\u001b[38;5;241m.\u001b[39madd(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_own\u001b[39m\u001b[38;5;124m'\u001b[39m, train_time\u001b[38;5;241m=\u001b[39mtimer\u001b[38;5;241m.\u001b[39mget_processing_time(), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[43mhistory_this\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_best\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "Input \u001b[1;32mIn [5]\u001b[0m, in \u001b[0;36mLossHistory.get_best\u001b[1;34m(self, target_type, need_extra_data)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_best\u001b[39m(\u001b[38;5;28mself\u001b[39m, target_type:\u001b[38;5;28mstr\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m'\u001b[39m, need_extra_data:\u001b[38;5;28mbool\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;66;03m# Get the index of the best record\u001b[39;00m\n\u001b[1;32m---> 66\u001b[0m     max_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccuracy[target_type]\u001b[38;5;241m.\u001b[39mindex(\u001b[38;5;28;43mmax\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccuracy\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtarget_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     67\u001b[0m     \u001b[38;5;66;03m# Return the accuracy, loss, val_acc, val_loss of the best record\u001b[39;00m\n\u001b[0;32m     68\u001b[0m     temp\u001b[38;5;241m=\u001b[39m{\n\u001b[0;32m     69\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccuracy[target_type][max_index], \n\u001b[0;32m     70\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlosses[target_type][max_index], \n\u001b[0;32m     71\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_acc\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mval_acc[target_type][max_index], \n\u001b[0;32m     72\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mval_loss[target_type][max_index]\n\u001b[0;32m     73\u001b[0m         }\n",
      "\u001b[1;31mTypeError\u001b[0m: '>' not supported between instances of 'NoneType' and 'NoneType'"
     ]
    }
   ],
   "source": [
    "output.add('model_own', train_time=timer.get_processing_time(), **history_this.get_best())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2: Xception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xception( num_class, epochs,savepath='./xception.h5',history=history_this,input_shape=INPUT_SIZE,timer=timer):\n",
    "    model_fine_tune = Xception(include_top=False, weights='imagenet', input_shape=input_shape)\n",
    "    for layer in model_fine_tune.layers[:131]:\t\t#could be tuned to be 50, 100, or 131\n",
    "        layer.trainable = False\n",
    "    for layer in model_fine_tune.layers[131:]:\n",
    "        layer.trainable = True\n",
    "    model = GlobalAveragePooling2D()(model_fine_tune.output)\n",
    "    model=Dense(units=256,activation='relu')(model)\n",
    "    model=Dropout(0.5)(model)\n",
    "    model = Dense(num_class, activation='softmax')(model)\n",
    "    model = Model(model_fine_tune.input, model, name='xception')\n",
    "    opt = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "    #train model\n",
    "    earlyStopping = kcallbacks.EarlyStopping(\n",
    "        monitor='val_acc', patience=3, verbose=1, mode='auto')\t#patience could be tuned by 2 and 3\n",
    "    saveBestModel = kcallbacks.ModelCheckpoint(\n",
    "        filepath=savepath,\n",
    "        monitor='val_acc',\n",
    "        verbose=1,\n",
    "        save_best_only=True,\n",
    "        mode='auto')\n",
    "    hist = model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=len(train_generator),\n",
    "        epochs=epochs,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=len(validation_generator),\n",
    "        #use_multiprocessing=True, \n",
    "        callbacks=[history, timer, earlyStopping, saveBestModel],\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#default only 50, tf36cnn 99\n",
    "xception(num_class=num_class,epochs=20)\n",
    "history_this.loss_plot('epoch')\n",
    "history_this.loss_plot('batch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation accuracy of Xception: 100.0%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.add('Xception', train_time=timer.get_processing_time(), **history_this.get_best())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3: VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def vgg16( num_class, epochs,savepath='./VGG16.h5',history=history_this,input_shape=INPUT_SIZE,timer=timer):\n",
    "    model_fine_tune = VGG16(include_top=False, weights='imagenet', input_shape=input_shape)\n",
    "    for layer in model_fine_tune.layers[:15]:\t#the number of frozen layers for transfer learning, have tuned from 5-18\n",
    "        layer.trainable = False\n",
    "    for layer in model_fine_tune.layers[15:]:\n",
    "        layer.trainable = True\n",
    "    model = GlobalAveragePooling2D()(model_fine_tune.output) #GlobalAveragePooling2D layer to convert the features to a single 1280-element vector per image\n",
    "    model=Dense(units=256,activation='relu')(model)\n",
    "    model=Dropout(0.5)(model)\n",
    "    model = Dense(num_class, activation='softmax')(model)\n",
    "    model = Model(model_fine_tune.input, model, name='vgg')\n",
    "    opt = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\t#tuned learning rate to be 0.001\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\t#set the loss function to be binary crossentropy\n",
    "    #train model\n",
    "    earlyStopping = kcallbacks.EarlyStopping(\n",
    "        monitor='val_acc', patience=2, verbose=1, mode='auto')\t#set early stop patience to save training time\n",
    "    saveBestModel = kcallbacks.ModelCheckpoint(\n",
    "        filepath=savepath,\n",
    "        monitor='val_acc',\n",
    "        verbose=1,\n",
    "        save_best_only=True,\n",
    "        mode='auto')\n",
    "    hist = model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=len(train_generator),\n",
    "        epochs=epochs,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=len(validation_generator),\n",
    "        #use_multiprocessing=True, \n",
    "        #workers=2,\n",
    "        callbacks=[history, timer, earlyStopping, saveBestModel],\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vgg16(num_class=num_class,epochs=20)\t#tf36cnn\n",
    "history_this.loss_plot('epoch')\n",
    "history_this.loss_plot('batch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation accuracy of VGG16: 100.0%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.add('VGG16', train_time=timer.get_processing_time(), **history_this.get_best())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 4: VGG19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def vgg19( num_class, epochs,savepath='./VGG19.h5',history=history_this,input_shape=INPUT_SIZE,timer=timer):\n",
    "    model_fine_tune = VGG19(include_top=False, weights='imagenet', input_shape=input_shape)\n",
    "    for layer in model_fine_tune.layers[:19]:\t#the number of frozen layers for transfer learning, have tuned from 5-18\n",
    "        layer.trainable = False\n",
    "    for layer in model_fine_tune.layers[19:]:\n",
    "        layer.trainable = True\n",
    "    model = GlobalAveragePooling2D()(model_fine_tune.output)\n",
    "    model=Dense(units=256,activation='relu')(model)\n",
    "    model=Dropout(0.5)(model)\n",
    "    model = Dense(num_class, activation='softmax')(model)\n",
    "    model = Model(model_fine_tune.input, model, name='vgg')\n",
    "    opt = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\t#tuned learning rate to be 0.001\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\t#set the loss function to be binary crossentropy\n",
    "    #train model\n",
    "    earlyStopping = kcallbacks.EarlyStopping(\n",
    "        monitor='val_acc', patience=2, verbose=1, mode='auto')\t#set early stop patience to save training time\n",
    "    saveBestModel = kcallbacks.ModelCheckpoint(\n",
    "        filepath=savepath,\n",
    "        monitor='val_acc',\n",
    "        verbose=1,\n",
    "        save_best_only=True,\n",
    "        mode='auto')\n",
    "    hist = model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=len(train_generator),\n",
    "        epochs=epochs,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=len(validation_generator),\n",
    "        #use_multiprocessing=True, \n",
    "        #workers=2,\n",
    "        callbacks=[history, timer, earlyStopping, saveBestModel],\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vgg19(num_class=num_class,epochs=20)\t#binary classificaiton\n",
    "history_this.loss_plot('epoch')\n",
    "history_this.loss_plot('batch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation accuracy of VGG19: 100.0%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.add('VGG19', train_time=timer.get_processing_time(), **history_this.get_best())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 5: ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def resnet( num_class, epochs,savepath='./resnet.h5',history=history_this,input_shape=INPUT_SIZE,timer=timer):\n",
    "    model_fine_tune = ResNet50(include_top=False, weights='imagenet', input_shape=input_shape)\n",
    "    for layer in model_fine_tune.layers[:120]:\t#the number of frozen layers for transfer learning, have tuned from 50-150\n",
    "        layer.trainable = False\n",
    "    for layer in model_fine_tune.layers[120:]:\t#the number of trainable layers for transfer learning\n",
    "        layer.trainable = True\n",
    "    model = GlobalAveragePooling2D()(model_fine_tune.output)\n",
    "    model=Dense(units=256,activation='relu')(model)\n",
    "    model=Dropout(0.5)(model)\n",
    "    model = Dense(num_class, activation='softmax')(model)\n",
    "    model = Model(model_fine_tune.input, model, name='resnet')\n",
    "    opt = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\t#tuned learning rate to be 0.001\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy']) #set the loss function to be binary crossentropy\n",
    "    #train model\n",
    "    earlyStopping = kcallbacks.EarlyStopping(\n",
    "        monitor='val_acc', patience=2, verbose=1, mode='auto')\t#set early stop patience to save training time\n",
    "    saveBestModel = kcallbacks.ModelCheckpoint(\n",
    "        filepath=savepath,\n",
    "        monitor='val_acc',\n",
    "        verbose=1,\n",
    "        save_best_only=True,\n",
    "        mode='auto')\n",
    "    hist = model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=len(train_generator),\n",
    "        epochs=epochs,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=len(validation_generator),\n",
    "        #use_multiprocessing=True, \n",
    "        callbacks=[history, timer, earlyStopping, saveBestModel],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "resnet(num_class=num_class,epochs=20)\t#binary classificaiton\n",
    "history_this.loss_plot('epoch')\n",
    "history_this.loss_plot('batch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation accuracy of Resnet: 98.652%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.add('Resnet', train_time=timer.get_processing_time(), **history_this.get_best())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 6: Inception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def inception( num_class, epochs,savepath='./inception.h5',history=history_this,input_shape=INPUT_SIZE, timer=timer):\n",
    "    model_fine_tune = InceptionV3(include_top=False, weights='imagenet', input_shape=input_shape)\n",
    "    for layer in model_fine_tune.layers[:35]:\t#the number of frozen layers for transfer learning, have tuned from 50-150\n",
    "        layer.trainable = False\n",
    "    for layer in model_fine_tune.layers[35:]:\t#the number of trainable layers for transfer learning\n",
    "        layer.trainable = True\n",
    "    model = GlobalAveragePooling2D()(model_fine_tune.output)\n",
    "    model=Dense(units=256,activation='relu')(model)\n",
    "    model=Dropout(0.5)(model)\n",
    "    model = Dense(num_class, activation='softmax')(model)\n",
    "    model = Model(model_fine_tune.input, model, name='resnet')\n",
    "    opt = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\t#tuned learning rate to be 0.001\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy']) #set the loss function to be binary crossentropy\n",
    "    #train model\n",
    "    earlyStopping = kcallbacks.EarlyStopping(\n",
    "        monitor='val_acc', patience=2, verbose=1, mode='auto')\t#set early stop patience to save training time\n",
    "    saveBestModel = kcallbacks.ModelCheckpoint(\n",
    "        filepath=savepath,\n",
    "        monitor='val_acc',\n",
    "        verbose=1,\n",
    "        save_best_only=True,\n",
    "        mode='auto')\n",
    "    hist = model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=len(train_generator),\n",
    "        epochs=epochs,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=len(validation_generator),\n",
    "        #use_multiprocessing=True, \n",
    "        callbacks=[history, timer, earlyStopping, saveBestModel],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "inception(num_class=num_class,epochs=20)\t#binary classificaiton\n",
    "history_this.loss_plot('epoch')\n",
    "history_this.loss_plot('batch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation accuracy of Inception: 100.0%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.add('Inception', train_time=timer.get_processing_time(), **history_this.get_best())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 7: InceptionResnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def inceptionresnet( num_class, epochs,savepath='./inceptionresnet.h5',history=history_this,input_shape=INPUT_SIZE, timer=timer):\n",
    "    model_fine_tune = InceptionResNetV2(include_top=False, weights='imagenet', input_shape=input_shape)\n",
    "    for layer in model_fine_tune.layers[:500]:\t#the number of frozen layers for transfer learning, have tuned from 400-550\n",
    "        layer.trainable = False\n",
    "    for layer in model_fine_tune.layers[500:]:\t#the number of trainable layers for transfer learning\n",
    "        layer.trainable = True\n",
    "    model = GlobalAveragePooling2D()(model_fine_tune.output)\n",
    "    model=Dense(units=256,activation='relu')(model)\n",
    "    model=Dropout(0.5)(model)\n",
    "    model = Dense(num_class, activation='softmax')(model)\n",
    "    model = Model(model_fine_tune.input, model, name='resnet')\n",
    "    opt = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\t#tuned learning rate to be 0.001\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy']) #set the loss function to be binary crossentropy\n",
    "    #train model\n",
    "    earlyStopping = kcallbacks.EarlyStopping(\n",
    "        monitor='val_acc', patience=2, verbose=1, mode='auto')\t#set early stop patience to save training time\n",
    "    saveBestModel = kcallbacks.ModelCheckpoint(\n",
    "        filepath=savepath,\n",
    "        monitor='val_acc',\n",
    "        verbose=1,\n",
    "        save_best_only=True,\n",
    "        mode='auto')\n",
    "    hist = model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=len(train_generator),\n",
    "        epochs=epochs,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=len(validation_generator),\n",
    "        #use_multiprocessing=True, \n",
    "        callbacks=[history, timer, earlyStopping, saveBestModel],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "inceptionresnet(num_class=num_class,epochs=20)\t# 5-class classificaiton\n",
    "history_this.loss_plot('epoch')\n",
    "history_this.loss_plot('batch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation accuracy of InceptionResnet: 99.993%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.add('InceptionResnet', train_time=timer.get_processing_time(), **history_this.get_best())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Optimization \n",
    "Use VGG16 as an example.  \n",
    "\n",
    "Tuned hyperparameters of CNN: \n",
    "1. The number of frozen layers\n",
    "2. The number of epochs\n",
    "3. Early stop patience\n",
    "4. Learning rate\n",
    "5. Dropout rate\n",
    "\n",
    "Hyperparameter optimization methods:\n",
    "1. Random search\n",
    "2. Bayesian optimization - Tree Parzen Estimator(BO-TPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prediction(model, test_labels=test_labels, test_images=test_images, label=label):\n",
    "#read images from validation folder\n",
    "    # test_labels = []\n",
    "    # test_images=[]\n",
    "    # for subdir, dirs, files in os.walk(rootdir):\n",
    "    #     for file in files:\n",
    "    #         if not (file.endswith(\".jpeg\"))|(file.endswith(\".jpg\"))|(file.endswith(\".png\")):\n",
    "    #             continue\n",
    "    #         test_labels.append(subdir.split('/')[-1])\n",
    "    #         test_images.append(os.path.join(subdir, file))\n",
    "\n",
    "    # label=validation_generator.class_indices\n",
    "    # label={v: k for k, v in label.items()}\n",
    "    # predict=[]\n",
    "    # length=len(test_images)\n",
    "    # for i in range(length):\n",
    "    #     inputimg=test_images[i]\n",
    "    #     test_batch=[]\n",
    "    #     thisimg=np.array(Image.open(inputimg))/255 #read all the images in validation set\n",
    "    #     #print(thisimg)\n",
    "    #     test_shape=(1,)+thisimg.shape\n",
    "    #     thisimg=thisimg.reshape(test_shape)\n",
    "    #     model_batch=model.predict(thisimg) #use master model to process the input image\n",
    "    #     #generate result by model 1\n",
    "    #     prob=model_batch[0,np.argmax(model_batch,axis=1)[0]]\n",
    "    #     res=label[np.argmax(model_batch,axis=1)[0]]\n",
    "    #     predict.append(res)\n",
    "    acc=accuracy_score(test_labels, get_prediction(model=model, test_images=test_images, label=label))\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model by Own"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_by_own(num_class,input_shape=INPUT_SIZE,epochs=20,patience=2, dropout_rate=0.5,verbose=0,savepath='./model_own.h5',history=history_this,timer=timer):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(64,(3,3),strides=(1,1),input_shape=input_shape,padding='same',activation='relu',kernel_initializer='glorot_uniform'))\n",
    "    model.add(Conv2D(64,(3,3),strides=(1,1),padding='same',activation='relu',kernel_initializer='glorot_uniform'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Conv2D(128,(3,3),strides=(1,1),padding='same',activation='relu',kernel_initializer='glorot_uniform'))\n",
    "    model.add(Conv2D(128,(3,3),strides=(1,1),padding='same',activation='relu',kernel_initializer='glorot_uniform'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Conv2D(256,(3,3),strides=(1,1),padding='same',activation='relu',kernel_initializer='glorot_uniform'))\n",
    "    model.add(Conv2D(256,(3,3),strides=(1,1),padding='same',activation='relu',kernel_initializer='glorot_uniform'))\n",
    "    model.add(Conv2D(256,(3,3),strides=(1,1),padding='same',activation='relu',kernel_initializer='glorot_uniform'))\n",
    "    model.add(GlobalAveragePooling2D())\n",
    "    model.add(Dense(256,activation='relu'))\n",
    "    model.add(Dropout(rate=dropout_rate))\n",
    "    model.add(Dense(num_class,activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "    #train model\n",
    "    earlyStopping=kcallbacks.EarlyStopping(monitor='val_acc', patience=patience, verbose=verbose, mode='auto', restore_best_weights=True)\n",
    "    saveBestModel = kcallbacks.ModelCheckpoint(filepath=savepath, monitor='val_acc', verbose=verbose, save_best_only=True, mode='auto')\n",
    "    hist=model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=len(train_generator),\n",
    "        epochs=epochs,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=len(validation_generator),\n",
    "        callbacks=[history, timer, earlyStopping, saveBestModel],\n",
    "        verbose=verbose\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the objective function to be optimized\n",
    "import time\n",
    "from hyperopt import hp, fmin, tpe, rand, STATUS_OK, Trials\n",
    "import matplotlib.pyplot as plt\n",
    "import statistics \n",
    "\n",
    "def objective(params, num_class=num_class, history=history_hpo):\n",
    "    \n",
    "    params = {\n",
    "        'epochs': int(params['epochs']),\n",
    "        'patience': int(params['patience']),\n",
    "        'dropout_rate': abs(float(params['dropout_rate'])),\n",
    "    }\n",
    "    # frozen=params['frozen']\n",
    "    # epochs=params['epochs']\n",
    "    # patience=params['patience']\n",
    "    # lr=params['lr']\n",
    "    # dropout_rate=params['dropout_rate']\n",
    "\n",
    "    # vgg16(num_class=num_class, frozen=frozen,epochs=epochs,patience=patience, lr=lr, dropout_rate=dropout_rate)\n",
    "\n",
    "    model = cnn_by_own(num_class=num_class, history=history, **params)\n",
    "\n",
    "    acc=prediction(model=model)\n",
    "\n",
    "    print('accuracy:%s'%acc)\n",
    "    return {'loss': -acc, 'status': STATUS_OK }\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BO-TPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameter optimization by Bayesian optimization - Tree Parzen Estimator\n",
    "space = {\n",
    "    'epochs': hp.quniform('epochs', 5, 21, 5),\n",
    "    'patience': hp.quniform('patience', 2, 4, 1),\n",
    "    'dropout_rate': hp.quniform('dropout_rate', 0.3, 0.6, 0.1),\n",
    "}\n",
    "\n",
    "t1=time.time()\n",
    "best = fmin(fn=objective,\n",
    "            space=space,\n",
    "            algo=tpe.suggest,\n",
    "            max_evals=10)\n",
    "print(\"Hyperopt estimated optimum {}\".format(best))\n",
    "t2=time.time()\n",
    "print(\"Time: \"+str(t2-t1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrain the model by using the best hyperparameter values to obtain the best model\n",
    "params = {\n",
    "        'epochs': int(best['epochs']),\n",
    "        'patience': int(best['patience']),\n",
    "        'dropout_rate': abs(float(best['dropout_rate'])),\n",
    "    }\n",
    "cnn_by_own(input_shape=INPUT_SIZE, num_class=num_class, verbose=1, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.add('model_own (BO-TPE)', hpo_time=t2-t1, train_time=timer.get_processing_time(), **history_this.get_best())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameter optimization by Random search\n",
    "space = {\n",
    "    'epochs': hp.quniform('epochs', 5, 21, 5),\n",
    "    'patience': hp.quniform('patience', 2, 4, 1),\n",
    "    'dropout_rate': hp.quniform('dropout_rate', 0.3, 0.6, 0.1),\n",
    "}\n",
    "\n",
    "t1=time.time()\n",
    "best = fmin(fn=objective,\n",
    "            space=space,\n",
    "            algo=rand.suggest,\n",
    "            max_evals=10)\n",
    "print(\"Hyperopt estimated optimum {}\".format(best))\n",
    "t2=time.time()\n",
    "print(\"Time: \"+str(t2-t1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrain the model by using the best hyperparameter values to obtain the best model\n",
    "params = {\n",
    "        'epochs': int(best['epochs']),\n",
    "        'patience': int(best['patience']),\n",
    "        'dropout_rate': abs(float(best['dropout_rate'])),\n",
    "    }\n",
    "cnn_by_own(input_shape=INPUT_SIZE, num_class=num_class, verbose=1, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.add('model_own (Random Search)', hpo_time=t2-t1, train_time=timer.get_processing_time(), **history_this.get_best())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Xception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xception( num_class,epochs=20,frozen=15,lr=0.001,patience=2, dropout_rate=0.5,verbose=0,savepath='./xception.h5',history=history_this,input_shape=INPUT_SIZE,timer=timer):\n",
    "    model_fine_tune = Xception(include_top=False, weights='imagenet', input_shape=input_shape)\n",
    "    for layer in model_fine_tune.layers[:frozen]:\t\t#could be tuned to be 50, 100, or 131\n",
    "        layer.trainable = False\n",
    "    for layer in model_fine_tune.layers[frozen:]:\n",
    "        layer.trainable = True\n",
    "    model = GlobalAveragePooling2D()(model_fine_tune.output)\n",
    "    model=Dense(units=256,activation='relu')(model)\n",
    "    model=Dropout(dropout_rate)(model)\n",
    "    model = Dense(num_class, activation='softmax')(model)\n",
    "    model = Model(model_fine_tune.input, model, name='xception')\n",
    "    opt = keras.optimizers.Adam(lr=lr, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "    #train model\n",
    "    earlyStopping = kcallbacks.EarlyStopping(\n",
    "        monitor='val_acc', patience=patience, verbose=verbose, mode='auto', restore_best_weights=True)\t#patience could be tuned by 2 and 3\n",
    "    saveBestModel = kcallbacks.ModelCheckpoint(\n",
    "        filepath=savepath,\n",
    "        monitor='val_acc',\n",
    "        verbose=verbose,\n",
    "        save_best_only=True,\n",
    "        mode='auto')\n",
    "    hist = model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=len(train_generator),\n",
    "        epochs=epochs,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=len(validation_generator),\n",
    "        #use_multiprocessing=True, \n",
    "        verbose=verbose,\n",
    "        callbacks=[history, timer, earlyStopping, saveBestModel],\n",
    "    )\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the objective function to be optimized\n",
    "import time\n",
    "from hyperopt import hp, fmin, tpe, rand, STATUS_OK, Trials\n",
    "import matplotlib.pyplot as plt\n",
    "import statistics \n",
    "\n",
    "def objective(params, num_class=num_class, history=history_hpo):\n",
    "    \n",
    "    params = {\n",
    "        'frozen': int(params['frozen']),\n",
    "        'epochs': int(params['epochs']),\n",
    "        'patience': int(params['patience']),\n",
    "        'lr': abs(float(params['lr'])),\n",
    "        'dropout_rate': abs(float(params['dropout_rate'])),\n",
    "    }\n",
    "    # frozen=params['frozen']\n",
    "    # epochs=params['epochs']\n",
    "    # patience=params['patience']\n",
    "    # lr=params['lr']\n",
    "    # dropout_rate=params['dropout_rate']\n",
    "\n",
    "    # vgg16(num_class=num_class, frozen=frozen,epochs=epochs,patience=patience, lr=lr, dropout_rate=dropout_rate)\n",
    "\n",
    "    model = xception(num_class=num_class, history=history, **params)\n",
    "\n",
    "    acc=prediction(model=model)\n",
    "\n",
    "    print('accuracy:%s'%acc)\n",
    "    return {'loss': -acc, 'status': STATUS_OK }\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BO-TPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameter optimization by Bayesian optimization - Tree Parzen Estimator\n",
    "available_frozen = [50, 100, 131]\n",
    "space = {\n",
    "    'frozen': hp.choice('frozen', available_frozen),\n",
    "    'epochs': hp.quniform('epochs', 5, 21, 5),\n",
    "    'patience': hp.quniform('patience', 2, 4, 1),\n",
    "    'lr': hp.quniform('lr', 0.001, 0.006, 0.001),\n",
    "    'dropout_rate': hp.quniform('dropout_rate', 0.3, 0.6, 0.1),\n",
    "}\n",
    "\n",
    "t1=time.time()\n",
    "best = fmin(fn=objective,\n",
    "            space=space,\n",
    "            algo=tpe.suggest,\n",
    "            max_evals=10)\n",
    "print(\"Hyperopt estimated optimum {}\".format(best))\n",
    "t2=time.time()\n",
    "print(\"Time: \"+str(t2-t1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrain the model by using the best hyperparameter values to obtain the best model\n",
    "params = {\n",
    "        'frozen': available_frozen[int(best['frozen'])],\n",
    "        'epochs': int(best['epochs']),\n",
    "        'patience': int(best['patience']),\n",
    "        'lr': abs(float(best['lr'])),\n",
    "        'dropout_rate': abs(float(best['dropout_rate'])),\n",
    "    }\n",
    "xception(num_class=num_class, verbose=1, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.add('Xception (BO-TPE)', hpo_time=t2-t1, train_time=timer.get_processing_time(), **history_this.get_best())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameter optimization by Random search\n",
    "available_frozen = [50, 100, 131]\n",
    "space = {\n",
    "    'frozen': hp.choice('frozen', available_frozen),\n",
    "    'epochs': hp.quniform('epochs', 5, 21, 5),\n",
    "    'patience': hp.quniform('patience', 2, 4, 1),\n",
    "    'lr': hp.quniform('lr', 0.001, 0.006, 0.001),\n",
    "    'dropout_rate': hp.quniform('dropout_rate', 0.3, 0.6, 0.1),\n",
    "}\n",
    "\n",
    "t1=time.time()\n",
    "best = fmin(fn=objective,\n",
    "            space=space,\n",
    "            algo=rand.suggest,\n",
    "            max_evals=10)\n",
    "print(\"Hyperopt estimated optimum {}\".format(best))\n",
    "t2=time.time()\n",
    "print(\"Time: \"+str(t2-t1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrain the model by using the best hyperparameter values to obtain the best model\n",
    "params = {\n",
    "        'frozen': available_frozen[int(best['frozen'])],\n",
    "        'epochs': int(best['epochs']),\n",
    "        'patience': int(best['patience']),\n",
    "        'lr': abs(float(best['lr'])),\n",
    "        'dropout_rate': abs(float(best['dropout_rate'])),\n",
    "    }\n",
    "xception(num_class=num_class, verbose=1, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.add('Xception (Random Search)', hpo_time=t2-t1, train_time=timer.get_processing_time(), **history_this.get_best())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def vgg16(num_class,epochs=20,frozen=15,lr=0.001,patience=2, dropout_rate=0.5,verbose=0, savepath='./VGG16.h5',history=history_this,timer=timer,input_shape=INPUT_SIZE):\n",
    "    model_fine_tune = VGG16(include_top=False, weights='imagenet', input_shape=input_shape)\n",
    "    for layer in model_fine_tune.layers[:frozen]:\t#the number of frozen layers for transfer learning, have tuned from 5-18\n",
    "        layer.trainable = False\n",
    "    for layer in model_fine_tune.layers[frozen:]:\n",
    "        layer.trainable = True\n",
    "    model = GlobalAveragePooling2D()(model_fine_tune.output)\n",
    "    model=Dense(units=256,activation='relu')(model)\n",
    "    model=Dropout(dropout_rate)(model)\n",
    "    model = Dense(num_class, activation='softmax')(model)\n",
    "    model = Model(model_fine_tune.input, model, name='vgg')\n",
    "    opt = keras.optimizers.Adam(lr=lr, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\t#tuned learning rate to be 0.001\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\t#set the loss function to be binary crossentropy\n",
    "    #train model\n",
    "    earlyStopping = kcallbacks.EarlyStopping(\n",
    "        monitor='val_acc', patience=patience, verbose=verbose, mode='auto', restore_best_weights=True)\t#set early stop patience to save training time\n",
    "    saveBestModel = kcallbacks.ModelCheckpoint(\n",
    "        filepath=savepath,\n",
    "        monitor='val_acc',\n",
    "        verbose=verbose,\n",
    "        save_best_only=True,\n",
    "        mode='auto')\n",
    "    hist = model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=len(train_generator),\n",
    "        epochs=epochs,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=len(validation_generator),\n",
    "        #use_multiprocessing=True, \n",
    "        #workers=2,\n",
    "        callbacks=[history, timer, earlyStopping, saveBestModel],\n",
    "        verbose = verbose\n",
    "    )\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#define the objective function to be optimized\n",
    "import time\n",
    "from hyperopt import hp, fmin, tpe, rand, STATUS_OK, Trials\n",
    "import matplotlib.pyplot as plt\n",
    "import statistics \n",
    "\n",
    "def objective(params, num_class=num_class, history=history_hpo):\n",
    "    \n",
    "    params = {\n",
    "        'frozen': int(params['frozen']),\n",
    "        'epochs': int(params['epochs']),\n",
    "        'patience': int(params['patience']),\n",
    "        'lr': abs(float(params['lr'])),\n",
    "        'dropout_rate': abs(float(params['dropout_rate'])),\n",
    "    }\n",
    "    # frozen=params['frozen']\n",
    "    # epochs=params['epochs']\n",
    "    # patience=params['patience']\n",
    "    # lr=params['lr']\n",
    "    # dropout_rate=params['dropout_rate']\n",
    "\n",
    "    # vgg16(num_class=num_class, frozen=frozen,epochs=epochs,patience=patience, lr=lr, dropout_rate=dropout_rate)\n",
    "\n",
    "    model = vgg16(num_class=num_class, history=history, **params)\n",
    "\n",
    "    acc=prediction(model=model)\n",
    "\n",
    "    print('accuracy:%s'%acc)\n",
    "    return {'loss': -acc, 'status': STATUS_OK }\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BO-TPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#Hyperparameter optimization by Bayesian optimization - Tree Parzen Estimator\n",
    "space = {\n",
    "    'frozen': hp.quniform('frozen', 15, 18, 1),\n",
    "    'epochs': hp.quniform('epochs', 5, 21, 5),\n",
    "    'patience': hp.quniform('patience', 2, 4, 1),\n",
    "    'lr': hp.quniform('lr', 0.001, 0.006, 0.001),\n",
    "    'dropout_rate': hp.quniform('dropout_rate', 0.3, 0.6, 0.1),\n",
    "}\n",
    "\n",
    "t1=time.time()\n",
    "best = fmin(fn=objective,\n",
    "            space=space,\n",
    "            algo=tpe.suggest,\n",
    "            max_evals=10)\n",
    "print(\"Hyperopt estimated optimum {}\".format(best))\n",
    "t2=time.time()\n",
    "print(\"Time: \"+str(t2-t1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrain the model by using the best hyperparameter values to obtain the best model\n",
    "params = {\n",
    "        'frozen': int(best['frozen']),\n",
    "        'epochs': int(best['epochs']),\n",
    "        'patience': int(best['patience']),\n",
    "        'lr': abs(float(best['lr'])),\n",
    "        'dropout_rate': abs(float(best['dropout_rate'])),\n",
    "    }\n",
    "vgg16(num_class=num_class, verbose=1, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.add('VGG16 (BO-TPE)', hpo_time=t2-t1, train_time=timer.get_processing_time(), **history_this.get_best())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#Hyperparameter optimization by Random search\n",
    "space = {\n",
    "    'frozen': hp.quniform('frozen', 15, 18, 1),\n",
    "    'epochs': hp.quniform('epochs', 5, 21, 5),\n",
    "    'patience': hp.quniform('patience', 2, 4, 1),\n",
    "    'lr': hp.quniform('lr', 0.001, 0.006, 0.001),\n",
    "    'dropout_rate': hp.quniform('dropout_rate', 0.3, 0.6, 0.1),\n",
    "}\n",
    "\n",
    "t1=time.time()\n",
    "best = fmin(fn=objective,\n",
    "            space=space,\n",
    "            algo=rand.suggest,\n",
    "            max_evals=10)\n",
    "print(\"Hyperopt estimated optimum {}\".format(best))\n",
    "t2=time.time()\n",
    "print(\"Time: \"+str(t2-t1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Retrain the model by using the best hyperparameter values to obtain the best model\n",
    "params = {\n",
    "        'frozen': int(best['frozen']),\n",
    "        'epochs': int(best['epochs']),\n",
    "        'patience': int(best['patience']),\n",
    "        'lr': abs(float(best['lr'])),\n",
    "        'dropout_rate': abs(float(best['dropout_rate'])),\n",
    "    }\n",
    "vgg16(num_class=num_class, verbose=1, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.add('VGG16 (Random Search)', hpo_time=t2-t1, train_time=timer.get_processing_time(), **history_this.get_best())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGG19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vgg19( num_class,epochs=20,frozen=15,lr=0.001,patience=2, dropout_rate=0.5,verbose=0,savepath='./VGG19.h5',history=history_this,input_shape=INPUT_SIZE,timer=timer):\n",
    "    model_fine_tune = VGG19(include_top=False, weights='imagenet', input_shape=input_shape)\n",
    "    for layer in model_fine_tune.layers[:frozen]:\t#the number of frozen layers for transfer learning, have tuned from 5-18\n",
    "        layer.trainable = False\n",
    "    for layer in model_fine_tune.layers[frozen:]:\n",
    "        layer.trainable = True\n",
    "    model = GlobalAveragePooling2D()(model_fine_tune.output)\n",
    "    model=Dense(units=256,activation='relu')(model)\n",
    "    model=Dropout(dropout_rate)(model)\n",
    "    model = Dense(num_class, activation='softmax')(model)\n",
    "    model = Model(model_fine_tune.input, model, name='vgg')\n",
    "    opt = keras.optimizers.Adam(lr=lr, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\t#tuned learning rate to be 0.001\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\t#set the loss function to be binary crossentropy\n",
    "    #train model\n",
    "    earlyStopping = kcallbacks.EarlyStopping(\n",
    "        monitor='val_acc', patience=patience, verbose=verbose, mode='auto', restore_best_weights=True)\t#set early stop patience to save training time\n",
    "    saveBestModel = kcallbacks.ModelCheckpoint(\n",
    "        filepath=savepath,\n",
    "        monitor='val_acc',\n",
    "        verbose=verbose,\n",
    "        save_best_only=True,\n",
    "        mode='auto')\n",
    "    hist = model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=len(train_generator),\n",
    "        epochs=epochs,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=len(validation_generator),\n",
    "        verbose=verbose,\n",
    "        #use_multiprocessing=True, \n",
    "        #workers=2,\n",
    "        callbacks=[history, timer, earlyStopping, saveBestModel],\n",
    "    )\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the objective function to be optimized\n",
    "import time\n",
    "from hyperopt import hp, fmin, tpe, rand, STATUS_OK, Trials\n",
    "import matplotlib.pyplot as plt\n",
    "import statistics \n",
    "\n",
    "def objective(params, num_class=num_class, history=history_hpo):\n",
    "    \n",
    "    params = {\n",
    "        'frozen': int(params['frozen']),\n",
    "        'epochs': int(params['epochs']),\n",
    "        'patience': int(params['patience']),\n",
    "        'lr': abs(float(params['lr'])),\n",
    "        'dropout_rate': abs(float(params['dropout_rate'])),\n",
    "    }\n",
    "    # frozen=params['frozen']\n",
    "    # epochs=params['epochs']\n",
    "    # patience=params['patience']\n",
    "    # lr=params['lr']\n",
    "    # dropout_rate=params['dropout_rate']\n",
    "\n",
    "    # vgg16(num_class=num_class, frozen=frozen,epochs=epochs,patience=patience, lr=lr, dropout_rate=dropout_rate)\n",
    "\n",
    "    model = vgg19(num_class=num_class, history=history, **params)\n",
    "\n",
    "    acc=prediction(model=model)\n",
    "\n",
    "    print('accuracy:%s'%acc)\n",
    "    return {'loss': -acc, 'status': STATUS_OK }\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BO-TPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameter optimization by Bayesian optimization - Tree Parzen Estimator\n",
    "space = {\n",
    "    'frozen': hp.quniform('frozen', 15, 18, 1),\n",
    "    'epochs': hp.quniform('epochs', 5, 21, 5),\n",
    "    'patience': hp.quniform('patience', 2, 4, 1),\n",
    "    'lr': hp.quniform('lr', 0.001, 0.006, 0.001),\n",
    "    'dropout_rate': hp.quniform('dropout_rate', 0.3, 0.6, 0.1),\n",
    "}\n",
    "\n",
    "t1=time.time()\n",
    "best = fmin(fn=objective,\n",
    "            space=space,\n",
    "            algo=tpe.suggest,\n",
    "            max_evals=10)\n",
    "print(\"Hyperopt estimated optimum {}\".format(best))\n",
    "t2=time.time()\n",
    "print(\"Time: \"+str(t2-t1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrain the model by using the best hyperparameter values to obtain the best model\n",
    "params = {\n",
    "        'frozen': int(best['frozen']),\n",
    "        'epochs': int(best['epochs']),\n",
    "        'patience': int(best['patience']),\n",
    "        'lr': abs(float(best['lr'])),\n",
    "        'dropout_rate': abs(float(best['dropout_rate'])),\n",
    "    }\n",
    "vgg19(num_class=num_class, verbose=1, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.add('VGG19 (BO-TPE)', hpo_time=t2-t1, train_time=timer.get_processing_time(), **history_this.get_best())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameter optimization by Random search\n",
    "space = {\n",
    "    'frozen': hp.quniform('frozen', 15, 18, 1),\n",
    "    'epochs': hp.quniform('epochs', 5, 21, 5),\n",
    "    'patience': hp.quniform('patience', 2, 4, 1),\n",
    "    'lr': hp.quniform('lr', 0.001, 0.006, 0.001),\n",
    "    'dropout_rate': hp.quniform('dropout_rate', 0.3, 0.6, 0.1),\n",
    "}\n",
    "\n",
    "t1=time.time()\n",
    "best = fmin(fn=objective,\n",
    "            space=space,\n",
    "            algo=rand.suggest,\n",
    "            max_evals=10)\n",
    "print(\"Hyperopt estimated optimum {}\".format(best))\n",
    "t2=time.time()\n",
    "print(\"Time: \"+str(t2-t1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrain the model by using the best hyperparameter values to obtain the best model\n",
    "params = {\n",
    "        'frozen': int(best['frozen']),\n",
    "        'epochs': int(best['epochs']),\n",
    "        'patience': int(best['patience']),\n",
    "        'lr': abs(float(best['lr'])),\n",
    "        'dropout_rate': abs(float(best['dropout_rate'])),\n",
    "    }\n",
    "vgg19(num_class=num_class, verbose=1, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.add('VGG19 (Random Search)', hpo_time=t2-t1, train_time=timer.get_processing_time(), **history_this.get_best())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resnet( num_class, epochs=20,frozen=120,lr=0.001,patience=2, dropout_rate=0.5,verbose=0,savepath='./resnet.h5',history=history_this,input_shape=INPUT_SIZE,timer=timer):\n",
    "    model_fine_tune = ResNet50(include_top=False, weights='imagenet', input_shape=input_shape)\n",
    "    for layer in model_fine_tune.layers[:frozen]:\t#the number of frozen layers for transfer learning, have tuned from 50-150\n",
    "        layer.trainable = False\n",
    "    for layer in model_fine_tune.layers[frozen:]:\t#the number of trainable layers for transfer learning\n",
    "        layer.trainable = True\n",
    "    model = GlobalAveragePooling2D()(model_fine_tune.output)\n",
    "    model=Dense(units=256,activation='relu')(model)\n",
    "    model=Dropout(dropout_rate)(model)\n",
    "    model = Dense(num_class, activation='softmax')(model)\n",
    "    model = Model(model_fine_tune.input, model, name='resnet')\n",
    "    opt = keras.optimizers.Adam(lr=lr, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\t#tuned learning rate to be 0.001\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy']) #set the loss function to be binary crossentropy\n",
    "    #train model\n",
    "    earlyStopping = kcallbacks.EarlyStopping(\n",
    "        monitor='val_acc', patience=patience, verbose=verbose, mode='auto', restore_best_weights=True)\t#set early stop patience to save training time\n",
    "    saveBestModel = kcallbacks.ModelCheckpoint(\n",
    "        filepath=savepath,\n",
    "        monitor='val_acc',\n",
    "        verbose=verbose,\n",
    "        save_best_only=True,\n",
    "        mode='auto')\n",
    "    hist = model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=len(train_generator),\n",
    "        epochs=epochs,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=len(validation_generator),\n",
    "        #use_multiprocessing=True, \n",
    "        verbose=verbose,\n",
    "        callbacks=[history, timer, earlyStopping, saveBestModel],\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the objective function to be optimized\n",
    "import time\n",
    "from hyperopt import hp, fmin, tpe, rand, STATUS_OK, Trials\n",
    "import matplotlib.pyplot as plt\n",
    "import statistics \n",
    "\n",
    "def objective(params, num_class=num_class, history=history_hpo):\n",
    "    \n",
    "    params = {\n",
    "        'frozen': int(params['frozen']),\n",
    "        'epochs': int(params['epochs']),\n",
    "        'patience': int(params['patience']),\n",
    "        'lr': abs(float(params['lr'])),\n",
    "        'dropout_rate': abs(float(params['dropout_rate'])),\n",
    "    }\n",
    "    # frozen=params['frozen']\n",
    "    # epochs=params['epochs']\n",
    "    # patience=params['patience']\n",
    "    # lr=params['lr']\n",
    "    # dropout_rate=params['dropout_rate']\n",
    "\n",
    "    # vgg16(num_class=num_class, frozen=frozen,epochs=epochs,patience=patience, lr=lr, dropout_rate=dropout_rate)\n",
    "\n",
    "    model = resnet(num_class=num_class, history=history, **params)\n",
    "\n",
    "    acc=prediction(model=model)\n",
    "\n",
    "    print('accuracy:%s'%acc)\n",
    "    return {'loss': -acc, 'status': STATUS_OK }\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BO-TPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameter optimization by Bayesian optimization - Tree Parzen Estimator\n",
    "space = {\n",
    "    'frozen': hp.quniform('frozen', 50, 150, 10),\n",
    "    'epochs': hp.quniform('epochs', 5, 21, 5),\n",
    "    'patience': hp.quniform('patience', 2, 4, 1),\n",
    "    'lr': hp.quniform('lr', 0.001, 0.006, 0.001),\n",
    "    'dropout_rate': hp.quniform('dropout_rate', 0.3, 0.6, 0.1),\n",
    "}\n",
    "\n",
    "t1=time.time()\n",
    "best = fmin(fn=objective,\n",
    "            space=space,\n",
    "            algo=tpe.suggest,\n",
    "            max_evals=10)\n",
    "print(\"Hyperopt estimated optimum {}\".format(best))\n",
    "t2=time.time()\n",
    "print(\"Time: \"+str(t2-t1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrain the model by using the best hyperparameter values to obtain the best model\n",
    "params = {\n",
    "        'frozen': int(best['frozen']),\n",
    "        'epochs': int(best['epochs']),\n",
    "        'patience': int(best['patience']),\n",
    "        'lr': abs(float(best['lr'])),\n",
    "        'dropout_rate': abs(float(best['dropout_rate'])),\n",
    "    }\n",
    "resnet(num_class=num_class, verbose=1, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.add('ResNet (BO-TPE)', hpo_time=t2-t1, train_time=timer.get_processing_time(), **history_this.get_best())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameter optimization by Random search\n",
    "space = {\n",
    "    'frozen': hp.quniform('frozen', 50, 150, 10),\n",
    "    'epochs': hp.quniform('epochs', 5, 21, 5),\n",
    "    'patience': hp.quniform('patience', 2, 4, 1),\n",
    "    'lr': hp.quniform('lr', 0.001, 0.006, 0.001),\n",
    "    'dropout_rate': hp.quniform('dropout_rate', 0.3, 0.6, 0.1),\n",
    "}\n",
    "\n",
    "t1=time.time()\n",
    "best = fmin(fn=objective,\n",
    "            space=space,\n",
    "            algo=rand.suggest,\n",
    "            max_evals=10)\n",
    "print(\"Hyperopt estimated optimum {}\".format(best))\n",
    "t2=time.time()\n",
    "print(\"Time: \"+str(t2-t1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrain the model by using the best hyperparameter values to obtain the best model\n",
    "params = {\n",
    "        'frozen': int(best['frozen']),\n",
    "        'epochs': int(best['epochs']),\n",
    "        'patience': int(best['patience']),\n",
    "        'lr': abs(float(best['lr'])),\n",
    "        'dropout_rate': abs(float(best['dropout_rate'])),\n",
    "    }\n",
    "resnet(num_class=num_class, verbose=1, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.add('ResNet (Random Search)', hpo_time=t2-t1, train_time=timer.get_processing_time(), **history_this.get_best())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inception( num_class, epochs=20,frozen=120,lr=0.001,patience=2, dropout_rate=0.5,verbose=0,savepath='./inception.h5',history=history_this,input_shape=INPUT_SIZE, timer=timer):\n",
    "    model_fine_tune = InceptionV3(include_top=False, weights='imagenet', input_shape=input_shape)\n",
    "    for layer in model_fine_tune.layers[:frozen]:\t#the number of frozen layers for transfer learning, have tuned from 50-150\n",
    "        layer.trainable = False\n",
    "    for layer in model_fine_tune.layers[frozen:]:\t#the number of trainable layers for transfer learning\n",
    "        layer.trainable = True\n",
    "    model = GlobalAveragePooling2D()(model_fine_tune.output)\n",
    "    model=Dense(units=256,activation='relu')(model)\n",
    "    model=Dropout(dropout_rate)(model)\n",
    "    model = Dense(num_class, activation='softmax')(model)\n",
    "    model = Model(model_fine_tune.input, model, name='resnet')\n",
    "    opt = keras.optimizers.Adam(lr=lr, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\t#tuned learning rate to be 0.001\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy']) #set the loss function to be binary crossentropy\n",
    "    #train model\n",
    "    earlyStopping = kcallbacks.EarlyStopping(\n",
    "        monitor='val_acc', patience=patience, verbose=verbose, mode='auto', restore_best_weights=True)\t#set early stop patience to save training time\n",
    "    saveBestModel = kcallbacks.ModelCheckpoint(\n",
    "        filepath=savepath,\n",
    "        monitor='val_acc',\n",
    "        verbose=verbose,\n",
    "        save_best_only=True,\n",
    "        mode='auto')\n",
    "    hist = model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=len(train_generator),\n",
    "        epochs=epochs,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=len(validation_generator),\n",
    "        #use_multiprocessing=True, \n",
    "        verbose=verbose,\n",
    "        callbacks=[history, timer, earlyStopping, saveBestModel],\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the objective function to be optimized\n",
    "import time\n",
    "from hyperopt import hp, fmin, tpe, rand, STATUS_OK, Trials\n",
    "import matplotlib.pyplot as plt\n",
    "import statistics \n",
    "\n",
    "def objective(params, num_class=num_class, history=history_hpo):\n",
    "    \n",
    "    params = {\n",
    "        'frozen': int(params['frozen']),\n",
    "        'epochs': int(params['epochs']),\n",
    "        'patience': int(params['patience']),\n",
    "        'lr': abs(float(params['lr'])),\n",
    "        'dropout_rate': abs(float(params['dropout_rate'])),\n",
    "    }\n",
    "    # frozen=params['frozen']\n",
    "    # epochs=params['epochs']\n",
    "    # patience=params['patience']\n",
    "    # lr=params['lr']\n",
    "    # dropout_rate=params['dropout_rate']\n",
    "\n",
    "    # vgg16(num_class=num_class, frozen=frozen,epochs=epochs,patience=patience, lr=lr, dropout_rate=dropout_rate)\n",
    "\n",
    "    model = inception(num_class=num_class, history=history, **params)\n",
    "\n",
    "    acc=prediction(model=model)\n",
    "\n",
    "    print('accuracy:%s'%acc)\n",
    "    return {'loss': -acc, 'status': STATUS_OK }\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BO-TPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameter optimization by Bayesian optimization - Tree Parzen Estimator\n",
    "space = {\n",
    "    'frozen': hp.quniform('frozen', 50, 150, 10),\n",
    "    'epochs': hp.quniform('epochs', 5, 21, 5),\n",
    "    'patience': hp.quniform('patience', 2, 4, 1),\n",
    "    'lr': hp.quniform('lr', 0.001, 0.006, 0.001),\n",
    "    'dropout_rate': hp.quniform('dropout_rate', 0.3, 0.6, 0.1),\n",
    "}\n",
    "\n",
    "t1=time.time()\n",
    "best = fmin(fn=objective,\n",
    "            space=space,\n",
    "            algo=tpe.suggest,\n",
    "            max_evals=10)\n",
    "print(\"Hyperopt estimated optimum {}\".format(best))\n",
    "t2=time.time()\n",
    "print(\"Time: \"+str(t2-t1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrain the model by using the best hyperparameter values to obtain the best model\n",
    "params = {\n",
    "        'frozen': int(best['frozen']),\n",
    "        'epochs': int(best['epochs']),\n",
    "        'patience': int(best['patience']),\n",
    "        'lr': abs(float(best['lr'])),\n",
    "        'dropout_rate': abs(float(best['dropout_rate'])),\n",
    "    }\n",
    "inception(num_class=num_class, verbose=1, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.add('Inception (BO-TPE)', hpo_time=t2-t1, train_time=timer.get_processing_time(), **history_this.get_best())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameter optimization by Random search\n",
    "space = {\n",
    "    'frozen': hp.quniform('frozen', 50, 150, 10),\n",
    "    'epochs': hp.quniform('epochs', 5, 21, 5),\n",
    "    'patience': hp.quniform('patience', 2, 4, 1),\n",
    "    'lr': hp.quniform('lr', 0.001, 0.006, 0.001),\n",
    "    'dropout_rate': hp.quniform('dropout_rate', 0.3, 0.6, 0.1),\n",
    "}\n",
    "\n",
    "t1=time.time()\n",
    "best = fmin(fn=objective,\n",
    "            space=space,\n",
    "            algo=rand.suggest,\n",
    "            max_evals=10)\n",
    "print(\"Hyperopt estimated optimum {}\".format(best))\n",
    "t2=time.time()\n",
    "print(\"Time: \"+str(t2-t1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrain the model by using the best hyperparameter values to obtain the best model\n",
    "params = {\n",
    "        'frozen': int(best['frozen']),\n",
    "        'epochs': int(best['epochs']),\n",
    "        'patience': int(best['patience']),\n",
    "        'lr': abs(float(best['lr'])),\n",
    "        'dropout_rate': abs(float(best['dropout_rate'])),\n",
    "    }\n",
    "inception(num_class=num_class, verbose=1, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.add('Inception (Random Search)', hpo_time=t2-t1, train_time=timer.get_processing_time(), **history_this.get_best())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## InceptionResnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inceptionresnet( num_class, epochs=20,frozen=120,lr=0.001,patience=2, dropout_rate=0.5,verbose=0,savepath='./inceptionresnet.h5',history=history_this,input_shape=INPUT_SIZE, timer=timer):\n",
    "    model_fine_tune = InceptionResNetV2(include_top=False, weights='imagenet', input_shape=input_shape)\n",
    "    for layer in model_fine_tune.layers[:frozen]:\t#the number of frozen layers for transfer learning, have tuned from 400-550\n",
    "        layer.trainable = False\n",
    "    for layer in model_fine_tune.layers[frozen:]:\t#the number of trainable layers for transfer learning\n",
    "        layer.trainable = True\n",
    "    model = GlobalAveragePooling2D()(model_fine_tune.output)\n",
    "    model=Dense(units=256,activation='relu')(model)\n",
    "    model=Dropout(dropout_rate)(model)\n",
    "    model = Dense(num_class, activation='softmax')(model)\n",
    "    model = Model(model_fine_tune.input, model, name='resnet')\n",
    "    opt = keras.optimizers.Adam(lr=lr, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\t#tuned learning rate to be 0.001\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy']) #set the loss function to be binary crossentropy\n",
    "    #train model\n",
    "    earlyStopping = kcallbacks.EarlyStopping(\n",
    "        monitor='val_acc', patience=patience, verbose=verbose, mode='auto', restore_best_weights=True)\t#set early stop patience to save training time\n",
    "    saveBestModel = kcallbacks.ModelCheckpoint(\n",
    "        filepath=savepath,\n",
    "        monitor='val_acc',\n",
    "        verbose=verbose,\n",
    "        save_best_only=True,\n",
    "        mode='auto')\n",
    "    hist = model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=len(train_generator),\n",
    "        epochs=epochs,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=len(validation_generator),\n",
    "        #use_multiprocessing=True, \n",
    "        verbose=verbose,\n",
    "        callbacks=[history, timer, earlyStopping, saveBestModel],\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the objective function to be optimized\n",
    "import time\n",
    "from hyperopt import hp, fmin, tpe, rand, STATUS_OK, Trials\n",
    "import matplotlib.pyplot as plt\n",
    "import statistics \n",
    "\n",
    "def objective(params, num_class=num_class, history=history_hpo):\n",
    "    \n",
    "    params = {\n",
    "        'frozen': int(params['frozen']),\n",
    "        'epochs': int(params['epochs']),\n",
    "        'patience': int(params['patience']),\n",
    "        'lr': abs(float(params['lr'])),\n",
    "        'dropout_rate': abs(float(params['dropout_rate'])),\n",
    "    }\n",
    "    # frozen=params['frozen']\n",
    "    # epochs=params['epochs']\n",
    "    # patience=params['patience']\n",
    "    # lr=params['lr']\n",
    "    # dropout_rate=params['dropout_rate']\n",
    "\n",
    "    # vgg16(num_class=num_class, frozen=frozen,epochs=epochs,patience=patience, lr=lr, dropout_rate=dropout_rate)\n",
    "\n",
    "    model = inceptionresnet(num_class=num_class, history=history, **params)\n",
    "\n",
    "    acc=prediction(model=model)\n",
    "\n",
    "    print('accuracy:%s'%acc)\n",
    "    return {'loss': -acc, 'status': STATUS_OK }\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BO-TPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameter optimization by Bayesian optimization - Tree Parzen Estimator\n",
    "space = {\n",
    "    'frozen': hp.quniform('frozen', 400, 500, 10),\n",
    "    'epochs': hp.quniform('epochs', 5, 21, 5),\n",
    "    'patience': hp.quniform('patience', 2, 4, 1),\n",
    "    'lr': hp.quniform('lr', 0.001, 0.006, 0.001),\n",
    "    'dropout_rate': hp.quniform('dropout_rate', 0.3, 0.6, 0.1),\n",
    "}\n",
    "\n",
    "t1=time.time()\n",
    "best = fmin(fn=objective,\n",
    "            space=space,\n",
    "            algo=tpe.suggest,\n",
    "            max_evals=10)\n",
    "print(\"Hyperopt estimated optimum {}\".format(best))\n",
    "t2=time.time()\n",
    "print(\"Time: \"+str(t2-t1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrain the model by using the best hyperparameter values to obtain the best model\n",
    "params = {\n",
    "        'frozen': int(best['frozen']),\n",
    "        'epochs': int(best['epochs']),\n",
    "        'patience': int(best['patience']),\n",
    "        'lr': abs(float(best['lr'])),\n",
    "        'dropout_rate': abs(float(best['dropout_rate'])),\n",
    "    }\n",
    "inceptionresnet(num_class=num_class, verbose=1, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.add('InceptionResnet (BO-TPE)', hpo_time=t2-t1, train_time=timer.get_processing_time(), **history_this.get_best())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameter optimization by Random search\n",
    "space = {\n",
    "    'frozen': hp.quniform('frozen', 400, 500, 10),\n",
    "    'epochs': hp.quniform('epochs', 5, 21, 5),\n",
    "    'patience': hp.quniform('patience', 2, 4, 1),\n",
    "    'lr': hp.quniform('lr', 0.001, 0.006, 0.001),\n",
    "    'dropout_rate': hp.quniform('dropout_rate', 0.3, 0.6, 0.1),\n",
    "}\n",
    "\n",
    "t1=time.time()\n",
    "best = fmin(fn=objective,\n",
    "            space=space,\n",
    "            algo=rand.suggest,\n",
    "            max_evals=10)\n",
    "print(\"Hyperopt estimated optimum {}\".format(best))\n",
    "t2=time.time()\n",
    "print(\"Time: \"+str(t2-t1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrain the model by using the best hyperparameter values to obtain the best model\n",
    "params = {\n",
    "        'frozen': int(best['frozen']),\n",
    "        'epochs': int(best['epochs']),\n",
    "        'patience': int(best['patience']),\n",
    "        'lr': abs(float(best['lr'])),\n",
    "        'dropout_rate': abs(float(best['dropout_rate'])),\n",
    "    }\n",
    "inceptionresnet(num_class=num_class, verbose=1, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.add('InceptionResnet (Random Search)', hpo_time=t2-t1, train_time=timer.get_processing_time(), **history_this.get_best())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.to_excel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Online GPU renting platform specification\n",
    "# WeChat Message\n",
    "import requests\n",
    "resp = requests.get(\n",
    "    \"https://www.autodl.com/api/v1/wechat/message/push?token={token}&title={title}&name={name}&content={content}\".format(\n",
    "        token=\"\",\n",
    "        title=\"From AutoDL\",\n",
    "        name=\"UNSW-NB15 CNN\",\n",
    "        content=\"Training Complete\")\n",
    ")\n",
    "print(resp.content.decode())\n",
    "# Shutdown\n",
    "!shutdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "320b296e7109c883957f133fbd8949a190ec3224eb5024468a10effb492e8d5e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
